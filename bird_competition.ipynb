{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 70203,
          "databundleVersionId": 8068726,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkranyak/project_3/blob/main/bird_competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Index:\n",
        "# 1. Install Necessary Libraries\n",
        "# 2. Set Up Directory Structure for Processed Data\n",
        "# 3. Define Spectrogram Conversion Function\n",
        "# 4. Preprocess Audio Files into Spectrograms\n",
        "# 5. Data Augmentation (Optional)\n",
        "# 6. Split Dataset\n",
        "# 7. Prepare Model Input\n",
        "# 8. Define Model Architecture\n",
        "# 9. Compile Model\n",
        "# 10. Prepare for Training (Callbacks)\n",
        "# 11. Train Model\n",
        "# 12. Evaluate Model\n",
        "# 13. Submit to Kaggle"
      ],
      "metadata": {
        "id": "GjkOx8w3M1YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Necessary Libraries\n"
      ],
      "metadata": {
        "id": "dcxkxLnfNROe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile tensorflow scikit-learn numpy pandas matplotlib seaborn tqdm\n",
        "!pip install audiomentations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nZ-md9VOIwO",
        "outputId": "7529f896-bb61-4f86-edba-f969a3cb3cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.11.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Collecting audiomentations\n",
            "  Downloading audiomentations-0.35.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.25.2)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.10.1)\n",
            "Requirement already satisfied: scipy<2,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.11.4)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.3.7)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.11.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.4.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.2.2)\n",
            "Installing collected packages: audiomentations\n",
            "Successfully installed audiomentations-0.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, LSTM, TimeDistributed, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from keras.utils import Sequence\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import joblib\n",
        "from multiprocessing import Pool\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n"
      ],
      "metadata": {
        "id": "IkJjbRYFImN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "xT6x2R81FwpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Setup\n",
        "\n",
        "---\n",
        "\n",
        "###Download and unpack dataset\n"
      ],
      "metadata": {
        "id": "ooRVEW5nJZaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set up directory structure for processed data"
      ],
      "metadata": {
        "id": "3J-YdH9zMaFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output directories\n",
        "output_directories = ['processed_data', 'models', 'submissions', 'visualizations']\n",
        "base_working_dir = '/kaggle/working/'\n",
        "\n",
        "# Creating subdirectories\n",
        "for directory in output_directories:\n",
        "    path = os.path.join(base_working_dir, directory)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created directory: {path}\")\n",
        "\n",
        "# Input data paths\n",
        "train_audio_path = \"/kaggle/input/birdclef-2024/train_audio\"\n",
        "unlabeled_soundscapes_path = \"/kaggle/input/birdclef-2024/unlabeled_soundscapes\"\n",
        "train_metadata_csv_path = \"/kaggle/input/birdclef-2024/train_metadata.csv\"\n",
        "taxonomy_csv_path = \"/kaggle/input/birdclef-2024/eBird_Taxonomy_v2021.csv\"\n",
        "sample_submission_csv_path = \"/kaggle/input/birdclef-2024/sample_submission.csv\"\n",
        "test_soundscapes_path = \"/kaggle/input/birdclef-2024/test_soundscapes\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyzD5bbcai06",
        "outputId": "165a828a-3929-47c8-dc9a-663aad4c105d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /kaggle/working/processed_data\n",
            "Created directory: /kaggle/working/models\n",
            "Created directory: /kaggle/working/submissions\n",
            "Created directory: /kaggle/working/visualizations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert audio to spectrograms\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "augment for processing later\n"
      ],
      "metadata": {
        "id": "itdvR__PJeFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geographic Area of Western Ghats:\n",
        "\n",
        "Latitude: Approximately ranges from 8°N to 21°N.\n",
        "Longitude: Approximately ranges from 72°E to 78°E.\n",
        "\n",
        "We'll add a cushion to these ranges to account for nearby areas and potential data variability or migration patterns slightly beyond these strict boundaries."
      ],
      "metadata": {
        "id": "vxJDsGhfvf2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\n",
        "\n",
        "# Define the geographical bounds of the Western Ghats with some cushion\n",
        "lat_bounds = (8, 21)\n",
        "lon_bounds = (72, 78)\n",
        "\n",
        "# Filter entries based on these bounds\n",
        "western_ghats_metadata = metadata[\n",
        "    (metadata['latitude'] >= lat_bounds[0]) & (metadata['latitude'] <= lat_bounds[1]) &\n",
        "    (metadata['longitude'] >= lon_bounds[0]) & (metadata['longitude'] <= lon_bounds[1])\n",
        "]\n",
        "\n",
        "# Update the filename to point to the spectrogram images expected to be in the processed data directory\n",
        "processed_data_dir = '/kaggle/working/processed_data/'\n",
        "western_ghats_metadata['filename'] = western_ghats_metadata['filename'].apply(\n",
        "    lambda x: processed_data_dir + x.split('/')[-1].replace('.ogg', '_augmented_spectrogram.png')\n",
        ")\n",
        "\n",
        "# Save the filtered metadata\n",
        "western_ghats_metadata.to_csv('/kaggle/working/western_ghats_birds_metadata.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "S3V9eiUMvfOI",
        "outputId": "16ad74cb-29f3-430d-f5fc-be17e34e90de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/birdclef-2024/train_metadata.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-30ef819ef5f6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/birdclef-2024/train_metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the geographical bounds of the Western Ghats with some cushion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlat_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/birdclef-2024/train_metadata.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_audio_augmentation(audio_signal, sr):\n",
        "    time_stretch_factor = np.random.uniform(0.9, 1.1)\n",
        "    audio_signal = librosa.effects.time_stretch(audio_signal, rate=time_stretch_factor)\n",
        "    n_steps = np.random.randint(-2, 3)\n",
        "    audio_signal = librosa.effects.pitch_shift(audio_signal, sr=sr, n_steps=n_steps)\n",
        "    noise_level = np.random.uniform(0.001, 0.005)\n",
        "    noise = np.random.normal(0, noise_level, len(audio_signal))\n",
        "    audio_signal += noise\n",
        "    return audio_signal\n",
        "\n",
        "def create_and_augment_spectrogram(audio_path, save_path):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "    y_augmented = advanced_audio_augmentation(y, sr)\n",
        "    S = librosa.feature.melspectrogram(y=y_augmented, sr=sr)\n",
        "    S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Augmented Mel-frequency spectrogram')\n",
        "    plt.tight_layout()\n",
        "    # Ensure the directory exists before saving\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "def process_filtered_audio_files(metadata, base_audio_dir, processed_data_dir):\n",
        "    for index, row in metadata.iterrows():\n",
        "        audio_path = os.path.join(base_audio_dir, row['filename'])\n",
        "        save_path = os.path.join(processed_data_dir, f\"{os.path.splitext(row['filename'])[0]}_augmented_spectrogram.png\")\n",
        "        if os.path.exists(audio_path):\n",
        "            create_and_augment_spectrogram(audio_path, save_path)\n",
        "        else:\n",
        "            print(f\"File not found: {audio_path}\")\n",
        "\n",
        "# Load metadata\n",
        "metadata_path = '/kaggle/input/birdclef-2024/train_metadata.csv'\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "\n",
        "# Filter metadata for Western Ghats\n",
        "western_ghats_bounds = {'min_latitude': 10.0, 'max_latitude': 20.0, 'min_longitude': 73.0, 'max_longitude': 78.0}\n",
        "filtered_metadata = metadata[\n",
        "    (metadata['latitude'] >= western_ghats_bounds['min_latitude']) &\n",
        "    (metadata['latitude'] <= western_ghats_bounds['max_latitude']) &\n",
        "    (metadata['longitude'] >= western_ghats_bounds['min_longitude']) &\n",
        "    (metadata['longitude'] <= western_ghats_bounds['max_longitude'])\n",
        "]\n",
        "\n",
        "base_audio_dir = '/kaggle/input/birdclef-2024/train_audio'\n",
        "processed_data_dir = '/kaggle/working/processed_data'\n",
        "process_filtered_audio_files(filtered_metadata, base_audio_dir, processed_data_dir)\n"
      ],
      "metadata": {
        "id": "EHYd5Ewfz6TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter metadata to include relevant columns\n",
        "filtered_metadata_df = train_metadata[['primary_label', 'latitude', 'longitude', 'url', 'filename']]\n",
        "\n",
        "# Extract features to be normalized and standardized\n",
        "features_to_scale = ['latitude', 'longitude']\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler to the features and transform them\n",
        "filtered_metadata_df.loc[:, features_to_scale] = scaler.fit_transform(filtered_metadata_df.loc[:, features_to_scale])\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder to the primary_label column and transform it\n",
        "filtered_metadata_df['primary_label'] = label_encoder.fit_transform(filtered_metadata_df['primary_label']) + 1\n",
        "\n",
        "# Print the mapping between original labels and encoded values\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label Encoding Mapping:\")\n",
        "print(label_mapping)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(\"Processed DataFrame:\")\n",
        "print(filtered_metadata_df.head())"
      ],
      "metadata": {
        "id": "4qZs69_wey5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the spectrogram of the processed sample\n",
        "sample_audio_file = '/kaggle/input/birdclef-2024/train_audio/asbfly/XC134896.ogg'  # Example path, adjust as needed\n",
        "\n",
        "# Loading and processing the audio file to create a spectrogram\n",
        "y, sr = librosa.load(sample_audio_file, sr=None)\n",
        "S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel-frequency spectrogram of Asbfly')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uMuhrwPhyjk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets check out the metadata"
      ],
      "metadata": {
        "id": "pdc1l2tmO1pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize and standardize features"
      ],
      "metadata": {
        "id": "5dhM3bHgdZ7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "extra augmentation, lets go ahead and add in a function to add in some background noise to the training set and pitch shifting."
      ],
      "metadata": {
        "id": "Tova1CYJsBfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Development\n",
        "Define the CRNN model arhitecture\n",
        "Compile the model with appropriate loss function and optimizer"
      ],
      "metadata": {
        "id": "_pmb-eOeJgmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_crnn_model(input_shape=(128, 128, 1), num_metadata_features=2, time_steps=1):\n",
        "    # Adjust input shape to include time dimension\n",
        "    adjusted_input_shape = (time_steps,) + input_shape  # Now it's (time_steps, 128, 128, 1)\n",
        "\n",
        "    # Spectrogram input branch\n",
        "    spectrogram_input = Input(shape=adjusted_input_shape, name='spectrogram_input')\n",
        "    x = TimeDistributed(Conv2D(32, kernel_size=(3, 3), activation='relu'))(spectrogram_input)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # LSTM layer to handle temporal features\n",
        "    x = LSTM(64, return_sequences=False)(x)\n",
        "\n",
        "    # Metadata input branch\n",
        "    metadata_input = Input(shape=(num_metadata_features,), name='metadata_input')\n",
        "    metadata_dense = Dense(32, activation='relu')(metadata_input)\n",
        "\n",
        "    # Combining both branches\n",
        "    combined = concatenate([x, metadata_dense])\n",
        "    combined = Dense(64, activation='relu')(combined)\n",
        "    combined = Dropout(0.5)(combined)\n",
        "    outputs = Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "    model = Model(inputs=[spectrogram_input, metadata_input], outputs=outputs)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vyoEjjahJgvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_metadata_df['filename'] = filtered_metadata_df['filename'].apply(\n",
        "    lambda x: os.path.join(processed_data_dir, x.replace('.ogg', '_augmented_spectrogram.png'))\n",
        ")"
      ],
      "metadata": {
        "id": "hcvBKathJg_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, file_paths, labels, batch_size=32, dim=(128, 128), n_channels=1, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.file_paths = file_paths\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.file_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_paths = [self.file_paths[k] for k in indexes]\n",
        "        X, y = self.__data_generation(batch_paths)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.file_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, file_paths_temp):\n",
        "        'Generates data containing batch_size samples'\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "        actual_batch_size = 0  # To handle cases where files might be missing\n",
        "\n",
        "        for i, file_path in enumerate(file_paths_temp):\n",
        "          if not os.path.exists(file_path):\n",
        "            print(f\"File not found: {file_path}\")\n",
        "            continue  # Skip this file if not found\n",
        "          img = load_img(file_path, target_size=self.dim, color_mode='grayscale')\n",
        "          img = img_to_array(img) / 255.0\n",
        "          X[actual_batch_size,] = img\n",
        "          y[actual_batch_size] = self.labels[i]\n",
        "          actual_batch_size += 1\n",
        "\n",
        "        return X[:actual_batch_size], np.array(y[:actual_batch_size])\n"
      ],
      "metadata": {
        "id": "SVhjPritDIzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Verify files exist\n",
        "processed_data_dir = '/kaggle/working/processed_data'\n",
        "all_files = os.listdir(processed_data_dir)\n",
        "\n",
        "if not all_files:\n",
        "    print(\"No files in the directory.\")\n",
        "else:\n",
        "    print(f\"Number of files in the directory: {len(all_files)}\")\n",
        "    print(\"Sample files:\", all_files[:5])  # Print first 5 file names to check\n"
      ],
      "metadata": {
        "id": "xMigSLrZcbYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Training\n",
        "Since we are using images, we will need to set up datagenerators and then we can Split data into training and validation sets\n"
      ],
      "metadata": {
        "id": "dev6z8eJJg3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_files = [f for f in filtered_metadata_df['filename'] if not os.path.exists(f)]\n",
        "if missing_files:\n",
        "    print(f\"Missing files: {missing_files[:5]}\")  # Print first few missing files\n",
        "else:\n",
        "    print(\"All files accounted for.\")"
      ],
      "metadata": {
        "id": "Kr0DnyqFdoub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the model\n",
        "model = build_crnn_model(input_shape=(128, 128, 1))\n",
        "\n",
        "# Split data into training and validation sets correctly referencing 'filename' and 'primary_label'\n",
        "X = filtered_metadata_df['filename'].values\n",
        "y = filtered_metadata_df['primary_label'].values\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Use updated parameters\n",
        "params = {'dim': (128, 128), 'batch_size': 32, 'n_channels': 1, 'shuffle': True}\n",
        "\n",
        "training_generator = DataGenerator(X_train, y_train, **params)\n",
        "validation_generator = DataGenerator(X_val, y_val, **params)\n",
        "\n",
        "# Model training\n",
        "history = model.fit(training_generator, validation_data=validation_generator, epochs=100, callbacks=callbacks, verbose=1)\n"
      ],
      "metadata": {
        "id": "n1zTdWBMLqNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Callbacks"
      ],
      "metadata": {
        "id": "Ku7J9S6sEjsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(base_working_dir, 'models/best_model.h5'),\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.1,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n"
      ],
      "metadata": {
        "id": "YpI3gvV8Fvq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###build & train the model"
      ],
      "metadata": {
        "id": "R2KsWKfIEn6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training with properly defined callbacks and using the DataGenerator\n",
        "history = model.fit(\n",
        "    training_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=100,  # Adjust epochs and batch_size as needed\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "rxDaM3dAGK5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "valuation and Testing\n",
        "Evaluate the model on the test set or soundscapes\n",
        "Apply any post-processing needed for predictions\n",
        "Analyze model performance metrics"
      ],
      "metadata": {
        "id": "Yx8esY3BJhHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# After training, summarize the model\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JoouEkpoJ6SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimization and Tuning\n",
        "Fine-tune model parameters\n",
        "Experiment with different architectures or features\n",
        "Re-train and evaluate the model"
      ],
      "metadata": {
        "id": "A0BlKhYbJ5CI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iit6YwOFJ5d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "submission\n"
      ],
      "metadata": {
        "id": "mt4Ps4x-J68w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DX9YqvWbGHNa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}