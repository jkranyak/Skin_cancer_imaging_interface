{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/jkranyak/project_3/blob/main/project3.ipynb",
      "authorship_tag": "ABX9TyMItqcUv/n5twCsooYFunjM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkranyak/project_3/blob/main/master_project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Project 3: Team Decision Makers***\n",
        "## Jesse Kranyak &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  Siriesha Mandava\n",
        "## Mohamed Altoobli &nbsp;&nbsp;   Jeffery Boczkaja"
      ],
      "metadata": {
        "id": "TEmF0Os8F3x8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skin Lesion Classification Project\n",
        "This project focuses on classifying skin lesions using deep learning techniques. The dataset used for training and testing the model is the ISIC 2019 dataset, which consists of images of various skin lesions along with associated metadata."
      ],
      "metadata": {
        "id": "7nlRnjPiF9fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Initial setup and Library installation\n",
        "Colab requires many libraries to be installed for use everytime you run a new notebook!"
      ],
      "metadata": {
        "id": "N91NvKfoGB37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install isic-cli\n",
        "!pip install isic-cli\n",
        "!pip install kaggle\n",
        "!pip install imblearn\n",
        "!pip install optuna\n",
        "!pip install tqdm\n",
        "!pip install optuna-integration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9dKgTXpPs_K",
        "outputId": "7dcd1b68-195e-48b3-d804-8d5c8cd35621"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Collecting isic-cli\n",
            "  Downloading isic_cli-10.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.1.7)\n",
            "Collecting django-s3-file-field-client>=1.0.0 (from isic-cli)\n",
            "  Downloading django_s3_file_field_client-1.0.1-py3-none-any.whl (3.2 kB)\n",
            "Collecting girder-cli-oauth-client<1.0.0 (from isic-cli)\n",
            "  Downloading girder_cli_oauth_client-0.4.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting humanize (from isic-cli)\n",
            "  Downloading humanize-4.9.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isic-metadata>=1.2.0 (from isic-cli)\n",
            "  Downloading isic_metadata-1.5.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from isic-cli) (8.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from isic-cli) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from isic-cli) (2.31.0)\n",
            "Collecting retryable-requests (from isic-cli)\n",
            "  Downloading retryable_requests-0.1.2-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from isic-cli) (13.7.1)\n",
            "Collecting sentry-sdk (from isic-cli)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.2.3)\n",
            "Collecting authlib (from girder-cli-oauth-client<1.0.0->isic-cli)\n",
            "  Downloading Authlib-1.3.0-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyxdg (from girder-cli-oauth-client<1.0.0->isic-cli)\n",
            "  Downloading pyxdg-0.28-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.4 in /usr/local/lib/python3.10/dist-packages (from isic-metadata>=1.2.0->isic-cli) (2.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2024.2.2)\n",
            "Collecting requests-toolbelt (from retryable-requests->isic-cli)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->isic-cli) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (4.11.0)\n",
            "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from authlib->girder-cli-oauth-client<1.0.0->isic-cli) (3.4.8)\n",
            "Installing collected packages: pyxdg, sentry-sdk, humanize, authlib, requests-toolbelt, girder-cli-oauth-client, django-s3-file-field-client, retryable-requests, isic-metadata, isic-cli\n",
            "Successfully installed authlib-1.3.0 django-s3-file-field-client-1.0.1 girder-cli-oauth-client-0.4.0 humanize-4.9.0 isic-cli-10.0.0 isic-metadata-1.5.0 pyxdg-0.28 requests-toolbelt-1.0.0 retryable-requests-0.1.2 sentry-sdk-1.45.0\n",
            "Requirement already satisfied: isic-cli in /usr/local/lib/python3.10/dist-packages (10.0.0)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.1.7)\n",
            "Requirement already satisfied: django-s3-file-field-client>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.0.1)\n",
            "Requirement already satisfied: girder-cli-oauth-client<1.0.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (0.4.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from isic-cli) (4.9.0)\n",
            "Requirement already satisfied: isic-metadata>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.5.0)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from isic-cli) (8.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from isic-cli) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from isic-cli) (2.31.0)\n",
            "Requirement already satisfied: retryable-requests in /usr/local/lib/python3.10/dist-packages (from isic-cli) (0.1.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from isic-cli) (13.7.1)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.45.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.2.3)\n",
            "Requirement already satisfied: authlib in /usr/local/lib/python3.10/dist-packages (from girder-cli-oauth-client<1.0.0->isic-cli) (1.3.0)\n",
            "Requirement already satisfied: pyxdg in /usr/local/lib/python3.10/dist-packages (from girder-cli-oauth-client<1.0.0->isic-cli) (0.28)\n",
            "Requirement already satisfied: pydantic>=2.4 in /usr/local/lib/python3.10/dist-packages (from isic-metadata>=1.2.0->isic-cli) (2.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2024.2.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from retryable-requests->isic-cli) (1.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->isic-cli) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (4.11.0)\n",
            "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from authlib->girder-cli-oauth-client<1.0.0->isic-cli) (3.4.8)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting imbalanced-learn (from imblearn)\n",
            "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.0/258.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.4.0)\n",
            "Installing collected packages: imbalanced-learn, imblearn\n",
            "Successfully installed imbalanced-learn-0.12.2 imblearn-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from IPython.display import Image\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import auc, classification_report, confusion_matrix, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.utils import resample\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Dense, Dropout,\n",
        "                                     Flatten, GlobalAveragePooling2D,\n",
        "                                     Input, MaxPooling2D, concatenate)\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.utils import Sequence, plot_model, to_categorical\n",
        "\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "Qydcku5hO9JU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Query the Dataset\n",
        "In this step, we interact with the ISIC API to query the dataset directly. We use specific API endpoints to perform operations such as listing available images, retrieving image metadata, and downloading images. This section covers setting up API requests, managing data retrieval, and handling query parameters to filter the dataset based on criteria such as diagnosis, image type, or other metadata."
      ],
      "metadata": {
        "id": "oonk5DVpQvQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! isic user login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW33o_VNdZdO",
        "outputId": "43e4fecd-66f8-406e-8006-2186c6d72936"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visit the following url in a browser:\n",
            "https://api.isic-archive.com/oauth/authorize?response_type=code&client_id=RpCzc4hFjv5gOJdM2DM2nBdokOviOh5ne63Tpn7Q&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&state=FX7JwnPRV1QTePPgPQ8zzLI3u362HR&code_challenge=P4zXBIfazY65I2l-ppCXQsSgzUR3wW3jcaRz7rHVLtQ&code_challenge_method=S256\n",
            "enter the code shown in your browser: RKWDN4MFVruyytmS32P7pNXwMQTx9x\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!isic collection list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHkk6C7UfgMk",
        "outputId": "29c34f06-4d52-45cc-a23e-14acc83b83b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mID \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName                                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPublic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPinned\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLocked\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDOI            \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
            "│ 249 │ BCN20000                                      │ True   │ False  │ False  │ None            │\n",
            "│ 61  │ Challenge 2016: Test                          │ True   │ True   │ True   │ None            │\n",
            "│ 74  │ Challenge 2016: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 69  │ Challenge 2017: Test                          │ True   │ True   │ True   │ None            │\n",
            "│ 60  │ Challenge 2017: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 71  │ Challenge 2017: Validation                    │ True   │ True   │ True   │ None            │\n",
            "│ 64  │ Challenge 2018: Task 1-2: Test                │ True   │ True   │ True   │ None            │\n",
            "│ 63  │ Challenge 2018: Task 1-2: Training            │ True   │ True   │ True   │ None            │\n",
            "│ 62  │ Challenge 2018: Task 1-2: Validation          │ True   │ True   │ True   │ None            │\n",
            "│ 67  │ Challenge 2018: Task 3: Test                  │ True   │ True   │ True   │ None            │\n",
            "│ 66  │ Challenge 2018: Task 3: Training              │ True   │ True   │ True   │ None            │\n",
            "│ 73  │ Challenge 2018: Task 3: Validation            │ True   │ True   │ True   │ None            │\n",
            "│ 65  │ Challenge 2019: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 70  │ Challenge 2020: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 97  │ Collection for ISBI 2016: 100 Lesion          │ True   │ False  │ True   │ None            │\n",
            "│     │ Classification                                │        │        │        │                 │\n",
            "│ 216 │ Consecutive biopsies for melanoma across year │ True   │ False  │ True   │ 10.34970/151324 │\n",
            "│     │ 2020                                          │        │        │        │                 │\n",
            "│ 75  │ Consumer AI apps                              │ True   │ False  │ True   │ 10.34970/401946 │\n",
            "│ 166 │ EASY Dermoscopy Expert Agreement Study        │ True   │ False  │ False  │ None            │\n",
            "│ 212 │ HAM10000                                      │ True   │ True   │ True   │ None            │\n",
            "│ 175 │ HIBA Skin Lesions                             │ True   │ False  │ True   │ 10.34970/559884 │\n",
            "│ 251 │ Hospital Italiano de Buenos Aires - Skin      │ True   │ False  │ True   │ 10.34970/587329 │\n",
            "│     │ Lesions Images (2019-2022)                    │        │        │        │                 │\n",
            "│ 176 │ Hospital Italiano de Buenos Aires Skin        │ True   │ False  │ True   │ 10.34970/432362 │\n",
            "│     │ Lesions                                       │        │        │        │                 │\n",
            "│ 383 │ Longitudinal Images with Various Types for    │ True   │ False  │ False  │ None            │\n",
            "│     │ Lesion Viewer Testing                         │        │        │        │                 │\n",
            "│ 217 │ Longitudinal overview images of posterior     │ True   │ False  │ True   │ 10.34970/630662 │\n",
            "│     │ trunks                                        │        │        │        │                 │\n",
            "│ 289 │ MSK-1                                         │ True   │ False  │ True   │ None            │\n",
            "│ 290 │ MSK-2                                         │ True   │ False  │ True   │ None            │\n",
            "│ 288 │ MSK-3                                         │ True   │ False  │ True   │ None            │\n",
            "│ 287 │ MSK-4                                         │ True   │ False  │ True   │ None            │\n",
            "│ 286 │ MSK-5                                         │ True   │ False  │ True   │ None            │\n",
            "│ 163 │ MSKCC Consecutive biopsies across year        │ True   │ False  │ True   │ None            │\n",
            "│     │ 2020_cohort                                   │        │        │        │                 │\n",
            "│ 77  │ Melanocytic lesions used for dermoscopic      │ True   │ False  │ True   │ 10.34970/108631 │\n",
            "│     │ feature annotations                           │        │        │        │                 │\n",
            "│ 294 │ Melanoma and Nevus Dermoscopy Images with     │ True   │ False  │ True   │ 10.34970/277003 │\n",
            "│     │ Confirmed Histopathological Diagnosis         │        │        │        │                 │\n",
            "│ 215 │ Newly-acquired and longer-existing acquired   │ True   │ False  │ True   │ 10.34970/408649 │\n",
            "│     │ melanoma and nevi                             │        │        │        │                 │\n",
            "│ 218 │ PROVe-AI                                      │ True   │ True   │ True   │ 10.34970/576276 │\n",
            "│ 328 │ Repeated Dermoscopic Images of Melanocytic    │ True   │ False  │ True   │ 10.34970/560760 │\n",
            "│     │ Lesions                                       │        │        │        │                 │\n",
            "│ 293 │ SONIC                                         │ True   │ False  │ True   │ None            │\n",
            "│ 292 │ UDA-1                                         │ True   │ False  │ True   │ None            │\n",
            "│ 291 │ UDA-2                                         │ True   │ False  │ True   │ None            │\n",
            "│ 285 │ lesions                                       │ True   │ False  │ False  │ None            │\n",
            "│ 172 │ screenshot_public_230207                      │ True   │ False  │ False  │ None            │\n",
            "└─────┴───────────────────────────────────────────────┴────────┴────────┴────────┴─────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPl5l2XioO78",
        "outputId": "d54f8741-1e6b-477c-a5f2-e889d63e0fa5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to have kaggle API key ready to go in .json file. Place it into the 'content' folder."
      ],
      "metadata": {
        "id": "BxB2rplSGL90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Necessary Mdels and Datasets are available in Git Repo folder:\n",
        "\n",
        "#####saved_models_for_final_project"
      ],
      "metadata": {
        "id": "2RJkPIfDe5gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the current path of kaggle.json file\n",
        "current_path = '/content/kaggle.json'\n",
        "\n",
        "# Desired path where the Kaggle API expects the kaggle.json file\n",
        "desired_path = '/root/.kaggle/kaggle.json'\n",
        "\n",
        "if os.path.exists(current_path):\n",
        "    os.makedirs(os.path.dirname(desired_path), exist_ok=True)\n",
        "    os.rename(current_path, desired_path)\n",
        "\n",
        "    # Set the file's permissions to avoid a permissions error\n",
        "    os.chmod(desired_path, 0o600)\n",
        "else:\n",
        "    print(f\"Error: '{current_path}' does not exist. Please upload the file.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GxvlgkCfxBCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32484eca-0123-497d-8404-c8b8066db36f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: '/content/kaggle.json' does not exist. Please upload the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "find the data set on kaggle\n",
        "\n",
        "Comment out if running and already have downloaded images"
      ],
      "metadata": {
        "id": "MZqQai6mypGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/isic-2019\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPsM6paiysEU",
        "outputId": "67944313-f5cf-4fe0-c85f-00ef613bbcd5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading isic-2019.zip to /content\n",
            "100% 9.09G/9.10G [01:11<00:00, 118MB/s]\n",
            "100% 9.10G/9.10G [01:11<00:00, 137MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q isic-2019.zip\n"
      ],
      "metadata": {
        "id": "XlWuOA7dyxnf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Data Preparation and Preprocessing DICOM Images\n",
        "This step involves handling the metadata for the ISIC 2019 dataset, which which will be used for training, testing, and validation data. The processes highlighted in this step are crucial for understanding the dataset's structure and preparing it for subsequent analysis.\n",
        "\n",
        "IMPORTANT NOTE we removed the DICOM image sets from this project for ease of processing on our colab wallets...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1_BWFCj4QINb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metadata\n",
        "metadata = pd.read_csv('/content/ISIC_2019_Training_Metadata.csv')\n",
        "\n",
        "# Display the first few rows of each DataFrame\n",
        "metadata.head()\n"
      ],
      "metadata": {
        "id": "HXZ38h4-2j6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = pd.read_csv('/content/ISIC_2019_Training_GroundTruth.csv')\n",
        "ground_truth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DWo9AD0P5VwB",
        "outputId": "1bc1e00a-cd99-4a79-c15a-927dd307f453"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              image  MEL   NV  BCC   AK  BKL   DF  VASC  SCC  UNK\n",
              "0      ISIC_0000000  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "1      ISIC_0000001  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "2      ISIC_0000002  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "3      ISIC_0000003  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "4      ISIC_0000004  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "...             ...  ...  ...  ...  ...  ...  ...   ...  ...  ...\n",
              "25326  ISIC_0073247  0.0  0.0  1.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "25327  ISIC_0073248  0.0  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0\n",
              "25328  ISIC_0073249  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "25329  ISIC_0073251  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "25330  ISIC_0073254  0.0  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0\n",
              "\n",
              "[25331 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1ae9609-e48d-4ce7-b0a6-57d157b962db\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>MEL</th>\n",
              "      <th>NV</th>\n",
              "      <th>BCC</th>\n",
              "      <th>AK</th>\n",
              "      <th>BKL</th>\n",
              "      <th>DF</th>\n",
              "      <th>VASC</th>\n",
              "      <th>SCC</th>\n",
              "      <th>UNK</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_0000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_0000001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_0000002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_0000003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0000004</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25326</th>\n",
              "      <td>ISIC_0073247</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25327</th>\n",
              "      <td>ISIC_0073248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25328</th>\n",
              "      <td>ISIC_0073249</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25329</th>\n",
              "      <td>ISIC_0073251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25330</th>\n",
              "      <td>ISIC_0073254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25331 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1ae9609-e48d-4ce7-b0a6-57d157b962db')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1ae9609-e48d-4ce7-b0a6-57d157b962db button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1ae9609-e48d-4ce7-b0a6-57d157b962db');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3814f622-56e7-4159-a690-b2423e77af4e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3814f622-56e7-4159-a690-b2423e77af4e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3814f622-56e7-4159-a690-b2423e77af4e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_55a34966-dd15-473b-a21f-01dab607120a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ground_truth')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_55a34966-dd15-473b-a21f-01dab607120a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('ground_truth');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ground_truth",
              "summary": "{\n  \"name\": \"ground_truth\",\n  \"rows\": 25331,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25331,\n        \"samples\": [\n          \"ISIC_0000360\",\n          \"ISIC_0031596\",\n          \"ISIC_0069981\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3829544511327576,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49994146244558546,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33760719760553826,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AK\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1818149205142737,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BKL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30473197962790843,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0966769248200901,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"VASC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09944041938641913,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15549302363605394,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UNK\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hIRZpHWLIVzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV files\n",
        "ground_truth = pd.read_csv('/content/ISIC_2019_Training_GroundTruth.csv')\n",
        "metadata = pd.read_csv('/content/ISIC_2019_Training_Metadata.csv')\n",
        "\n",
        "image_dir = Path('/content/ISIC_2019_Training_Images')\n",
        "ground_truth['image_path'] = ground_truth['image'].apply(lambda x: image_dir / f\"{x}.jpg\")\n",
        "\n",
        "# Merge the ground_truth with metadata if necessary\n",
        "full_metadata = pd.merge(ground_truth, metadata, on='image', how='left')  # Adjust 'on' parameter as needed\n",
        "full_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "gdODxFjP4Ydi",
        "outputId": "06d0a4fe-652f-4935-978e-1b48712ae8e0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              image  MEL   NV  BCC   AK  BKL   DF  VASC  SCC  UNK  \\\n",
              "0      ISIC_0000000  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "1      ISIC_0000001  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "2      ISIC_0000002  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "3      ISIC_0000003  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "4      ISIC_0000004  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "...             ...  ...  ...  ...  ...  ...  ...   ...  ...  ...   \n",
              "25326  ISIC_0073247  0.0  0.0  1.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "25327  ISIC_0073248  0.0  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0   \n",
              "25328  ISIC_0073249  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "25329  ISIC_0073251  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
              "25330  ISIC_0073254  0.0  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0   \n",
              "\n",
              "                                              image_path  age_approx  \\\n",
              "0      /content/ISIC_2019_Training_Images/ISIC_000000...        55.0   \n",
              "1      /content/ISIC_2019_Training_Images/ISIC_000000...        30.0   \n",
              "2      /content/ISIC_2019_Training_Images/ISIC_000000...        60.0   \n",
              "3      /content/ISIC_2019_Training_Images/ISIC_000000...        30.0   \n",
              "4      /content/ISIC_2019_Training_Images/ISIC_000000...        80.0   \n",
              "...                                                  ...         ...   \n",
              "25326  /content/ISIC_2019_Training_Images/ISIC_007324...        85.0   \n",
              "25327  /content/ISIC_2019_Training_Images/ISIC_007324...        65.0   \n",
              "25328  /content/ISIC_2019_Training_Images/ISIC_007324...        70.0   \n",
              "25329  /content/ISIC_2019_Training_Images/ISIC_007325...        55.0   \n",
              "25330  /content/ISIC_2019_Training_Images/ISIC_007325...        50.0   \n",
              "\n",
              "      anatom_site_general    lesion_id     sex  \n",
              "0          anterior torso          NaN  female  \n",
              "1          anterior torso          NaN  female  \n",
              "2         upper extremity          NaN  female  \n",
              "3         upper extremity          NaN    male  \n",
              "4         posterior torso          NaN    male  \n",
              "...                   ...          ...     ...  \n",
              "25326           head/neck  BCN_0003925  female  \n",
              "25327      anterior torso  BCN_0001819    male  \n",
              "25328     lower extremity  BCN_0001085    male  \n",
              "25329         palms/soles  BCN_0002083  female  \n",
              "25330     upper extremity  BCN_0001079    male  \n",
              "\n",
              "[25331 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac22779a-850d-487a-a3b5-3ee9967afc41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>MEL</th>\n",
              "      <th>NV</th>\n",
              "      <th>BCC</th>\n",
              "      <th>AK</th>\n",
              "      <th>BKL</th>\n",
              "      <th>DF</th>\n",
              "      <th>VASC</th>\n",
              "      <th>SCC</th>\n",
              "      <th>UNK</th>\n",
              "      <th>image_path</th>\n",
              "      <th>age_approx</th>\n",
              "      <th>anatom_site_general</th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_0000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_000000...</td>\n",
              "      <td>55.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>NaN</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_0000001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_000000...</td>\n",
              "      <td>30.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>NaN</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_0000002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_000000...</td>\n",
              "      <td>60.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>NaN</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_0000003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_000000...</td>\n",
              "      <td>30.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>NaN</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0000004</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_000000...</td>\n",
              "      <td>80.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>NaN</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25326</th>\n",
              "      <td>ISIC_0073247</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_007324...</td>\n",
              "      <td>85.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>BCN_0003925</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25327</th>\n",
              "      <td>ISIC_0073248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_007324...</td>\n",
              "      <td>65.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>BCN_0001819</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25328</th>\n",
              "      <td>ISIC_0073249</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_007324...</td>\n",
              "      <td>70.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>BCN_0001085</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25329</th>\n",
              "      <td>ISIC_0073251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_007325...</td>\n",
              "      <td>55.0</td>\n",
              "      <td>palms/soles</td>\n",
              "      <td>BCN_0002083</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25330</th>\n",
              "      <td>ISIC_0073254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/content/ISIC_2019_Training_Images/ISIC_007325...</td>\n",
              "      <td>50.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>BCN_0001079</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25331 rows × 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac22779a-850d-487a-a3b5-3ee9967afc41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac22779a-850d-487a-a3b5-3ee9967afc41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac22779a-850d-487a-a3b5-3ee9967afc41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1725d8e8-6071-49ed-9814-742c2014ae83\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1725d8e8-6071-49ed-9814-742c2014ae83')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1725d8e8-6071-49ed-9814-742c2014ae83 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c1e34dbc-fa8e-4c0c-b568-dcdafa10bc80\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('full_metadata')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c1e34dbc-fa8e-4c0c-b568-dcdafa10bc80 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('full_metadata');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "full_metadata",
              "summary": "{\n  \"name\": \"full_metadata\",\n  \"rows\": 25331,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25331,\n        \"samples\": [\n          \"ISIC_0000360\",\n          \"ISIC_0031596\",\n          \"ISIC_0069981\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3829544511327576,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49994146244558546,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33760719760553826,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AK\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1818149205142737,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BKL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30473197962790843,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0966769248200901,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"VASC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09944041938641913,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15549302363605394,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UNK\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25331,\n        \"samples\": [\n          \"/content/ISIC_2019_Training_Images/ISIC_0000360.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age_approx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.130971451746515,\n        \"min\": 0.0,\n        \"max\": 85.0,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          55.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"anatom_site_general\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"upper extremity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lesion_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11847,\n        \"samples\": [\n          \"HAM_0004599\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the image paths for the first few entries\n",
        "print(full_metadata['image_path'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bzo5qSzHAk2",
        "outputId": "458c91c7-4dfa-4fa9-820d-1b6f9abc8d8b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    /content/ISIC_2019_Training_Images/ISIC_000000...\n",
            "1    /content/ISIC_2019_Training_Images/ISIC_000000...\n",
            "2    /content/ISIC_2019_Training_Images/ISIC_000000...\n",
            "3    /content/ISIC_2019_Training_Images/ISIC_000000...\n",
            "4    /content/ISIC_2019_Training_Images/ISIC_000000...\n",
            "Name: image_path, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct the base path in 'image_path' column\n",
        "correct_base_path = \"/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\n",
        "\n",
        "full_metadata['image_path'] = full_metadata['image'].apply(lambda x: f\"{correct_base_path}/{x}.jpg\")\n",
        "\n",
        "# Verify the correction by printing the first few entries again\n",
        "print(full_metadata['image_path'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FDMb1wGHk3d",
        "outputId": "a4c2c3b7-9ca7-4581-fe24-c2117fc435f6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    /content/ISIC_2019_Training_Input/ISIC_2019_Tr...\n",
            "1    /content/ISIC_2019_Training_Input/ISIC_2019_Tr...\n",
            "2    /content/ISIC_2019_Training_Input/ISIC_2019_Tr...\n",
            "3    /content/ISIC_2019_Training_Input/ISIC_2019_Tr...\n",
            "4    /content/ISIC_2019_Training_Input/ISIC_2019_Tr...\n",
            "Name: image_path, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = full_metadata['image_path']"
      ],
      "metadata": {
        "id": "0BjtJ7JO9mHO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values for 'age_approx' with its median\n",
        "full_metadata['age_approx'].fillna(full_metadata['age_approx'].median(), inplace=True)\n",
        "\n",
        "# For categorical data, fill missing values with 'unknown'\n",
        "full_metadata['anatom_site_general'].fillna('unknown', inplace=True)\n",
        "full_metadata['sex'].fillna('unknown', inplace=True)\n"
      ],
      "metadata": {
        "id": "-W0QskH29_-g"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_metadata = pd.get_dummies(full_metadata, columns=['sex'])\n"
      ],
      "metadata": {
        "id": "sAeGNqs--IWq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values in 'lesion_id'\n",
        "full_metadata['lesion_id'].fillna('unknown', inplace=True)\n",
        "\n",
        "# Convert booleans to integers\n",
        "bool_columns = ['sex_female', 'sex_male', 'sex_unknown']\n",
        "full_metadata[bool_columns] = full_metadata[bool_columns].astype(int)\n",
        "\n",
        "# One-hot encoding of 'anatom_site_general'\n",
        "full_metadata = pd.get_dummies(full_metadata, columns=['anatom_site_general'])\n",
        "\n",
        "# Ensure all floating point columns are float32\n",
        "float_columns = full_metadata.select_dtypes(include=['float64']).columns\n",
        "full_metadata[float_columns] = full_metadata[float_columns].astype('float32')\n",
        "\n",
        "# Convert boolean columns from one-hot encoding to integers\n",
        "for col in full_metadata.columns:\n",
        "    if full_metadata[col].dtype == 'bool':\n",
        "        full_metadata[col] = full_metadata[col].astype(int)\n",
        "\n",
        "# Normalize the 'age_approx' column (Optional: based on model requirements)\n",
        "full_metadata['age_approx'] = full_metadata['age_approx'].astype('float32') / 100\n",
        "\n",
        "full_metadata.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCkE5qqErf8e",
        "outputId": "6c0d7f2d-284c-4934-f936-fe864a864d41"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25331 entries, 0 to 25330\n",
            "Data columns (total 25 columns):\n",
            " #   Column                               Non-Null Count  Dtype  \n",
            "---  ------                               --------------  -----  \n",
            " 0   image                                25331 non-null  object \n",
            " 1   MEL                                  25331 non-null  float32\n",
            " 2   NV                                   25331 non-null  float32\n",
            " 3   BCC                                  25331 non-null  float32\n",
            " 4   AK                                   25331 non-null  float32\n",
            " 5   BKL                                  25331 non-null  float32\n",
            " 6   DF                                   25331 non-null  float32\n",
            " 7   VASC                                 25331 non-null  float32\n",
            " 8   SCC                                  25331 non-null  float32\n",
            " 9   UNK                                  25331 non-null  float32\n",
            " 10  image_path                           25331 non-null  object \n",
            " 11  age_approx                           25331 non-null  float32\n",
            " 12  lesion_id                            25331 non-null  object \n",
            " 13  sex_female                           25331 non-null  int64  \n",
            " 14  sex_male                             25331 non-null  int64  \n",
            " 15  sex_unknown                          25331 non-null  int64  \n",
            " 16  anatom_site_general_anterior torso   25331 non-null  bool   \n",
            " 17  anatom_site_general_head/neck        25331 non-null  bool   \n",
            " 18  anatom_site_general_lateral torso    25331 non-null  bool   \n",
            " 19  anatom_site_general_lower extremity  25331 non-null  bool   \n",
            " 20  anatom_site_general_oral/genital     25331 non-null  bool   \n",
            " 21  anatom_site_general_palms/soles      25331 non-null  bool   \n",
            " 22  anatom_site_general_posterior torso  25331 non-null  bool   \n",
            " 23  anatom_site_general_unknown          25331 non-null  bool   \n",
            " 24  anatom_site_general_upper extremity  25331 non-null  bool   \n",
            "dtypes: bool(9), float32(10), int64(3), object(3)\n",
            "memory usage: 2.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_metadata.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVb3nRrPtujM",
        "outputId": "8d150f2b-8895-4c04-f6a5-d374c06ead43"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25331 entries, 0 to 25330\n",
            "Data columns (total 25 columns):\n",
            " #   Column                               Non-Null Count  Dtype  \n",
            "---  ------                               --------------  -----  \n",
            " 0   image                                25331 non-null  object \n",
            " 1   MEL                                  25331 non-null  float32\n",
            " 2   NV                                   25331 non-null  float32\n",
            " 3   BCC                                  25331 non-null  float32\n",
            " 4   AK                                   25331 non-null  float32\n",
            " 5   BKL                                  25331 non-null  float32\n",
            " 6   DF                                   25331 non-null  float32\n",
            " 7   VASC                                 25331 non-null  float32\n",
            " 8   SCC                                  25331 non-null  float32\n",
            " 9   UNK                                  25331 non-null  float32\n",
            " 10  image_path                           25331 non-null  object \n",
            " 11  age_approx                           25331 non-null  float32\n",
            " 12  lesion_id                            25331 non-null  object \n",
            " 13  sex_female                           25331 non-null  float32\n",
            " 14  sex_male                             25331 non-null  float32\n",
            " 15  sex_unknown                          25331 non-null  float32\n",
            " 16  anatom_site_general_anterior torso   25331 non-null  float32\n",
            " 17  anatom_site_general_head/neck        25331 non-null  float32\n",
            " 18  anatom_site_general_lateral torso    25331 non-null  float32\n",
            " 19  anatom_site_general_lower extremity  25331 non-null  float32\n",
            " 20  anatom_site_general_oral/genital     25331 non-null  float32\n",
            " 21  anatom_site_general_palms/soles      25331 non-null  float32\n",
            " 22  anatom_site_general_posterior torso  25331 non-null  float32\n",
            " 23  anatom_site_general_unknown          25331 non-null  float32\n",
            " 24  anatom_site_general_upper extremity  25331 non-null  float32\n",
            "dtypes: float32(22), object(3)\n",
            "memory usage: 2.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned and encoded DataFrame to a CSV file\n",
        "full_metadata.to_csv('/content/full_metadata.csv', index=False)"
      ],
      "metadata": {
        "id": "ih6nNMrkYDKB"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Experimenting on the TTS&V\n",
        "\n",
        "### Splitting Data into Training, Validation, and Testing Sets\n",
        "\n",
        "The dataset is initially split into training and temporary sets, with the temporary set reserved for further division into validation and test sets. This method ensures that the model can be trained extensively, validated accurately, and finally tested to evaluate its performance on unseen data.\n",
        "\n",
        "##results before image input augmentation part 2\n",
        "###Test 1, no modification 68% acc score\n",
        "###Test 2, resampling 67% acc score\n",
        "\n",
        "| Class | Count    |\n",
        "|-------|----------|\n",
        "| MEL   | 20651.0  |\n",
        "| NV    | 6957.0   |\n",
        "| BCC   | 25309.0  |\n",
        "| AK    | 77387.0  |\n",
        "| BKL   | 30227.0  |\n",
        "| DF    | 261561.0 |\n",
        "| VASC  | 261561.0 |\n",
        "| SCC   | 99679.0  |\n",
        "\n",
        "\n",
        "###Test 3, weighted resampling 69.1%\n",
        "\n",
        "| Class | Samples  |\n",
        "|-------|----------|\n",
        "| MEL   | 373.0    |\n",
        "| NV    | 130.0    |\n",
        "| BCC   | 504.0    |\n",
        "| AK    | 7682.0   |\n",
        "| BKL   | 644.0    |\n",
        "| DF    | 268194.0 |\n",
        "| VASC  | 268194.0 |\n",
        "| SCC   | 15448.0  |\n",
        "\n",
        "###Test 4 , 62% acc\n",
        "New class counts after resampling w communism:\n",
        "\n",
        "| Class | Samples |\n",
        "|-------|---------|\n",
        "| MEL   | 191.0   |\n",
        "| NV    | 191.0   |\n",
        "| BCC   | 191.0   |\n",
        "| AK    | 191.0   |\n",
        "| BKL   | 191.0   |\n",
        "| DF    | 191.0   |\n",
        "| VASC  | 191.0   |\n",
        "| SCC   | 191.0   |\n",
        "\n",
        "###Test 5, lets try a different kind of distributoin and classify the critically important (malignant) lesions with more weight:\n",
        "\n",
        "| Class | Samples |\n",
        "|-------|---------|\n",
        "| MEL   | 1910.0  |\n",
        "| NV    | 191.0   |\n",
        "| BCC   | 1910.0  |\n",
        "| AK    | 191.0   |\n",
        "| BKL   | 191.0   |\n",
        "| DF    | 191.0   |\n",
        "| VASC  | 191.0   |\n",
        "| SCC   | 524.0   |\n",
        "\n",
        "####Not much happening with this adjustment, will leave it weighted, with punishment on malignant classes and start hypertuning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uXEBi9lGLVro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.utils import resample\n",
        "\n",
        "# # Assuming 'full_metadata' is your complete dataset\n",
        "# X = full_metadata.drop(['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK'], axis=1)\n",
        "# y = full_metadata[['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# # Calculate class weights\n",
        "# class_weights = 1 / y_train.sum()\n",
        "# class_weights /= class_weights.sum()\n",
        "\n",
        "# # Calculate the target number of samples based on weighted average\n",
        "# weighted_samples = (class_weights * y_train.sum().sum()).astype(int)\n",
        "\n",
        "# # Create new DataFrame for the balanced dataset\n",
        "# X_train_resampled = pd.DataFrame(columns=X_train.columns)\n",
        "# y_train_resampled = pd.DataFrame(columns=y_train.columns)\n",
        "\n",
        "# # Resampling\n",
        "# for column in y_train.columns:\n",
        "#     filter_mask = y_train[column] == 1\n",
        "#     X_class = X_train[filter_mask]\n",
        "#     y_class = y_train[filter_mask]\n",
        "\n",
        "#     num_samples = len(X_class)\n",
        "#     target_samples = weighted_samples[column]\n",
        "#     if num_samples > 0:\n",
        "#         if num_samples < target_samples:\n",
        "#             X_class_resampled, y_class_resampled = resample(X_class, y_class,\n",
        "#                                                             replace=True,  # Sample with replacement\n",
        "#                                                             n_samples=target_samples,  # Match the target samples\n",
        "#                                                             random_state=42)\n",
        "#         else:\n",
        "#             X_class_resampled, y_class_resampled = resample(X_class, y_class,\n",
        "#                                                             replace=False,\n",
        "#                                                             n_samples=target_samples,\n",
        "#                                                             random_state=42)\n",
        "\n",
        "#         # Append resampled data back to the overall dataset\n",
        "#         X_train_resampled = pd.concat([X_train_resampled, X_class_resampled], axis=0)\n",
        "#         y_train_resampled = pd.concat([y_train_resampled, y_class_resampled], axis=0)\n",
        "#     else:\n",
        "#         print(f\"No instances to resample for class '{column}'\")\n",
        "\n",
        "# # Shuffle the dataset to mix up class order (important for training)\n",
        "# X_train_resampled = X_train_resampled.sample(frac=1, random_state=42)\n",
        "# y_train_resampled = y_train_resampled.loc[X_train_resampled.index]\n",
        "\n",
        "# print(\"New class counts after resampling:\\n\", y_train_resampled.sum())\n"
      ],
      "metadata": {
        "id": "Xe2_hFzW2MrO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'full_metadata' is your complete dataset and properly loaded\n",
        "X = full_metadata.drop(['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK'], axis=1)\n",
        "y = full_metadata[['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']]\n",
        "\n",
        "# Define class weights based on your specification\n",
        "weights = {\n",
        "    'MEL': 10.0, 'NV': 1.0, 'BCC': 10.0, 'AK': 1.0, 'BKL': 1.0,\n",
        "    'DF': 10.0, 'VASC': 1.0, 'SCC': 10.0\n",
        "}\n",
        "\n",
        "# Calculate class totals and total samples for weight adjustments\n",
        "class_totals = y.sum()\n",
        "total_samples = y.sum().sum()\n",
        "\n",
        "# Compute weights inversely proportional to class frequencies\n",
        "class_weights = total_samples / (len(class_totals) * class_totals)\n",
        "\n",
        "# Adjust weights according to your specific weights\n",
        "adjusted_weights = {key: weights[key] for key in weights}\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Output the class weights for reference\n",
        "print(\"New class counts after resampling:\\n\", y_train_resampled.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egs3PEnz6oyH",
        "outputId": "ad013a6f-ee06-42bf-9930-5cfeec0d6833"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New class counts after resampling:\n",
            " MEL     1910.0\n",
            "NV       191.0\n",
            "BCC     1910.0\n",
            "AK       191.0\n",
            "BKL      191.0\n",
            "DF       191.0\n",
            "VASC     191.0\n",
            "SCC      524.0\n",
            "dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.utils import resample\n",
        "\n",
        "# # Determine the count of each class in the training data\n",
        "# class_counts = y_train.sum()\n",
        "\n",
        "# # Find the minimum class size (size of the smallest class) and ensure it's an integer\n",
        "# min_class_size = int(class_counts.min())\n",
        "\n",
        "# # Modify the resampling code to undersample the majority classes\n",
        "# X_train_resampled = pd.DataFrame(columns=X_train.columns)\n",
        "# y_train_resampled = pd.DataFrame(columns=y_train.columns)\n",
        "\n",
        "# for column in y_train.columns:\n",
        "#     filter_mask = y_train[column] == 1\n",
        "#     X_class = X_train[filter_mask]\n",
        "#     y_class = y_train[filter_mask]\n",
        "\n",
        "#     num_samples = len(X_class)\n",
        "\n",
        "#     # Check if the current class is a majority class and it has more samples than the minimum\n",
        "#     if num_samples > min_class_size:\n",
        "#         # Apply undersampling\n",
        "#         X_class_resampled, y_class_resampled = resample(X_class, y_class,\n",
        "#                                                         replace=False,\n",
        "#                                                         n_samples=min_class_size,  # Reducing to the size of the smallest class\n",
        "#                                                         random_state=42)\n",
        "#     else:\n",
        "#         # If not a majority class, keep as is\n",
        "#         X_class_resampled, y_class_resampled = X_class.copy(), y_class.copy()\n",
        "\n",
        "#     # Append resampled data back to the overall dataset\n",
        "#     X_train_resampled = pd.concat([X_train_resampled, X_class_resampled], axis=0)\n",
        "#     y_train_resampled = pd.concat([y_train_resampled, y_class_resampled], axis=0)\n",
        "\n",
        "# # Shuffle the dataset to mix up class order (important for training)\n",
        "# X_train_resampled = X_train_resampled.sample(frac=1, random_state=42)\n",
        "# y_train_resampled = y_train_resampled.loc[X_train_resampled.index]\n",
        "\n",
        "# print(\"New class counts after resampling:\\n\", y_train_resampled.sum())\n"
      ],
      "metadata": {
        "id": "6yDSWMYpzOXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcKsOCJcaFiO",
        "outputId": "0dc6ca54-77bb-4fb3-fdcd-1130a608e7aa"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20264, 16) (20264, 8)\n",
            "(2533, 16) (2533, 8)\n",
            "(2534, 16) (2534, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: First Model Preparation and Training\n",
        "\n",
        "We will use a Dual Input Generator we custom built for our first model and the following steps will prep, then train our model."
      ],
      "metadata": {
        "id": "b-NrfRMfHRso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dual Input Data Generator\n",
        "\n",
        "The `DualInputGenerator` class is a custom data generator for handling datasets that include both image paths and associated metadata. It is designed to work with Keras/TensorFlow, enabling efficient batch processing which is crucial for training deep learning models on large datasets. Here's a breakdown of its functionality:\n",
        "\n",
        "- **Initialization (`__init__`)**: This method sets up the generator with all necessary parameters, including image paths, metadata, and labels. It also initializes the batch size, image size, and whether the dataset should be shuffled during training to introduce randomness into the training process.\n",
        "\n",
        "- **Preprocessing (`preprocess_image`)**: A helper function to read and preprocess images. It converts images to the appropriate size and scale ([224x224] in this case) and normalizes pixel values to the range [0, 1].\n",
        "\n",
        "- **Length Calculation (`__len__`)**: This method calculates how many batches are in the dataset, which is used by Keras during training to determine the number of steps per epoch.\n",
        "\n",
        "- **Batch Generation (`__getitem__`)**: This method retrieves a batch of data by processing the images, metadata, and labels. It loads and preprocesses the images specified by the batch indexes, extracts the corresponding metadata, and gathers the labels. The function returns a list containing two arrays (images and metadata) and the batch of labels.\n",
        "\n",
        "- **Epoch End Handling (`on_epoch_end`)**: If shuffling is enabled, this method shuffles the indexes after each epoch to ensure that the model does not see the same sequence of batches every epoch, helping the model to generalize better.\n",
        "\n",
        "This structured approach ensures that the model receives properly formatted and preprocessed data for each training step, facilitating effective learning and performance improvement.\n"
      ],
      "metadata": {
        "id": "lmdnx1WuLlkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Updated input generator to provide code for more data augmentation on images, using\n",
        "\n",
        "1.   flip: left and right\n",
        "2.   brightness & contrast shifting\n",
        "\n",
        "####Not a large effect noticed in this type of augmentation on our specific model\n",
        "\n"
      ],
      "metadata": {
        "id": "86lCVqd8qSaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DualInputGenerator(Sequence):\n",
        "    def __init__(self, image_paths, metadata, labels, batch_size, img_size=(224, 224), shuffle=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.metadata = metadata\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def preprocess_image(self, file_path):\n",
        "        img = tf.io.read_file(file_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [224, 224])\n",
        "        img = img / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        # Apply data augmentation\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_brightness(img, max_delta=0.2)\n",
        "        img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
        "        # Additional augmentations can be added here\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_idx = index * self.batch_size\n",
        "        end_idx = (index + 1) * self.batch_size\n",
        "        end_idx = min(end_idx, len(self.image_paths))\n",
        "        current_batch_size = end_idx - start_idx\n",
        "\n",
        "        X_images = np.empty((current_batch_size, *self.img_size, 3))\n",
        "        X_metadata = np.empty((current_batch_size, self.metadata.shape[1]))\n",
        "        y = np.empty((current_batch_size, self.labels.shape[1]), dtype=int)\n",
        "\n",
        "        for i, idx in enumerate(range(start_idx, end_idx)):\n",
        "            img_path = self.image_paths[idx]\n",
        "            img = self.preprocess_image(img_path)\n",
        "            X_images[i, ] = img\n",
        "            X_metadata[i, ] = self.metadata[idx]\n",
        "            y[i, ] = self.labels[idx]\n",
        "\n",
        "        return [X_images, X_metadata], y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n"
      ],
      "metadata": {
        "id": "QIRrsmdRawsT"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Data Generators for Training, Validation, and Testing\n",
        "\n",
        "To ensure our model is trained, validated, and tested with data that is efficiently loaded and preprocessed, we initialize three instances of the `DualInputGenerator` class:\n",
        "\n",
        "- **Training Generator (`train_gen`)**: This generator is configured with training data paths, metadata, and labels. It is responsible for feeding the training data into the model in batches of 32, ensuring that each batch is shuffled to promote model generalization.\n",
        "\n",
        "- **Validation Generator (`val_gen`)**: Similar to the training generator, but using the validation dataset. This generator provides data for evaluating the model during the training process, allowing us to monitor the model's performance and make adjustments if needed without seeing the test data.\n",
        "\n",
        "- **Testing Generator (`test_gen`)**: Finally, the testing generator is set up using the test dataset to evaluate the model's performance after training has been completed. This step is crucial for assessing how well the model is likely to perform on unseen real-world data.\n",
        "\n",
        "Each generator uses the `image_paths`, `metadata`, and `labels` from their respective subsets of the data, ensuring that the model receives all necessary inputs for making predictions during each phase of the training and evaluation process.\n",
        "\n"
      ],
      "metadata": {
        "id": "PmC1R4dDLw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the training generator with image size parameter\n",
        "train_gen = DualInputGenerator(\n",
        "    image_paths=X_train['image_path'].values,\n",
        "    metadata=X_train.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_train.values,\n",
        "    batch_size=32,\n",
        "    img_size=(224, 224)\n",
        ")\n",
        "\n",
        "# Initialize the validation generator with image size parameter\n",
        "val_gen = DualInputGenerator(\n",
        "    image_paths=X_val['image_path'].values,\n",
        "    metadata=X_val.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_val.values,\n",
        "    batch_size=32,\n",
        "    img_size=(224, 224)\n",
        ")\n",
        "\n",
        "# Initialize the test generator with image size parameter\n",
        "test_gen = DualInputGenerator(\n",
        "    image_paths=X_test['image_path'].values,\n",
        "    metadata=X_test.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_test.values,\n",
        "    batch_size=32,\n",
        "    img_size=(224, 224)\n",
        ")\n",
        "\n",
        "# Example line to check if the image paths from X_test are being accessed correctly\n",
        "print(X_test['image_path'].values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrENsN6Nb-0G",
        "outputId": "e044c707-4c3e-4b8a-9a1d-9113dd11613c"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0031599.jpg'\n",
            " '/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0057178.jpg'\n",
            " '/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0060330.jpg'\n",
            " ...\n",
            " '/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0056152.jpg'\n",
            " '/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0070938.jpg'\n",
            " '/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0057966.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dual Input Neural Network Architecture\n",
        "\n",
        "The neural network architecture is designed to handle two types of input: images and metadata. Here's a breakdown of the model architecture and the role of each component:\n",
        "\n",
        "#### Image Input Branch\n",
        "- **Image Input**: The model starts with an image input layer that accepts images of shape (224, 224, 3).\n",
        "- **Convolution and Pooling Layers**: Following the input, the model includes two sets of convolutional layers (`Conv2D`) and max pooling layers (`MaxPooling2D`). Each convolutional layer is followed by batch normalization (`BatchNormalization`), which helps to accelerate the training process and stabilize the learning environment by normalizing the activations.\n",
        "- **Flattening and Dense Layer**: After extracting and pooling features through convolutions, the data is flattened (`Flatten`) and passed through a dense layer with ReLU activation, which is again batch normalized.\n",
        "\n",
        "#### Metadata Input Branch\n",
        "- **Metadata Input**: This branch begins with an input for metadata features, shaped dynamically based on the number of metadata features (`num_metadata_features`).\n",
        "- **Dense Layers and Normalization**: It includes dense layers (`Dense`) with ReLU activation, interspersed with batch normalization to ensure the model learns effectively from the structured data.\n",
        "\n",
        "#### Combining Branches\n",
        "- **Concatenation**: The outputs of the image and metadata branches are combined into a single vector (`concatenate`), allowing the model to learn from both image features and metadata simultaneously.\n",
        "- **Final Dense Layers**: The combined data is then passed through additional dense layers, including a dropout layer (`Dropout`) to prevent overfitting, culminating in a softmax output layer (`Dense`) that classifies the images into one of nine diagnostic categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hg7trvZfbOp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_metadata_features = X_train.drop(columns=['image', 'image_path', 'lesion_id']).shape[1]\n",
        "print(\"Number of metadata features:\", num_metadata_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUTOAkX0lHTp",
        "outputId": "d1d72f93-92ed-4a78-c69d-d76317158906"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of metadata features: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_metadata_features = 13\n",
        "\n",
        "# Image input branch\n",
        "image_input = Input(shape=(224, 224, 3), name='image_input')\n",
        "x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "# Add batch normalization here\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "# Add batch normalization here\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "image_branch = Dense(128, activation='relu')(x)\n",
        "image_branch = BatchNormalization()(image_branch)\n",
        "\n",
        "# Metadata input branch\n",
        "metadata_input = Input(shape=(num_metadata_features,), name='metadata_input')\n",
        "y = Dense(32, activation='relu')(metadata_input)\n",
        "# Normalize after the first dense layer\n",
        "metadata_branch = BatchNormalization()(y)\n",
        "metadata_branch = Dense(64, activation='relu')(metadata_branch)\n",
        "# Normalize before combining\n",
        "metadata_branch = BatchNormalization()(metadata_branch)\n",
        "\n",
        "# Combine the outputs of the two branches\n",
        "combined = concatenate([image_branch, metadata_branch])\n",
        "z = Dense(256, activation='relu')(combined)\n",
        " # Adding dropout for regularization\n",
        "z = Dropout(0.2232357393141235)(z)\n",
        "# Adjust the number of units to match the number of classes\n",
        "output = Dense(8, activation='softmax')(z)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[image_input, metadata_input], outputs=output)\n",
        "\n",
        "# Adjust the learning rate if testing indicates it's beneficial\n",
        "# Start with your last setting and adjust based on performance\n",
        "learning_rate = 1e-5\n"
      ],
      "metadata": {
        "id": "EOCCRKIH6pE3"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augmentaion of Automated hyperparameter optimization ☄\n",
        "\n",
        "After numerous trials we are unable to break the low 70% accuracies, we will try some hypertuning...\n",
        "\n",
        "### How Optuna Chooses the Optimizer:\n",
        "In your objective function, you specify a categorical distribution for the optimizer using `trial.suggest_categorical()`. This function tells Optuna to randomly choose one of the provided options for each trial.\n",
        "\n",
        "### Optuna's Optimization Algorithms:\n",
        "Optuna uses several algorithms to optimize the search for the best hyperparameters:\n",
        "\n",
        "- **Tree-structured Parzen Estimator (TPE):** A default and commonly used approach that models the search space using Bayesian optimization.\n",
        "- **CMA-ES:** An evolutionary algorithm suitable for optimizing in continuous spaces, which can be more effective when many trials are possible.\n",
        "- **Random Search and Grid Search:** Simpler methods that are sometimes used, particularly random search in the early phases of a study.\n",
        "\n",
        "\n",
        "###Extra method set here to reduce time;\n",
        "\n",
        "def objective_and_fit_wrapper(trial):\n",
        "\n",
        "Parallelize Computations: Structuring our model and computations to maximize parallelism, allowing the TPU to efficiently utilize its cores. This lets us run more epochs faster. We also applied pruning to the studies and that will end the early identified losers more rapidly."
      ],
      "metadata": {
        "id": "nAHLjtFyiZf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming 'full_metadata' is your complete dataset and properly loaded\n",
        "# X = full_metadata.drop(['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK'], axis=1)\n",
        "# y = full_metadata[['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']]\n",
        "\n",
        "# # Split the data into training, validation, and testing sets (these are not augmented)\n",
        "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "class DualInputGenerator(Sequence):\n",
        "    def __init__(self, image_paths, metadata, labels, batch_size, img_size=(224, 224), shuffle=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.metadata = metadata\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def preprocess_image(self, file_path):\n",
        "        img = tf.io.read_file(file_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, self.img_size)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_idx = index * self.batch_size\n",
        "        end_idx = (index + 1) * self.batch_size\n",
        "        end_idx = min(end_idx, len(self.image_paths))\n",
        "        current_batch_size = end_idx - start_idx\n",
        "        X_images = np.empty((current_batch_size, *self.img_size, 3))\n",
        "        X_metadata = np.empty((current_batch_size, self.metadata.shape[1]))\n",
        "        y = np.empty((current_batch_size, self.labels.shape[1]), dtype=int)\n",
        "        for i, idx in enumerate(range(start_idx, end_idx)):\n",
        "            img_path = self.image_paths[idx]\n",
        "            img = self.preprocess_image(img_path)\n",
        "            X_images[i, ] = img\n",
        "            X_metadata[i, ] = self.metadata[idx]\n",
        "            y[i, ] = self.labels[idx]\n",
        "        return [X_images, X_metadata], y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "def build_model(learning_rate, dropout_rate, num_metadata_features):\n",
        "    image_input = Input(shape=(224, 224, 3), name='image_input')\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    image_branch = Dense(128, activation='relu')(x)\n",
        "    image_branch = BatchNormalization()(image_branch)\n",
        "\n",
        "    metadata_input = Input(shape=(num_metadata_features,), name='metadata_input')\n",
        "    y = Dense(32, activation='relu')(metadata_input)\n",
        "    metadata_branch = BatchNormalization()(y)\n",
        "    metadata_branch = Dense(64, activation='relu')(metadata_branch)\n",
        "    metadata_branch = BatchNormalization()(metadata_branch)\n",
        "\n",
        "    combined = concatenate([image_branch, metadata_branch])\n",
        "    z = Dense(256, activation='relu')(combined)\n",
        "    z = Dropout(dropout_rate)(z)\n",
        "    output = Dense(8, activation='softmax')(z)\n",
        "\n",
        "    model = Model(inputs=[image_input, metadata_input], outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def objective_and_fit(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "    model = build_model(learning_rate, dropout_rate, 13)\n",
        "    train_gen = DualInputGenerator(X_train['image_path'].values, X_train.drop(columns=['image', 'image_path', 'lesion_id']).values, y_train.values, batch_size)\n",
        "    val_gen = DualInputGenerator(X_val['image_path'].values, X_val.drop(columns=['image', 'image_path', 'lesion_id']).values, y_val.values, batch_size)\n",
        "    # Add TFKerasPruningCallback to enable pruning at each epoch\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        TFKerasPruningCallback(trial, 'val_accuracy')  # Monitor validation accuracy for pruning\n",
        "    ]\n",
        "    history = model.fit(train_gen, epochs=3, validation_data=val_gen, callbacks=callbacks, verbose=1)\n",
        "    max_val_accuracy = np.max(history.history['val_accuracy'])\n",
        "    return max_val_accuracy\n",
        "\n",
        "# Set up parallelism for Optuna\n",
        "def objective_and_fit_wrapper(trial):\n",
        "    return objective_and_fit(trial)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pruner = optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=5,  # Number of trials to run before pruning starts\n",
        "        n_warmup_steps=2,    # Number of steps to run before starting to apply prune logic\n",
        "        interval_steps=1     # Interval steps to apply pruning\n",
        "    )\n",
        "\n",
        "    study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "    # Uncomment the line below to run the optimization\n",
        "    #study.optimize(objective_and_fit_wrapper, n_trials=50)\n",
        "\n",
        "    print(\"Best hyperparameters: \", study.best_trial.params)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KnsZXwvT7Dx4",
        "outputId": "e43f2a6e-0e38-42d0-fc91-8817217b39c5"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 20:38:23,214] A new study created in memory with name: no-name-986a4a84-32b5-4693-b627-5f7a69ed52c8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1266/1266 [==============================] - 452s 355ms/step - loss: 1.1587 - accuracy: 0.5881 - val_loss: 1.0641 - val_accuracy: 0.6250\n",
            "Epoch 2/3\n",
            "1266/1266 [==============================] - 449s 354ms/step - loss: 1.0308 - accuracy: 0.6233 - val_loss: 1.0507 - val_accuracy: 0.6191\n",
            "Epoch 3/3\n",
            "1266/1266 [==============================] - 451s 356ms/step - loss: 0.9874 - accuracy: 0.6379 - val_loss: 1.0772 - val_accuracy: 0.6230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 21:00:55,401] Trial 0 finished with value: 0.625 and parameters: {'learning_rate': 0.0028012345263178262, 'dropout_rate': 0.16355387148358058, 'batch_size': 16}. Best is trial 0 with value: 0.625.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "633/633 [==============================] - 383s 601ms/step - loss: 1.1478 - accuracy: 0.5985 - val_loss: 0.9860 - val_accuracy: 0.6535\n",
            "Epoch 2/3\n",
            "633/633 [==============================] - 383s 605ms/step - loss: 0.9100 - accuracy: 0.6739 - val_loss: 0.9244 - val_accuracy: 0.6709\n",
            "Epoch 3/3\n",
            "633/633 [==============================] - 383s 605ms/step - loss: 0.7479 - accuracy: 0.7365 - val_loss: 0.8903 - val_accuracy: 0.6804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 21:20:05,345] Trial 1 finished with value: 0.6803797483444214 and parameters: {'learning_rate': 7.879710412375101e-05, 'dropout_rate': 0.14240551142447977, 'batch_size': 32}. Best is trial 1 with value: 0.6803797483444214.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "633/633 [==============================] - 381s 598ms/step - loss: 1.1804 - accuracy: 0.5817 - val_loss: 1.1431 - val_accuracy: 0.5993\n",
            "Epoch 2/3\n",
            "633/633 [==============================] - 384s 607ms/step - loss: 0.9885 - accuracy: 0.6433 - val_loss: 0.9695 - val_accuracy: 0.6428\n",
            "Epoch 3/3\n",
            "633/633 [==============================] - 385s 607ms/step - loss: 0.9095 - accuracy: 0.6646 - val_loss: 0.8917 - val_accuracy: 0.6677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 21:39:15,918] Trial 2 finished with value: 0.6677215099334717 and parameters: {'learning_rate': 0.0007035527020928377, 'dropout_rate': 0.4694065082622678, 'batch_size': 32}. Best is trial 1 with value: 0.6803797483444214.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "316/316 [==============================] - 343s 1s/step - loss: 1.1837 - accuracy: 0.5844 - val_loss: 1.1570 - val_accuracy: 0.5962\n",
            "Epoch 2/3\n",
            "316/316 [==============================] - 341s 1s/step - loss: 1.0226 - accuracy: 0.6342 - val_loss: 1.0481 - val_accuracy: 0.6278\n",
            "Epoch 3/3\n",
            "316/316 [==============================] - 342s 1s/step - loss: 0.9736 - accuracy: 0.6439 - val_loss: 1.1115 - val_accuracy: 0.6186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 21:56:22,653] Trial 3 finished with value: 0.6278044581413269 and parameters: {'learning_rate': 0.009608238223225617, 'dropout_rate': 0.3050713216935987, 'batch_size': 64}. Best is trial 1 with value: 0.6803797483444214.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1266/1266 [==============================] - 447s 351ms/step - loss: 1.1454 - accuracy: 0.5904 - val_loss: 0.9798 - val_accuracy: 0.6531\n",
            "Epoch 2/3\n",
            "1266/1266 [==============================] - 446s 352ms/step - loss: 0.9662 - accuracy: 0.6486 - val_loss: 0.9396 - val_accuracy: 0.6555\n",
            "Epoch 3/3\n",
            "1266/1266 [==============================] - 450s 356ms/step - loss: 0.8683 - accuracy: 0.6803 - val_loss: 0.9475 - val_accuracy: 0.6673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 22:18:46,242] Trial 4 finished with value: 0.6673259735107422 and parameters: {'learning_rate': 0.00021982932946292507, 'dropout_rate': 0.16935778040551458, 'batch_size': 16}. Best is trial 1 with value: 0.6803797483444214.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1266/1266 [==============================] - 446s 350ms/step - loss: 1.1513 - accuracy: 0.5922 - val_loss: 0.9420 - val_accuracy: 0.6626\n",
            "Epoch 2/3\n",
            "1266/1266 [==============================] - 445s 351ms/step - loss: 0.9468 - accuracy: 0.6594 - val_loss: 0.9099 - val_accuracy: 0.6741\n",
            "Epoch 3/3\n",
            "1266/1266 [==============================] - 446s 352ms/step - loss: 0.8319 - accuracy: 0.6969 - val_loss: 0.9166 - val_accuracy: 0.6705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 22:41:02,433] Trial 5 finished with value: 0.6740506291389465 and parameters: {'learning_rate': 0.0001375875550185406, 'dropout_rate': 0.198863963752195, 'batch_size': 16}. Best is trial 1 with value: 0.6803797483444214.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "633/633 [==============================] - 385s 604ms/step - loss: 1.2546 - accuracy: 0.5610 - val_loss: 0.9853 - val_accuracy: 0.6483\n",
            "Epoch 2/3\n",
            "633/633 [==============================] - 389s 614ms/step - loss: 1.0131 - accuracy: 0.6383 - val_loss: 0.9405 - val_accuracy: 0.6570\n",
            "Epoch 3/3\n",
            "633/633 [==============================] - 393s 621ms/step - loss: 0.8985 - accuracy: 0.6785 - val_loss: 0.8812 - val_accuracy: 0.6812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 23:00:30,478] Trial 6 finished with value: 0.6811708807945251 and parameters: {'learning_rate': 0.00012074817064355612, 'dropout_rate': 0.4459520080349169, 'batch_size': 32}. Best is trial 6 with value: 0.6811708807945251.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "316/316 [==============================] - 351s 1s/step - loss: 1.0986 - accuracy: 0.6044 - val_loss: 1.5205 - val_accuracy: 0.4002\n",
            "Epoch 2/3\n",
            "316/316 [==============================] - 352s 1s/step - loss: 0.9375 - accuracy: 0.6557 - val_loss: 0.9498 - val_accuracy: 0.6418\n",
            "Epoch 3/3\n",
            "316/316 [==============================] - 355s 1s/step - loss: 0.8746 - accuracy: 0.6789 - val_loss: 0.8917 - val_accuracy: 0.6827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 23:18:09,998] Trial 7 finished with value: 0.682692289352417 and parameters: {'learning_rate': 0.0016785443821833478, 'dropout_rate': 0.16454796816564687, 'batch_size': 64}. Best is trial 7 with value: 0.682692289352417.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "633/633 [==============================] - 398s 626ms/step - loss: 1.1795 - accuracy: 0.5877 - val_loss: 1.0314 - val_accuracy: 0.6325\n",
            "Epoch 2/3\n",
            "633/633 [==============================] - 395s 624ms/step - loss: 0.9385 - accuracy: 0.6648 - val_loss: 0.8848 - val_accuracy: 0.6828\n",
            "Epoch 3/3\n",
            "633/633 [==============================] - 397s 627ms/step - loss: 0.8042 - accuracy: 0.7135 - val_loss: 0.8921 - val_accuracy: 0.6697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-21 23:38:01,771] Trial 8 finished with value: 0.6827531456947327 and parameters: {'learning_rate': 0.00012617133871374036, 'dropout_rate': 0.2232357393141235, 'batch_size': 32}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1266/1266 [==============================] - 464s 365ms/step - loss: 1.4020 - accuracy: 0.5298 - val_loss: 1.0836 - val_accuracy: 0.6246\n",
            "Epoch 2/3\n",
            "1266/1266 [==============================] - 466s 368ms/step - loss: 1.1486 - accuracy: 0.6038 - val_loss: 1.0062 - val_accuracy: 0.6396\n",
            "Epoch 3/3\n",
            "1266/1266 [==============================] - 469s 370ms/step - loss: 1.0342 - accuracy: 0.6381 - val_loss: 0.9432 - val_accuracy: 0.6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-22 00:01:22,188] Trial 9 finished with value: 0.6629746556282043 and parameters: {'learning_rate': 1.3255183392884535e-05, 'dropout_rate': 0.359434195648277, 'batch_size': 16}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "633/633 [==============================] - 391s 614ms/step - loss: 1.2408 - accuracy: 0.5751 - val_loss: 1.1043 - val_accuracy: 0.6214\n",
            "Epoch 2/3\n",
            "633/633 [==============================] - 395s 623ms/step - loss: 0.9606 - accuracy: 0.6636 - val_loss: 0.9529 - val_accuracy: 0.6665\n",
            "Epoch 3/3\n",
            "633/633 [==============================] - 396s 625ms/step - loss: 0.8093 - accuracy: 0.7210 - val_loss: 0.9077 - val_accuracy: 0.6772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-22 00:21:04,925] Trial 10 finished with value: 0.6772152185440063 and parameters: {'learning_rate': 2.1301560612331817e-05, 'dropout_rate': 0.02229975041081872, 'batch_size': 32}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "316/316 [==============================] - 351s 1s/step - loss: 1.0666 - accuracy: 0.6161 - val_loss: 1.4813 - val_accuracy: 0.4439\n",
            "Epoch 2/3\n",
            "316/316 [==============================] - 352s 1s/step - loss: 0.9071 - accuracy: 0.6683 - val_loss: 0.9336 - val_accuracy: 0.6651\n",
            "Epoch 3/3\n",
            "316/316 [==============================] - 355s 1s/step - loss: 0.8004 - accuracy: 0.7090 - val_loss: 0.9188 - val_accuracy: 0.6763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-22 00:38:44,426] Trial 11 finished with value: 0.6762820482254028 and parameters: {'learning_rate': 0.0008981413138838566, 'dropout_rate': 0.05325777580436017, 'batch_size': 64}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "316/316 [==============================] - 355s 1s/step - loss: 1.0889 - accuracy: 0.6070 - val_loss: 1.3176 - val_accuracy: 0.5288\n",
            "Epoch 2/3\n",
            "316/316 [==============================] - 355s 1s/step - loss: 0.9426 - accuracy: 0.6511 - val_loss: 1.1319 - val_accuracy: 0.5982\n",
            "Epoch 3/3\n",
            "316/316 [==============================] - 360s 1s/step - loss: 0.8786 - accuracy: 0.6735 - val_loss: 1.1339 - val_accuracy: 0.5998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-22 00:56:35,095] Trial 12 finished with value: 0.5997596383094788 and parameters: {'learning_rate': 0.0013860784146505509, 'dropout_rate': 0.24688000875081467, 'batch_size': 64}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "316/316 [==============================] - 360s 1s/step - loss: 1.1249 - accuracy: 0.5997 - val_loss: 1.0481 - val_accuracy: 0.6370\n",
            "Epoch 2/3\n",
            "316/316 [==============================] - 357s 1s/step - loss: 0.9717 - accuracy: 0.6443 - val_loss: 1.1457 - val_accuracy: 0.5990\n",
            "Epoch 3/3\n",
            "316/316 [==============================] - 359s 1s/step - loss: 0.9192 - accuracy: 0.6605 - val_loss: 0.9356 - val_accuracy: 0.6494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-22 01:14:31,770] Trial 13 finished with value: 0.6494390964508057 and parameters: {'learning_rate': 0.005287547067868121, 'dropout_rate': 0.07812795541744338, 'batch_size': 64}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "316/316 [==============================] - 361s 1s/step - loss: 1.3554 - accuracy: 0.5369 - val_loss: 1.4630 - val_accuracy: 0.5176\n",
            "Epoch 2/3\n",
            "316/316 [==============================] - 359s 1s/step - loss: 1.0540 - accuracy: 0.6335 - val_loss: 1.0025 - val_accuracy: 0.6514\n",
            "Epoch 3/3\n",
            "316/316 [==============================] - 358s 1s/step - loss: 0.9186 - accuracy: 0.6755 - val_loss: 0.9025 - val_accuracy: 0.6767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-22 01:32:31,116] Trial 14 finished with value: 0.676682710647583 and parameters: {'learning_rate': 3.9033836719127514e-05, 'dropout_rate': 0.30355261925044486, 'batch_size': 64}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "633/633 [==============================] - 401s 628ms/step - loss: 1.1125 - accuracy: 0.5996 - val_loss: 1.0653 - val_accuracy: 0.6274\n",
            "Epoch 2/3\n",
            "633/633 [==============================] - 401s 633ms/step - loss: 0.9256 - accuracy: 0.6656 - val_loss: 0.9953 - val_accuracy: 0.6535\n",
            "Epoch 3/3\n",
            "633/633 [==============================] - 404s 637ms/step - loss: 0.8128 - accuracy: 0.7031 - val_loss: 0.9343 - val_accuracy: 0.6646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-22 01:52:37,613] Trial 15 finished with value: 0.6645569801330566 and parameters: {'learning_rate': 0.0003222053459025417, 'dropout_rate': 0.10052000502500325, 'batch_size': 32}. Best is trial 8 with value: 0.6827531456947327.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "393/633 [=================>............] - ETA: 2:20 - loss: 1.1823 - accuracy: 0.5881"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2024-04-22 01:56:31,265] Trial 16 failed with parameters: {'learning_rate': 0.0004278505111587838, 'dropout_rate': 0.2534668355578268, 'batch_size': 32} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-128-7466004b9a42>\", line 98, in objective_and_fit_wrapper\n",
            "    return objective_and_fit(trial)\n",
            "  File \"<ipython-input-128-7466004b9a42>\", line 92, in objective_and_fit\n",
            "    history = model.fit(train_gen, epochs=3, validation_data=val_gen, callbacks=callbacks, verbose=1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 832, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 868, in _call\n",
            "    return tracing_compilation.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1323, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\", line 1486, in call_function\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "[W 2024-04-22 01:56:31,266] Trial 16 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-7466004b9a42>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_and_fit_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     ):\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-128-7466004b9a42>\u001b[0m in \u001b[0;36mobjective_and_fit_wrapper\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Set up parallelism for Optuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective_and_fit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-128-7466004b9a42>\u001b[0m in \u001b[0;36mobjective_and_fit\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mTFKerasPruningCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Monitor validation accuracy for pruning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     ]\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mmax_val_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax_val_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Compilation\n",
        "- Now we can implement the hypertuned parameters discovered in the previous steps... fingers crossed we get a better output.  This process is automated, as the runtime of the above step is long.\n",
        "- The model was originally compiled with the Adam optimizer, using a learning rate of 1e-4. The loss function used is categorical crossentropy, suitable for multi-class classification tasks, and accuracy is used as the metric to evaluate model performance.\n",
        "\n",
        "This dual-input setup allows the model to leverage both detailed image data and accompanying metadata, aiming to improve diagnostic accuracy compared to using images or metadata alone.\n"
      ],
      "metadata": {
        "id": "qxsmGoXUuSV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_params = study.best_trial.params\n",
        "best_learning_rate = best_params['learning_rate']\n",
        "best_optimizer_name = best_params['optimizer']\n",
        "\n",
        "# Choose the optimizer based on the best hyperparameters found\n",
        "if best_optimizer_name == 'adam':\n",
        "    optimizer = Adam(learning_rate=best_learning_rate)\n",
        "else:\n",
        "    optimizer = RMSprop(learning_rate=best_learning_rate)\n",
        "\n",
        "# Compile the model with the best hyperparameters\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', Precision(), Recall()]\n",
        ")\n"
      ],
      "metadata": {
        "id": "4Sz07lP1iTF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Manual save in case colab boots us...\n",
        "run these 2 cells instead of the 2 cells above to save 15 - 96 hours"
      ],
      "metadata": {
        "id": "ZsF5SB9fS_Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best parameters from tuning process\n",
        "# best_params = {\n",
        "#     'learning_rate': 0.00029207045028036043,\n",
        "#     'optimizer': 'rmsprop',\n",
        "#     'batch_size': 64,\n",
        "#     'objective_value': 0.6776107549667358\n",
        "# }\n",
        "\n",
        "# # hypertested at .72 loss .74 acc\n",
        "# best_params = {\n",
        "#     'learning_rate': 0.00195497006883763,\n",
        "#     'optimizer': 'rmsprop',\n",
        "#     'batch_size': 16,\n",
        "#     'objective_value': 0.6507120132446289\n",
        "# }\n",
        "\n",
        "#\n",
        "# best_params = {\n",
        "#     'learning_rate': 0.00033474734484637854,\n",
        "#     'optimizer': 'rmsprop',\n",
        "#     'batch_size': 64,\n",
        "#     'objective_value': 0.6673259735107422\n",
        "# }\n",
        "\n",
        "best_params = {\n",
        "    'learning_rate': 0.00012617133871374036,\n",
        "    'optimizer': 'rmsprop',\n",
        "    'batch_size': 32,\n",
        "    'objective_value': 0.6673259735107422\n",
        "}\n",
        "\n",
        "# Save the parameters to disk\n",
        "with open('/content/drive/My Drive/best_hyperparams.pkl', 'wb') as f:\n",
        "    pickle.dump(best_params, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6HVhXpNgS-WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved parameters\n",
        "with open('/content/drive/My Drive/best_hyperparams.pkl', 'rb') as f:\n",
        "    best_params = pickle.load(f)\n",
        "\n",
        "# Assume `model` is your pre-existing model that you've already defined and compiled somewhere in your code\n",
        "\n",
        "# Configure the optimizer based on the loaded parameters\n",
        "if best_params['optimizer'] == 'adam':\n",
        "    optimizer = Adam(learning_rate=best_params['learning_rate'])\n",
        "elif best_params['optimizer'] == 'rmsprop':\n",
        "    optimizer = RMSprop(learning_rate=best_params['learning_rate'])\n",
        "else:\n",
        "    raise ValueError(\"Unsupported optimizer: {}\".format(best_params['optimizer']))\n",
        "\n",
        "# Set the new optimizer to the model\n",
        "optimizer = Adam(learning_rate=0.00012617133871374036)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Snc2jopXYy7v"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring Callbacks for Model Training\n",
        "\n",
        "To enhance the training process and ensure optimal model performance, we use a combination of callbacks in TensorFlow. These callbacks are set to monitor the training process and make decisions at the end of each epoch. Here's a breakdown of each callback used:\n",
        "\n",
        "#### EarlyStopping\n",
        "- **Purpose**: Prevents overfitting by halting the training if the model's validation loss does not improve for a specified number of consecutive epochs.\n",
        "- **Configuration**:\n",
        "  - `monitor='val_loss'`: Watches the validation loss metric for changes.\n",
        "  - `patience=2`: Allows the training to continue for 2 epochs without improvements in validation loss, we will run more later with a higher patience level.\n",
        "  - `verbose=1`: Enables logging for when the training is stopped early.\n",
        "\n",
        "#### ModelCheckpoint\n",
        "- **Purpose**: Saves the model in its current state after each epoch, but only if the model's performance (based on the monitored metric) has improved.\n",
        "- **Configuration**:\n",
        "  - `filepath='best_model.keras'`: Specifies the location and filename where the best model version will be saved.\n",
        "  - `monitor='val_loss'`: Monitors the validation loss for improvements.\n",
        "  - `save_best_only=True`: Ensures that the model is saved only when its validation loss is at its lowest point seen so far.\n",
        "  - `verbose=1`: Provides detailed logs when the model is saved.\n",
        "\n",
        "These callbacks are essential tools for managing long training sessions effectively. They help conserve resources by stopping training when additional epochs would not lead to improvements (`EarlyStopping`) and by ensuring that only the best model version is saved (`ModelCheckpoint`), thus simplifying deployment and further evaluation.\n"
      ],
      "metadata": {
        "id": "jtj_1lin7bTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/your_model_directory/model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n"
      ],
      "metadata": {
        "id": "WMPKC0Z4mid1"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss',\n",
        "                  mode='min',\n",
        "                  verbose=3,\n",
        "                  # Allows some room for loss to decrease again\n",
        "                  patience=10,\n",
        "                  # Stops training if the loss goes above 0.2\n",
        "                  baseline=0.25,\n",
        "                  restore_best_weights=True\n",
        "                  ),\n",
        "\n",
        "\n",
        "    ModelCheckpoint(filepath='best_model.keras',\n",
        "                    monitor='val_loss',\n",
        "                    save_best_only=True,\n",
        "                    verbose=1\n",
        "                    )\n",
        "]"
      ],
      "metadata": {
        "id": "aMr-5vG72Z98"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "#### AND Re-Training, the first model resulted in 70% this is after going back to do futher augmentation\n",
        "\n",
        "To train the dual-input model, we use the `model.fit` method provided by TensorFlow. This method executes the training cycle across a specified number of epochs, allowing for detailed monitoring and adjustments during the training process. Here's a breakdown of each parameter used in the `model.fit` call:\n",
        "\n",
        "- **train_gen**: The training data generator, which supplies batches of image and metadata inputs along with the corresponding labels.\n",
        "- **validation_data**: The validation data generator used to evaluate the model at the end of each epoch, helping monitor its performance on unseen data.\n",
        "- **epochs**: Sets the total number of training cycles the model will undergo. In this case, the model is set to train for 10 epochs.\n",
        "- **steps_per_epoch**: Specifies the number of batch steps to complete one epoch. This is set to the total number of batches available in the training generator (`len(train_gen)`), ensuring that each sample is used once per epoch.\n",
        "- **validation_steps**: Determines the number of batch steps used for validating the model, set to the length of the validation generator.\n",
        "- **callbacks**: Includes the `EarlyStopping` and `ModelCheckpoint` callbacks configured previously. These enhance the training process by:\n",
        "  - **EarlyStopping**: Automatically stops training when the validation loss ceases to decrease, preventing overfitting.\n",
        "  - **ModelCheckpoint**: Saves the best version of the model based on validation loss, ensuring that only the most accurate model is retained.\n",
        "\n",
        "This configuration ensures that the model is not only trained but also validated effectively, with checkpoints saved automatically and training potentially halted early if no further gains are observed. This approach optimizes both the efficiency and efficacy of the model training process.\n",
        "\n"
      ],
      "metadata": {
        "id": "9azv94GqNNcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    # Include callbacks for early stopping and model checkpointing\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGXmw8zk2imE",
        "outputId": "4d8cb517-f600-4364-b26d-63d347087678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 1.2084 - accuracy: 0.5752\n",
            "Epoch 1: val_loss improved from inf to 1.03564, saving model to best_model.keras\n",
            "633/633 [==============================] - 402s 631ms/step - loss: 1.2084 - accuracy: 0.5752 - val_loss: 1.0356 - val_accuracy: 0.6250\n",
            "Epoch 2/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 1.0193 - accuracy: 0.6308\n",
            "Epoch 2: val_loss improved from 1.03564 to 0.97243, saving model to best_model.keras\n",
            "633/633 [==============================] - 404s 637ms/step - loss: 1.0193 - accuracy: 0.6308 - val_loss: 0.9724 - val_accuracy: 0.6428\n",
            "Epoch 3/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 0.9468 - accuracy: 0.6569\n",
            "Epoch 3: val_loss improved from 0.97243 to 0.91466, saving model to best_model.keras\n",
            "633/633 [==============================] - 403s 637ms/step - loss: 0.9468 - accuracy: 0.6569 - val_loss: 0.9147 - val_accuracy: 0.6610\n",
            "Epoch 4/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 0.8905 - accuracy: 0.6720\n",
            "Epoch 4: val_loss improved from 0.91466 to 0.89972, saving model to best_model.keras\n",
            "633/633 [==============================] - 404s 638ms/step - loss: 0.8905 - accuracy: 0.6720 - val_loss: 0.8997 - val_accuracy: 0.6752\n",
            "Epoch 5/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 0.8295 - accuracy: 0.6993\n",
            "Epoch 5: val_loss improved from 0.89972 to 0.88921, saving model to best_model.keras\n",
            "633/633 [==============================] - 410s 647ms/step - loss: 0.8295 - accuracy: 0.6993 - val_loss: 0.8892 - val_accuracy: 0.6863\n",
            "Epoch 6/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 0.7722 - accuracy: 0.7204\n",
            "Epoch 6: val_loss did not improve from 0.88921\n",
            "633/633 [==============================] - 405s 640ms/step - loss: 0.7722 - accuracy: 0.7204 - val_loss: 0.8970 - val_accuracy: 0.6709\n",
            "Epoch 7/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.7497\n",
            "Epoch 7: val_loss improved from 0.88921 to 0.88214, saving model to best_model.keras\n",
            "633/633 [==============================] - 411s 648ms/step - loss: 0.6949 - accuracy: 0.7497 - val_loss: 0.8821 - val_accuracy: 0.6729\n",
            "Epoch 8/10\n",
            "633/633 [==============================] - ETA: 0s - loss: 0.5958 - accuracy: 0.7927\n",
            "Epoch 8: val_loss did not improve from 0.88214\n",
            "633/633 [==============================] - 405s 640ms/step - loss: 0.5958 - accuracy: 0.7927 - val_loss: 0.9109 - val_accuracy: 0.6729\n",
            "Epoch 9/10\n",
            "138/633 [=====>........................] - ETA: 4:54 - loss: 0.4791 - accuracy: 0.8367"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "results = model.evaluate(test_gen, steps=len(test_gen))\n",
        "test_loss, test_accuracy = results\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "DXSsK7n7P0Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Model Summary and Visualization\n",
        "\n",
        "The `model.summary()` method outputs the architecture of the model, listing each layer, its type, output shape, and number of parameters. This helps understanding the depth and complexity of the model's structure. Additionally, the `plot_model` function is used to generate a graphical representation of the model, which includes the shapes and names of each layer, offering a visual reference of the model's design.\n",
        "\n",
        "### Generating and Evaluating Predictions\n",
        "\n",
        "After training, predictions are generated for the test set using the `model.predict` method. This function computes the model's output predictions. The `np.argmax` function is then applied to these predictions to convert the model outputs from probabilities to explicit class predictions.\n",
        "\n",
        "### Collecting True Classes and Generating Confusion Matrix\n",
        "\n",
        "To evaluate the accuracy of our model's predictions, it is necessary to compare these predicted classes against the true classes from the test set. This comparison is facilitated through the collection of true class labels directly from the test generator. Subsequently, a confusion matrix is generated using the `confusion_matrix` function. This matrix visualizes the performance of a classification model, showing the actual versus predicted classifications, which helps in identifying how well the model is performing with respect to different classes. The `seaborn.heatmap` function is then used to plot the confusion matrix, providing a color-coded visualization of the results, which makes it easier to interpret the model's accuracy and misclassifications.\n",
        "\n"
      ],
      "metadata": {
        "id": "z2AWRmDMpZ5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "# Generate a plot of the model\n",
        "plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "-j1vR9VcpZGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Generate predictions using the test generator\n",
        "predictions = model.predict(test_gen, steps=len(test_gen))\n",
        "\n",
        "# Obtain the predicted classes by taking the argmax of the predictions array\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# test_gen should also be outputting the true classes in the same order as the predictions:\n",
        "# We need to collect all true classes from the generator (in the same order)\n",
        "true_classes = []\n",
        "for _, labels in test_gen:\n",
        "    true_classes.extend(np.argmax(labels, axis=1))\n",
        "    if len(true_classes) >= len(predictions):\n",
        "        break  # Stop once we have all the labels we need\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_classes[:len(predictions)], predicted_classes)\n",
        "\n",
        "print(cm)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "o0zNgkl56J9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4zTsYozZ7XKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summary of Confusion Matrix\n",
        "Classes 3, 4, 5, 6, and 7 show a particular need for augmentation due to low true positives and high confusion with other classes. Class 0 also needs attention due to being frequently confused with other high-incidence classes. These focused augmentations aim to enhance the model's ability to distinguish these classes more clearly, which should improve overall accuracy and reduce misclassifications.\n",
        "\n",
        "we will try and address this through resampling methods. Ideally we would jsut augment the data.\n",
        "\n",
        "####SEE section of repository labled experimental_code for our attempt at creating GAN images."
      ],
      "metadata": {
        "id": "NTCa0XIjR5TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Further Model Evaluation and Misclassification Analysis\n",
        "\n",
        "The process starts by loading a previously trained model from disk using `load_model`. This model is then used to evaluate its performance on unseen data provided by a test generator. The evaluation consists of several steps:\n",
        "\n",
        "#### Loading the Model\n",
        "The model is loaded from a saved file, ensuring that the most effective version of the model, as determined by prior validations, is used for evaluations.\n",
        "\n",
        "#### Fetching Predictions and Labels\n",
        "A custom function `get_predictions_and_labels` is employed to iterate over the test generator. This function fetches batches of data (both images and metadata) and corresponding labels, then uses the model to predict each batch. Predictions and true labels are accumulated from all batches, allowing for comprehensive evaluation.\n",
        "\n",
        "#### Calculating Confusion Matrix and Classification Report\n",
        "Once predictions are gathered they are converted from softmax probabilities to class predictions using `np.argmax`. With predictions and true labels in hand, a confusion matrix is generated to visually assess the model's performance across different classes. This matrix highlights which classes are being confused with others, providing insight into potential biases or weaknesses in the model.\n",
        "\n",
        "A classification report is also generated to provide key metrics for each class, such as precision, recall, and F1-score. This report helps in understanding the model's accuracy and identifying classes that might require more focus during further model training.\n",
        "\n",
        "#### Visualizing Misclassified Examples\n",
        "Finally, the script identifies and displays a set of misclassified images along with their true and predicted labels. This visualization is crucial for diagnosing what might be causing the errors, potentially guiding further data collection, augmentation strategies, or model adjustments.\n",
        "\n",
        "Each of these steps plays a role in evaluating the model's real-world applicability and robustness, ensuring that the model not only performs but also meets practical expectations."
      ],
      "metadata": {
        "id": "e4WEe_Pe4GPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets just run it a few more times and see if it can get any better before it gets any worse..."
      ],
      "metadata": {
        "id": "4qNKnHL5p2ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the previously saved model\n",
        "model = load_model('/content/best_model.keras')\n",
        "\n",
        "# Define your callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=2, verbose=1),\n",
        "    ModelCheckpoint(filepath='/content/best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Path where the checkpoint will be saved\n",
        "model_dir = '/content/drive/My Drive/Colab Models'\n",
        "# Generate a unique identifier based on the current date and time\n",
        "unique_identifier = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "checkpoint_filepath = os.path.join(model_dir, f'best_model_{unique_identifier}.keras')\n",
        "\n",
        "\n",
        "# Continue training with generators and include callbacks\n",
        "history2 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    # Adjust the total epochs as needed\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "BstjOq4WvTz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = load_model('/content/best_model.keras')\n",
        "\n",
        "# Function to fetch data and labels from the generator\n",
        "def get_predictions_and_labels(generator):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for _ in range(len(generator)):\n",
        "        # Getting the next batch of data\n",
        "        (images, metadata), labels = generator.__getitem__(_)\n",
        "        # Predict this batch\n",
        "        batch_predictions = model.predict([images, metadata])\n",
        "        # Store predictions and labels\n",
        "        predictions.append(batch_predictions)\n",
        "        true_labels.append(labels)\n",
        "\n",
        "    # Concatenate all batches\n",
        "    predictions = np.vstack(predictions)\n",
        "    true_labels = np.vstack(true_labels)\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Using the test generator to get predictions and labels\n",
        "predictions, true_labels = get_predictions_and_labels(test_gen)\n",
        "\n",
        "# Convert softmax probabilities to class predictions\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(true_labels, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "# Generate a classification report\n",
        "print('\\nClassification Report:\\n', classification_report(true_classes, predicted_classes))\n",
        "\n",
        "# Identify misclassified examples\n",
        "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
        "print(f'Total misclassified samples: {len(misclassified_indices)}')\n",
        "\n",
        "# Assuming you want to visualize some misclassified images:\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < len(misclassified_indices):\n",
        "        idx = misclassified_indices[i]\n",
        "        img = test_gen.image_paths[idx]  # Assuming image paths are stored here\n",
        "        true_label, pred_label = true_classes[idx], predicted_classes[idx]\n",
        "        ax.imshow(plt.imread(img))  # Reading the image from path\n",
        "        ax.set_title(f'True: {true_label}, Pred: {pred_label}')\n",
        "        ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N7xEntx2yeYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Classifier Performance Using ROC Curves\n",
        "\n",
        "To evaluate the performance of the classifier on a per-class basis, the Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC) are computed. These metrics are crucial for assessing the model's ability to distinguish between classes. Here's a breakdown of the steps involved in this evaluation:\n",
        "\n",
        "#### Preparing Labels and Predictions\n",
        "Firstly, the true labels are converted to a 1-dimensional array if they are not already in this format, ensuring they can be directly compared with the predicted probabilities. Predictions are checked to ensure they contain a probability for each class.\n",
        "\n",
        "#### Computing ROC Curves and AUC Values\n",
        "For each class, the ROC curve is calculated by:\n",
        "- Isolating the prediction probabilities for that class.\n",
        "- Creating a binary outcome array for the class, where the class of interest is labeled `1` and all others `0`.\n",
        "- Using the `roc_curve` function from `sklearn.metrics` to compute the true positive rate (TPR) and false positive rate (FPR) at various threshold settings.\n",
        "- Calculating the AUC value to quantify the overall ability of the model to discriminate between positive and negative classes for each specific class.\n",
        "\n",
        "#### Plotting the ROC Curves\n",
        "Each class's ROC curve is plotted with a unique color, and the AUC score is displayed in the legend to provide a visual and numerical representation of classifier performance. The diagonal line represents a random classifier's performance for comparison.\n",
        "\n",
        "- **X-axis**: False Positive Rate (FPR) — represents the proportion of negative data points that are mistakenly considered positive.\n",
        "- **Y-axis**: True Positive Rate (TPR) — represents the proportion of actual positives correctly identified.\n",
        "\n",
        "The area under each curve (AUC) provides a single measure of overall performance regardless of the classification threshold. The closer the AUC is to 1, the better the model is at predicting positive classes as positive and negative classes as negative. Values closer to 0.5 suggest no discriminative ability, equivalent to random guessing.\n",
        "\n",
        "This detailed analysis helps in identifying which classes the model performs well on and which ones might require further tuning, potentially guiding further data augmentation, additional training, or algorithm adjustments.\n"
      ],
      "metadata": {
        "id": "_WhYhVOuPPKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that true_labels is a 1D array of class indices\n",
        "if len(true_labels.shape) > 1:\n",
        "    true_labels = np.argmax(true_labels, axis=1)\n",
        "\n",
        "# Ensure predictions are probabilities\n",
        "if predictions.shape[1] == len(np.unique(true_labels)):\n",
        "    print(\"Predictions appear to be properly formatted.\")\n",
        "\n",
        "n_classes = len(np.unique(true_labels))\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "for i in range(n_classes):\n",
        "    # Isolate the probabilities for the current class\n",
        "    class_probs = predictions[:, i]\n",
        "    # Create a binary outcome for this class\n",
        "    class_true = (true_labels == i).astype(int)\n",
        "    fpr[i], tpr[i], _ = roc_curve(class_true, class_probs)\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, n_classes))\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve by class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MXn21DHQ1Oca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8: \"Teachers: The Architects of Tomorrow's... Model\""
      ],
      "metadata": {
        "id": "Y6x75FMpMDJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge Distillation:\n",
        "Uses our current model (the \"teacher\") to train a simpler model (the \"student\"). The student model learns to mimic the teacher model's behavior while often generalizing better on unseen data due to its simplicity.\n",
        "\n",
        "### Fine-tuning with Regularization:\n",
        "Load the weights of the best model and continue training with increased regularization (L1/L2 regularization) to force the model to learn more robust features, just because these students are in a BOOTCAMP."
      ],
      "metadata": {
        "id": "PuJo0oF1FgVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###And TA's help\n",
        "\n",
        "lets add in another layer of model prediction using visualBert"
      ],
      "metadata": {
        "id": "Pu4fXxaspGk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Paths to your data\n",
        "image_dir = \"/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\n",
        "metadata_csv_path = \"/content/full_metadata.csv\"\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv(metadata_csv_path)\n",
        "\n",
        "# Prepare the image dataset\n",
        "image_paths = metadata['image_path'].apply(lambda x: f\"{image_dir}/{x}\")\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])  # Resize to the size expected by VisualBERT\n",
        "    image = image / 255.0  # Normalize pixel values\n",
        "    return image\n",
        "\n",
        "image_dataset = image_dataset.map(preprocess_image)\n",
        "\n",
        "# Load the pre-trained VisualBERT model\n",
        "visual_bert_hub_url = \"https://tfhub.dev/google/visualbert/1\"  # Assuming this URL is correct\n",
        "visual_bert_layer = hub.KerasLayer(visual_bert_hub_url, trainable=True)\n",
        "\n",
        "# Drop unnecessary columns from metadata after preparing image paths\n",
        "metadata.drop(columns=['image', 'image_path', 'lesion_id'], inplace=True)\n",
        "\n",
        "# Prepare the metadata features\n",
        "metadata_features = metadata.values\n",
        "metadata_dataset = tf.data.Dataset.from_tensor_slices(metadata_features)\n",
        "\n",
        "# Combine image and metadata into a single dataset\n",
        "combined_dataset = tf.data.Dataset.zip((image_dataset, metadata_dataset))\n",
        "\n",
        "# Define the model\n",
        "input_image = Input(shape=(224, 224, 3), name=\"input_image\")\n",
        "# Adjust the shape according to actual metadata features\n",
        "input_metadata = Input(shape=(13,), name=\"input_metadata\")\n",
        "\n",
        "# VisualBERT processing\n",
        "visual_bert_output = visual_bert_layer(input_image)\n",
        "\n",
        "# Concatenate VisualBERT output with metadata\n",
        "combined_features = Concatenate()([visual_bert_output, input_metadata])\n",
        "\n",
        "# Classification head\n",
        "classification_output = Dense(8, activation='softmax')(combined_features)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=[input_image, input_metadata], outputs=classification_output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Define training procedure\n",
        "# Assuming you have a way to split the combined_dataset into training and validation datasets\n",
        "# Define training procedure with prefetching\n",
        "train_dataset = combined_dataset.take(int(len(combined_dataset)*0.8)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = combined_dataset.skip(int(len(combined_dataset)*0.8)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "80saIQmgpDQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(val_dataset)\n",
        "print(\"Loss, Accuracy:\", results)"
      ],
      "metadata": {
        "id": "2ZsPArjSvJLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/best_ta_model.keras')"
      ],
      "metadata": {
        "id": "ATACIVakvJHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.3, 0.7)\n",
        "    l1_value = trial.suggest_loguniform('l1_value', 1e-5, 1e-3)\n",
        "    l2_value = trial.suggest_loguniform('l2_value', 1e-4, 1e-2)\n",
        "\n",
        "    # Model building function with tuned hyperparameters\n",
        "    def build_tuned_student_model():\n",
        "        from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\n",
        "        from keras.models import Model\n",
        "        from keras.optimizers import Adam\n",
        "        from keras.regularizers import l1_l2\n",
        "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "        # Define the model architecture\n",
        "        image_input = Input(shape=(224, 224, 3), name='image_input')\n",
        "        x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        x = Flatten()(x)\n",
        "        image_branch = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=l1_value, l2=l2_value))(x)\n",
        "\n",
        "        metadata_input = Input(shape=(13,), name='metadata_input')\n",
        "        y = Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=l1_value, l2=l2_value))(metadata_input)\n",
        "        metadata_branch = BatchNormalization()(y)\n",
        "\n",
        "        # Combine the image and metadata branches\n",
        "        combined = concatenate([image_branch, metadata_branch])\n",
        "        z = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1_value, l2=l2_value))(combined)\n",
        "        z = Dropout(dropout_rate)(z)\n",
        "        output = Dense(8, activation='softmax')(z)\n",
        "\n",
        "        # Compile the model\n",
        "        student_model = Model(inputs=[image_input, metadata_input], outputs=output)\n",
        "        student_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        return student_model\n",
        "\n",
        "    # Compile and fit the model\n",
        "    student_model = build_tuned_student_model()\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=2, verbose=1),\n",
        "        ModelCheckpoint(filepath='/content/student_best_model_tuned.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "    history = student_model.fit(train_gen, validation_data=val_gen, epochs=6, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), callbacks=callbacks)\n",
        "\n",
        "    # Report the best validation loss to the trial\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "    return best_val_loss\n",
        "\n",
        "# Create a study object with a median pruner\n",
        "study = optuna.create_study(direction='minimize', pruner=MedianPruner(n_startup_trials=10,\n",
        "                                                                      n_warmup_steps=3,\n",
        "                                                                      interval_steps=1\n",
        "                                                                      ))\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    study.optimize(objective, n_trials=50, n_jobs=4)\n",
        "\n",
        "# uncomment to run hypertuning\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n"
      ],
      "metadata": {
        "id": "0q0jz23pQpCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the teacher model\n",
        "teacher_model = load_model('/content/best_model.keras')\n",
        "\n",
        "# Define the student model with increased regularization\n",
        "def build_student_model(input_shape_image=(224, 224, 3), num_metadata_features=13, num_classes=8):\n",
        "    # Image input branch - Reuses the same architecture as the teacher model\n",
        "    image_input = Input(shape=input_shape_image, name='image_input')\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    image_branch = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "\n",
        "    # Metadata input branch - Simplified version of the teacher model\n",
        "    metadata_input = Input(shape=(num_metadata_features,), name='metadata_input')\n",
        "    y = Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(metadata_input)\n",
        "    metadata_branch = BatchNormalization()(y)\n",
        "\n",
        "    # Combine the outputs of the two branches\n",
        "    combined = concatenate([image_branch, metadata_branch])\n",
        "    z = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(combined)\n",
        "    z = Dropout(0.5)(z)\n",
        "    output = Dense(num_classes, activation='softmax')(z)\n",
        "\n",
        "    # Create the student model\n",
        "    student_model = Model(inputs=[image_input, metadata_input], outputs=output)\n",
        "\n",
        "    # Compile the student model\n",
        "    student_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return student_model\n",
        "\n",
        "# Build the student model\n",
        "student_model = build_student_model()\n",
        "\n",
        "# Define callbacks for the student model\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
        "    ModelCheckpoint(filepath='/content/student_best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Initialize the training generator\n",
        "train_gen = DualInputGenerator(\n",
        "    image_paths=X_train['image_path'].values,\n",
        "    metadata=X_train.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_train.values,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Initialize the validation generator\n",
        "val_gen = DualInputGenerator(\n",
        "    image_paths=X_val['image_path'].values,\n",
        "    metadata=X_val.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_val.values,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Fit the student model -\n",
        "history = student_model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "E0I90ikWFzuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where the model should be saved\n",
        "model_path = '/content/best_student_model.keras'\n",
        "\n",
        "# Save the model\n",
        "student_model.save(model_path)\n",
        "\n",
        "print(f\"Model saved successfully at {model_path}\")\n"
      ],
      "metadata": {
        "id": "pd3Z2FJ2VRGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = student_model.evaluate(test_gen)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "qu7paceXXliw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####These students requires a lot of training....\n",
        "but it looks like they are improving over time.\n"
      ],
      "metadata": {
        "id": "iPJUSluITz12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the filepath for the saved model\n",
        "filepath = '/content/best_student_model.keras'\n",
        "\n",
        "# Define the ModelCheckpoint callback to save the best model\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath,\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='min',\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback to stop training when no improvement is seen\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    # Adjust the patience parameter based on your tolerance for waiting,\n",
        "    # this one is set high because im going to bed...\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# List of callbacks\n",
        "callbacks = [checkpoint, early_stopping]\n",
        "\n",
        "# Continue training the model\n",
        "history_continued = student_model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    # We set a higher number since training will stop automatically\n",
        "    epochs=50,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "WCCBNDiLTyzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = student_model.evaluate(test_gen)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "L-3azeZlH8N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teamwork makes the dream work."
      ],
      "metadata": {
        "id": "UIaugk72Mc94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets see how they perform as a team"
      ],
      "metadata": {
        "id": "Om2-43los9x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming models are already loaded; otherwise, load them:\n",
        "teacher_model = load_model('/content/best_model.keras')\n",
        "student_model = load_model('/content/best_student_model.keras')\n",
        "teaching_assistant_model = load_model('/content/best_ta_model.keras')\n",
        "\n",
        "# Function to evaluate ensemble accuracy\n",
        "def evaluate_ensemble_accuracy(teacher_model, student_model, test_gen):\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # Correctly unpack the data from the generator\n",
        "    for (images, metadata), labels in test_gen:\n",
        "        # Predict with both models\n",
        "        teacher_predictions = teacher_model.predict([images, metadata])\n",
        "        student_predictions = student_model.predict([images, metadata])\n",
        "\n",
        "        # Average the predictions (change based on each model's acc results)\n",
        "        averaged_predictions = (teacher_predictions * 0.8) + (student_predictions * 0.2)\n",
        "\n",
        "        # Convert probabilities to predicted class index\n",
        "        predicted_classes = np.argmax(averaged_predictions, axis=1)\n",
        "        true_classes = np.argmax(labels, axis=1)  # Assuming labels are one-hot encoded\n",
        "\n",
        "        # Increment correct count\n",
        "        correct_count += np.sum(predicted_classes == true_classes)\n",
        "        total_count += labels.shape[0]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_count / total_count\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate ensemble accuracy using the provided test generator\n",
        "accuracy = evaluate_ensemble_accuracy(teacher_model, student_model, test_gen)\n",
        "print(\"Ensemble accuracy: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "id": "jYDZeSsKEWjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQfp_PB5kfXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXXolT0YkfSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, well not every dream comes true, but we came this far, lets apply the models"
      ],
      "metadata": {
        "id": "JLne96qaMt56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source directory containing the files you want to copy\n",
        "source_dir = '/content/sample_data'\n",
        "\n",
        "# Target directory on Google Drive\n",
        "target_dir = '/content/drive/MyDrive/metadata_and_models'\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    shutil.copy(os.path.join(source_dir, file_name), os.path.join(target_dir, file_name))\n",
        "\n",
        "print(\"Files copied successfully!\")\n"
      ],
      "metadata": {
        "id": "kLHTRdVmVt5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: **The Interface** ⚡\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fCM0RYQ6fov0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Interface (UI)\n",
        "A simple web-based UI allows users to upload lesion images, input relevant metadata, and receive a prediction. We will use Gradio as learned in class for this model."
      ],
      "metadata": {
        "id": "Y_b53V90Dlbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio --upgrade\n",
        "! pip install --upgrade gradio\n",
        "! pip install gradio ai"
      ],
      "metadata": {
        "id": "H3vpy5AktpxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from gradio import Image, Number, Radio, Dropdown\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "vJ11PNQbghbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Loading and Prediction Testing\n",
        "\n",
        "#### Loading the Pre-trained Model\n",
        "The script begins by loading a pre-trained model from a specified path. This model is essential for predicting the type of skin lesions based on image data and associated metadata.\n",
        "\n",
        "#### Model Architecture Overview\n",
        "Once the model is loaded, `model.summary()` is called to print the architecture of the model. This summary provides insight into the model's layers, their shapes, and parameters, which is crucial for understanding how the model processes input data.\n",
        "\n",
        "#### Dummy Prediction\n",
        "To ensure the model is functioning correctly, a dummy input array is created using random data, shaped appropriately for the model's expected input (in this case, 224x224x3 for image data). This array is then used to make a prediction, testing the model's readiness for actual data.\n",
        "\n",
        "### Image Preprocessing and Prediction Explanation Function\n",
        "\n",
        "#### Image Preprocessing\n",
        "The script defines a function `preprocess_image` that takes an uploaded image file, resizes it to fit the model's input requirements, normalizes the pixel values, and expands its dimensions to include a batch size for model input compatibility.\n",
        "\n",
        "#### Prediction and Explanation Generation\n",
        "Another function, `predict_and_explain`, is designed to handle the end-to-end process from image and metadata input through to generating a human-readable explanation of the prediction. This function:\n",
        "- Preprocesses the input image for model prediction.\n",
        "- Uses the model to predict the lesion type based on the image.\n",
        "- Maps the prediction to a readable class description.\n",
        "- Optionally, uses GPT-3 to generate a detailed explanation of the diagnosis, integrating the lesion's metadata for a comprehensive overview.\n",
        "\n",
        "### User Interface for Skin Lesion Classification\n",
        "\n",
        "#### Interface Setup\n",
        "The script utilizes `gr.Interface` from the Gradio library to create an interactive web interface. This interface allows users to upload images of skin lesions and input relevant metadata (age, sex, and anatomical site).\n",
        "\n",
        "#### Interface Launch\n",
        "Finally, the interface is launched, making it accessible via a web browser. Users can interact with the model, upload images, input metadata, and receive predictions along with explanations right in the interface.\n",
        "\n",
        "### Usage and Disclaimer\n",
        "The interface includes a title and a detailed description, advising users on how to use the tool and noting that predictions should not replace professional medical advice. This ensures users understand the context and limitations of the model predictions.\n",
        "\n",
        "This comprehensive setup allows for an accessible and user-friendly way to leverage advanced machine learning models for educational and preliminary diagnostic support in dermatology.\n"
      ],
      "metadata": {
        "id": "zsqTPa3RhUkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "# Load the model\n",
        "model_path = '/content/best_model.h5'\n",
        "model = keras.models.load_model(model_path)\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Create a dummy input array for metadata\n",
        "test_metadata = np.random.random((1, 13))  # Assuming metadata has shape (1, 13)\n",
        "\n",
        "# Combine image data and metadata\n",
        "test_input_image = np.random.random((1, 224, 224, 3))  # Assuming image data has shape (1, 224, 224, 3)\n",
        "test_input = [test_input_image, test_metadata]\n",
        "\n",
        "# Make a prediction\n",
        "test_prediction = model.predict(test_input)\n",
        "print(test_prediction)\n"
      ],
      "metadata": {
        "id": "K3tNqmqHRVlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tri-Model Predictions\n",
        "##The Sudents & The Teachers 🥇\n",
        "###...and a little help from TA's\n",
        "###An Ode to Python AI Coding Bootcamp\n",
        "\n",
        "**Initial Binary Classification**: Here we use the binary medical classifier, Cancer-Net SCa Model, to determine if a lesion is malignant or benign. This first step allows the system to filter and focus the subsequent predictions.\n",
        "\n",
        "**Conditional Multiclass Classification**:\n",
        "- **If Malignant**: Route the input to the student-teacher model but restrict the classification to the malignant classes only. This reduces the complexity of the prediction task for these cases.\n",
        "- **If Benign**: Proceed with the student-teacher model using all classes or possibly excluding known malignant classes based on the prior binary result.\n",
        "\n",
        "This conditional approach leverages the strength of both models and optimizes the classification task by reducing the decision space when appropriate, potentially increasing overall accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "C-57LpOgepAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import gradio as gr\n",
        "\n",
        "# Load the pre-trained teacher and student models\n",
        "teacher_model_path = '/content/best_model.keras'\n",
        "student_model_path = '/content/best_student_model.keras'\n",
        "teacher_model = load_model(teacher_model_path)\n",
        "student_model = load_model(student_model_path)\n",
        "\n",
        "# Load the binary classifier, our TA\n",
        "# last model test; loss: 0.3760 - accuracy: 0.8446 - val_loss: 0.3826 - val_accuracy: 0.8474\n",
        "\n",
        "binary_classifier_path = 'content/best_cancer_net_sca_model.keras'\n",
        "binary_classifier = load_model(binary_classifier_path)\n",
        "\n",
        "# Dictionary of diagnosis descriptions\n",
        "diagnosis_descriptions = {\n",
        "    'MEL': 'Melanoma: a serious form of skin cancer that begins in cells known as melanocytes.',\n",
        "    'NV': 'Melanocytic nevus: a common type of skin growth that often appears as a small, dark brown spot.',\n",
        "    'BCC': 'Basal cell carcinoma: a type of skin cancer that most often develops on areas exposed to the sun.',\n",
        "    'AK': 'Actinic keratosis: a rough, scaly patch on the skin caused by years of sun exposure.',\n",
        "    'BKL': 'Benign keratosis: a non-cancerous skin condition that appears as a waxy brown, black, or tan growth.',\n",
        "    'DF': 'Dermatofibroma: a common growth on the skin, usually found on the lower legs, that can be pink, red, or brown.',\n",
        "    'VASC': 'Vascular lesion: a type of abnormal growth or mark on the skin that is made up of blood vessels.',\n",
        "    'SCC': 'Squamous cell carcinoma: a common form of skin cancer that develops in the squamous cells.'\n",
        "}\n",
        "\n",
        "def preprocess_image(image):\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    image = np.array(image)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = image / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    return image\n",
        "\n",
        "def predict_and_explain(image, age, sex, location):\n",
        "    # Preprocess the image\n",
        "    image = preprocess_image(image)\n",
        "\n",
        "    # Encode the sex and location for the student and teacher model\n",
        "    sex_encoded = [1 if sex == 'Male' else 0, 1 if sex == 'Female' else 0, 1 if sex == 'Other' else 0]\n",
        "    location_encoded = [1 if location == loc else 0 for loc in ['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other']]\n",
        "\n",
        "    # Construct metadata for student and teacher models, which seems to require more detailed encoding\n",
        "    std_teacher_metadata = np.array([[age] + sex_encoded + location_encoded])\n",
        "\n",
        "    # Construct metadata for binary classifier, adjust this based on your actual model's requirements\n",
        "    binary_classifier_metadata = np.array([[1 if condition in [\"MEL\", \"melanocytic\", \"NV\", \"benign\", \"concomitant_biopsy\"] else 0 for condition in location_encoded]])\n",
        "\n",
        "    # Binary classification to determine malignancy\n",
        "    is_malignant = binary_classifier.predict([image, binary_classifier_metadata])[0][0] > 0.5\n",
        "\n",
        "    # Remaining part of your function\n",
        "    if is_malignant:\n",
        "        relevant_classes = ['MEL', 'BCC', 'SCC']  # Malignant classes\n",
        "        mask = np.isin(np.array(list(diagnosis_descriptions.keys())), relevant_classes)\n",
        "    else:\n",
        "        mask = np.ones(len(diagnosis_descriptions), dtype=bool)\n",
        "\n",
        "    # Predict with both models\n",
        "    teacher_prediction = teacher_model.predict([image, std_teacher_metadata])[0]\n",
        "    student_prediction = student_model.predict([image, std_teacher_metadata])[0]\n",
        "    average_prediction = (teacher_prediction + student_prediction) / 2\n",
        "    masked_prediction = average_prediction[mask]\n",
        "\n",
        "    predicted_class = np.argmax(masked_prediction)\n",
        "    class_labels = np.array(list(diagnosis_descriptions.keys()))[mask]\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "\n",
        "    description = diagnosis_descriptions[predicted_label]\n",
        "    response = f\"Predicted Lesion Type: {predicted_label}\\nDescription: {description}\\nMalignant: {'Yes' if is_malignant else 'No'}\"\n",
        "    return response\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_and_explain,\n",
        "    inputs=[\n",
        "        gr.Image(image_mode='RGB', sources=[\"upload\", \"webcam\"], type='pil'),\n",
        "        gr.Number(label=\"Age\"),\n",
        "        gr.Radio(choices=['Male', 'Female', 'Other'], label=\"Sex\"),\n",
        "        gr.Dropdown(choices=['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other'], label=\"Anatomical Site\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Skin Lesion Classifier\",\n",
        "    description=\"Upload an image of a skin lesion and enter metadata to predict its type. The results provide a preliminary classification and are not a substitute for professional medical advice.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "2zWW1j-19oA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skin Lesion Classification Project Summary\n",
        "\n",
        "&emsp;&emsp;This project focused on classifying skin lesions using deep learning techniques with the ISIC 2019 dataset. A central aspect was the development of a dual-input model that integrates both image data and patient metadata, aiming to leverage additional context for improved diagnostic accuracy. TheDualInputGenerator was utilized for handling the simultaneous input of images and metadata during training. This setup facilitated the effective integration of both data types. The training process was fine-tuned using strategies like early stopping and model checkpoints, ensuring excellent model performance without extensive straining the our computational resources. <br>\n",
        "<br>\n",
        "&emsp;&emsp;A Gradio interface was deployed to allow user interaction with the model, enabling uploads of skin lesion images and entry of relevant metadata for real-time predictions. This user-friendly interface makes the model accessible to non-experts and provides educational insights into different types of skin lesions, demonstrating the application's potential as a diagnostic aid. <br>\n",
        "<br>\n",
        "&emsp;&emsp;The project highlighted the application of mixed data models in medical image analysis, demonstrating how machine learning can be applied in healthcare settings. It also underscored the importance of user interface design in making complex models accessible and useful to a broader audience. There is potential to expand the model to include more diagnostic categories and enhance the interface with more detailed explanations of predictions. Further development could involve integrating AI-driven insights or additional data sources. User testing and feedback will be essential in refining the tool to better meet user needs. The ultimate goal could be to integrate this tool into medical systems for preliminary diagnostics."
      ],
      "metadata": {
        "id": "Jv-GrP5fJRjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xqGDL9RQJS-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SzoroRr4JTC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s7ZmIm69JTLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z6G1KtBXJTN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MOCO7VHmJTPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JInjAe-MJTR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vz8lRRtrJTTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import load_model\n",
        "# import gradio as gr\n",
        "\n",
        "# # Load the pre-trained teacher and student models\n",
        "# teacher_model_path = '/content/best_model.keras'\n",
        "# student_model_path = '/content/best_student_model.keras'\n",
        "# teacher_model = load_model(teacher_model_path)\n",
        "# student_model = load_model(student_model_path)\n",
        "\n",
        "# # Dictionary of diagnosis descriptions\n",
        "# diagnosis_descriptions = {\n",
        "#     'MEL': 'Melanoma: a serious form of skin cancer that begins in cells known as melanocytes.',\n",
        "#     'NV': 'Melanocytic nevus: a common type of skin growth that often appears as a small, dark brown spot.',\n",
        "#     'BCC': 'Basal cell carcinoma: a type of skin cancer that most often develops on areas exposed to the sun.',\n",
        "#     'AK': 'Actinic keratosis: a rough, scaly patch on the skin caused by years of sun exposure.',\n",
        "#     'BKL': 'Benign keratosis: a non-cancerous skin condition that appears as a waxy brown, black, or tan growth.',\n",
        "#     'DF': 'Dermatofibroma: a common growth on the skin, usually found on the lower legs, that can be pink, red, or brown.',\n",
        "#     'VASC': 'Vascular lesion: a type of abnormal growth or mark on the skin that is made up of blood vessels.',\n",
        "#     'SCC': 'Squamous cell carcinoma: a common form of skin cancer that develops in the squamous cells.',\n",
        "# }\n",
        "\n",
        "# disclaimer = \"\"\"\n",
        "# **Disclaimer:** This tool is intended for educational and entertainment purposes only and should not be used as a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition. Remember, this AI is not a medical doctor, and its assessments are not diagnoses. Use this tool responsibly and always consult with a healthcare professional for any medical concerns.\n",
        "# \"\"\"\n",
        "\n",
        "# def preprocess_image(image):\n",
        "#     if image.mode != 'RGB':\n",
        "#         image = image.convert('RGB')\n",
        "#     image = np.array(image)  # Convert PIL image to numpy array\n",
        "#     image = tf.image.resize(image, [224, 224])  # Resize to model expected dimensions\n",
        "#     image = image / 255.0  # Normalize pixel values\n",
        "#     image = np.expand_dims(image, axis=0)  # Add batch dimension for model prediction\n",
        "#     return image\n",
        "\n",
        "# def predict_and_explain(image, age, sex, location):\n",
        "#     # Preprocess the image\n",
        "#     image = preprocess_image(image)\n",
        "\n",
        "#     # Prepare metadata\n",
        "#     sex_encoded = [1 if sex == 'Male' else 0, 1 if sex == 'Female' else 0, 1 if sex == 'Other' else 0]\n",
        "#     location_encoded = [1 if location == loc else 0 for loc in ['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other']]\n",
        "#     default_values = [0] * (13 - 1 - len(sex_encoded) - len(location_encoded))\n",
        "#     metadata = np.array([[age] + sex_encoded + location_encoded + default_values])\n",
        "\n",
        "#     # Predict with both models\n",
        "#     teacher_prediction = teacher_model.predict([image, metadata])[0]\n",
        "#     student_prediction = student_model.predict([image, metadata])[0]\n",
        "\n",
        "#     # Average the predictions if models perform the same\n",
        "#     average_prediction = (teacher_prediction + student_prediction) / 2\n",
        "\n",
        "#     # Suppose the teacher model is generally more reliable\n",
        "#     # combined_predictions = 0.7 * teacher_predictions + 0.3 * student_predictions\n",
        "\n",
        "#     # Weighted Prediction method from both models\n",
        "#     # teacher_predictions = teacher_model.predict(test_input)\n",
        "#     # student_predictions = student_model.predict(test_input)\n",
        "#     # Weighted average of predictions\n",
        "#     # weights = [0.7, 0.3]  # Adjust weights as necessary\n",
        "#     # combined_predictions = weights[0] * teacher_predictions + weights[1] * student_predictions\n",
        "#     # Decode the final prediction\n",
        "#     # final_prediction = np.argmax(combined_predictions, axis=1)\n",
        "#     # print(\"Final Prediction:\", final_prediction)\n",
        "\n",
        "#     predicted_class = np.argmax(average_prediction)\n",
        "#     class_labels = list(diagnosis_descriptions.keys())\n",
        "#     predicted_label = class_labels[predicted_class]\n",
        "\n",
        "#     # Build the response\n",
        "#     description = diagnosis_descriptions[predicted_label]\n",
        "#     response = f\"Predicted Lesion Type: {predicted_label}\\nDescription: {description}\"\n",
        "\n",
        "#     return response\n",
        "\n",
        "# iface = gr.Interface(\n",
        "#     fn=predict_and_explain,\n",
        "#     inputs=[\n",
        "#         gr.Image(image_mode='RGB', sources=[\"upload\", \"webcam\"], type='pil'),\n",
        "#         gr.Number(label=\"Age\"),\n",
        "#         gr.Radio(choices=['Male', 'Female', 'Other'], label=\"Sex\"),\n",
        "#         gr.Dropdown(choices=['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other'], label=\"Anatomical Site\")\n",
        "#     ],\n",
        "#     outputs=\"text\",\n",
        "#     title=\"Skin Lesion Classifier\",\n",
        "#     description=\"Upload an image of a skin lesion and enter metadata to predict its type. The results provide a preliminary classification and are not a substitute for professional medical advice.\"\n",
        "# )\n",
        "\n",
        "# iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "d2qmrTrleoV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "our original working model below"
      ],
      "metadata": {
        "id": "JKTI2l6Uf3Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/best_model.h5')\n",
        "\n",
        "# Dictionary of diagnosis descriptions\n",
        "diagnosis_descriptions = {\n",
        "    'MEL': 'Melanoma: a serious form of skin cancer that begins in cells known as melanocytes.',\n",
        "    'NV': 'Melanocytic nevus: a common type of skin growth that often appears as a small, dark brown spot.',\n",
        "    'BCC': 'Basal cell carcinoma: a type of skin cancer that most often develops on areas exposed to the sun.',\n",
        "    'AK': 'Actinic keratosis: a rough, scaly patch on the skin caused by years of sun exposure.',\n",
        "    'BKL': 'Benign keratosis: a non-cancerous skin condition that appears as a waxy brown, black, or tan growth.',\n",
        "    'DF': 'Dermatofibroma: a common growth on the skin, usually found on the lower legs, that can be pink, red, or brown.',\n",
        "    'VASC': 'Vascular lesion: a type of abnormal growth or mark on the skin that is made up of blood vessels.',\n",
        "    'SCC': 'Squamous cell carcinoma: a common form of skin cancer that develops in the squamous cells.',\n",
        "    'UNK': 'None of the others: the lesion does not fit into any of the other categories.'\n",
        "}\n",
        "\n",
        "def preprocess_image(image):\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    image = np.array(image)  # Convert PIL image to numpy array\n",
        "    image = tf.image.resize(image, [224, 224])  # Resize to model expected dimensions\n",
        "    image = image / 255.0  # Normalize pixel values\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension for model prediction\n",
        "    return image\n",
        "\n",
        "def predict_and_explain(image, age, sex, location):\n",
        "    # Preprocess the image\n",
        "    image = preprocess_image(image)\n",
        "\n",
        "    # Prepare metadata\n",
        "    sex_encoded = [1 if sex == 'Male' else 0, 1 if sex == 'Female' else 0, 1 if sex == 'Other' else 0]\n",
        "    location_encoded = [1 if location == loc else 0 for loc in ['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other']]\n",
        "    default_values = [0] * (13 - 1 - len(sex_encoded) - len(location_encoded))\n",
        "    metadata = np.array([[age] + sex_encoded + location_encoded + default_values])\n",
        "\n",
        "    # Predict with model\n",
        "    prediction = model.predict([image, metadata])[0]\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    class_labels = list(diagnosis_descriptions.keys())\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "\n",
        "    # Build the response\n",
        "    description = diagnosis_descriptions[predicted_label]\n",
        "    response = f\"Predicted Lesion Type: {predicted_label}\\nDescription: {description}\"\n",
        "\n",
        "    return response\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_and_explain,\n",
        "    inputs=[\n",
        "        gr.Image(image_mode='RGB', sources=[\"upload\", \"webcam\"], type='pil'),  # Configured for image uploads and webcam captures\n",
        "        gr.Number(label=\"Age\"),\n",
        "        gr.Radio(choices=['Male', 'Female', 'Other'], label=\"Sex\"),\n",
        "        gr.Dropdown(choices=['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other'], label=\"Anatomical Site\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Skin Lesion Classifier\",\n",
        "    description=\"Upload an image of a skin lesion and enter metadata to predict its type. The results provide a preliminary classification and are not a substitute for professional medical advice.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "JVanQA1QKvHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knU_FohFVKSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}