{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/jkranyak/project_3/blob/main/project3.ipynb",
      "authorship_tag": "ABX9TyPe1uXw0rgIkqr1Jnl2xrPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkranyak/project_3/blob/experimental-push/project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Set up GPU memory growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"Memory growth enabled for GPU\")\n",
        "    except RuntimeError as e:\n",
        "        print(\"Failed to set memory growth:\", e)\n"
      ],
      "metadata": {
        "id": "C0udQUizzRwi",
        "outputId": "5e6cd280-e687-43d6-fdf3-260e099a8dc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory growth enabled for GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install isic-cli\n",
        "!pip install isic-cli\n",
        "!pip install kaggle\n",
        "!pip install imblearn\n",
        "!pip install scikit-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9dKgTXpPs_K",
        "outputId": "3c980183-0d19-40d8-be68-a36e7a477e04"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: isic-cli in /usr/local/lib/python3.10/dist-packages (10.0.0)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.1.7)\n",
            "Requirement already satisfied: django-s3-file-field-client>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.0.1)\n",
            "Requirement already satisfied: girder-cli-oauth-client<1.0.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (0.4.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from isic-cli) (4.7.0)\n",
            "Requirement already satisfied: isic-metadata>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.5.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from isic-cli) (10.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from isic-cli) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from isic-cli) (2.31.0)\n",
            "Requirement already satisfied: retryable-requests in /usr/local/lib/python3.10/dist-packages (from isic-cli) (0.1.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from isic-cli) (13.7.1)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.45.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.2.3)\n",
            "Requirement already satisfied: authlib in /usr/local/lib/python3.10/dist-packages (from girder-cli-oauth-client<1.0.0->isic-cli) (1.3.0)\n",
            "Requirement already satisfied: pyxdg in /usr/local/lib/python3.10/dist-packages (from girder-cli-oauth-client<1.0.0->isic-cli) (0.28)\n",
            "Requirement already satisfied: pydantic>=2.4 in /usr/local/lib/python3.10/dist-packages (from isic-metadata>=1.2.0->isic-cli) (2.6.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2024.2.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from retryable-requests->isic-cli) (1.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->isic-cli) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (4.11.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib->girder-cli-oauth-client<1.0.0->isic-cli) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib->girder-cli-oauth-client<1.0.0->isic-cli) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib->girder-cli-oauth-client<1.0.0->isic-cli) (2.22)\n",
            "Requirement already satisfied: isic-cli in /usr/local/lib/python3.10/dist-packages (10.0.0)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.1.7)\n",
            "Requirement already satisfied: django-s3-file-field-client>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.0.1)\n",
            "Requirement already satisfied: girder-cli-oauth-client<1.0.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (0.4.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from isic-cli) (4.7.0)\n",
            "Requirement already satisfied: isic-metadata>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.5.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from isic-cli) (10.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from isic-cli) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from isic-cli) (2.31.0)\n",
            "Requirement already satisfied: retryable-requests in /usr/local/lib/python3.10/dist-packages (from isic-cli) (0.1.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from isic-cli) (13.7.1)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from isic-cli) (1.45.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from isic-cli) (8.2.3)\n",
            "Requirement already satisfied: authlib in /usr/local/lib/python3.10/dist-packages (from girder-cli-oauth-client<1.0.0->isic-cli) (1.3.0)\n",
            "Requirement already satisfied: pyxdg in /usr/local/lib/python3.10/dist-packages (from girder-cli-oauth-client<1.0.0->isic-cli) (0.28)\n",
            "Requirement already satisfied: pydantic>=2.4 in /usr/local/lib/python3.10/dist-packages (from isic-metadata>=1.2.0->isic-cli) (2.6.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->isic-cli) (2024.2.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from retryable-requests->isic-cli) (1.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->isic-cli) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->isic-cli) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (4.11.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib->girder-cli-oauth-client<1.0.0->isic-cli) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib->girder-cli-oauth-client<1.0.0->isic-cli) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib->girder-cli-oauth-client<1.0.0->isic-cli) (2.22)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.10/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! isic user login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW33o_VNdZdO",
        "outputId": "705c4c95-d562-492e-fd1d-1d803c605a0b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello jessekranyak@gmail.com!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!isic collection list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHkk6C7UfgMk",
        "outputId": "bd70d6eb-d143-4427-a499-3266b8082b6e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mID \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName                                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPublic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPinned\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLocked\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDOI            \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
            "│ 249 │ BCN20000                                      │ True   │ False  │ False  │ None            │\n",
            "│ 61  │ Challenge 2016: Test                          │ True   │ True   │ True   │ None            │\n",
            "│ 74  │ Challenge 2016: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 69  │ Challenge 2017: Test                          │ True   │ True   │ True   │ None            │\n",
            "│ 60  │ Challenge 2017: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 71  │ Challenge 2017: Validation                    │ True   │ True   │ True   │ None            │\n",
            "│ 64  │ Challenge 2018: Task 1-2: Test                │ True   │ True   │ True   │ None            │\n",
            "│ 63  │ Challenge 2018: Task 1-2: Training            │ True   │ True   │ True   │ None            │\n",
            "│ 62  │ Challenge 2018: Task 1-2: Validation          │ True   │ True   │ True   │ None            │\n",
            "│ 67  │ Challenge 2018: Task 3: Test                  │ True   │ True   │ True   │ None            │\n",
            "│ 66  │ Challenge 2018: Task 3: Training              │ True   │ True   │ True   │ None            │\n",
            "│ 73  │ Challenge 2018: Task 3: Validation            │ True   │ True   │ True   │ None            │\n",
            "│ 65  │ Challenge 2019: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 70  │ Challenge 2020: Training                      │ True   │ True   │ True   │ None            │\n",
            "│ 97  │ Collection for ISBI 2016: 100 Lesion          │ True   │ False  │ True   │ None            │\n",
            "│     │ Classification                                │        │        │        │                 │\n",
            "│ 216 │ Consecutive biopsies for melanoma across year │ True   │ False  │ True   │ 10.34970/151324 │\n",
            "│     │ 2020                                          │        │        │        │                 │\n",
            "│ 75  │ Consumer AI apps                              │ True   │ False  │ True   │ 10.34970/401946 │\n",
            "│ 166 │ EASY Dermoscopy Expert Agreement Study        │ True   │ False  │ False  │ None            │\n",
            "│ 212 │ HAM10000                                      │ True   │ True   │ True   │ None            │\n",
            "│ 175 │ HIBA Skin Lesions                             │ True   │ False  │ True   │ 10.34970/559884 │\n",
            "│ 251 │ Hospital Italiano de Buenos Aires - Skin      │ True   │ False  │ True   │ 10.34970/587329 │\n",
            "│     │ Lesions Images (2019-2022)                    │        │        │        │                 │\n",
            "│ 176 │ Hospital Italiano de Buenos Aires Skin        │ True   │ False  │ True   │ 10.34970/432362 │\n",
            "│     │ Lesions                                       │        │        │        │                 │\n",
            "│ 217 │ Longitudinal overview images of posterior     │ True   │ False  │ True   │ 10.34970/630662 │\n",
            "│     │ trunks                                        │        │        │        │                 │\n",
            "│ 289 │ MSK-1                                         │ True   │ False  │ True   │ None            │\n",
            "│ 290 │ MSK-2                                         │ True   │ False  │ True   │ None            │\n",
            "│ 288 │ MSK-3                                         │ True   │ False  │ True   │ None            │\n",
            "│ 287 │ MSK-4                                         │ True   │ False  │ True   │ None            │\n",
            "│ 286 │ MSK-5                                         │ True   │ False  │ True   │ None            │\n",
            "│ 163 │ MSKCC Consecutive biopsies across year        │ True   │ False  │ True   │ None            │\n",
            "│     │ 2020_cohort                                   │        │        │        │                 │\n",
            "│ 77  │ Melanocytic lesions used for dermoscopic      │ True   │ False  │ True   │ 10.34970/108631 │\n",
            "│     │ feature annotations                           │        │        │        │                 │\n",
            "│ 294 │ Melanoma and Nevus Dermoscopy Images with     │ True   │ False  │ True   │ 10.34970/277003 │\n",
            "│     │ Confirmed Histopathological Diagnosis         │        │        │        │                 │\n",
            "│ 215 │ Newly-acquired and longer-existing acquired   │ True   │ False  │ True   │ 10.34970/408649 │\n",
            "│     │ melanoma and nevi                             │        │        │        │                 │\n",
            "│ 218 │ PROVe-AI                                      │ True   │ True   │ True   │ 10.34970/576276 │\n",
            "│ 328 │ Repeated Dermoscopic Images of Melanocytic    │ True   │ False  │ True   │ 10.34970/560760 │\n",
            "│     │ Lesions                                       │        │        │        │                 │\n",
            "│ 293 │ SONIC                                         │ True   │ False  │ True   │ None            │\n",
            "│ 292 │ UDA-1                                         │ True   │ False  │ True   │ None            │\n",
            "│ 291 │ UDA-2                                         │ True   │ False  │ True   │ None            │\n",
            "│ 285 │ lesions                                       │ True   │ False  │ False  │ None            │\n",
            "│ 172 │ screenshot_public_230207                      │ True   │ False  │ False  │ None            │\n",
            "└─────┴───────────────────────────────────────────────┴────────┴────────┴────────┴─────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Query the Dataset\n",
        "Endpoints: Use the API endpoints to query the dataset. Common operations include listing available images, retrieving image metadata, and downloading images.\n",
        "Filtering: Utilize query parameters to filter the dataset based on your criteria, such as diagnosis, image type, or other metadata."
      ],
      "metadata": {
        "id": "oonk5DVpQvQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "from skimage import io\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "Qydcku5hO9JU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPl5l2XioO78",
        "outputId": "1ab14a74-ff8e-40fd-ecaa-a92f640357b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment out if already running"
      ],
      "metadata": {
        "id": "2RJkPIfDe5gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the current path of kaggle.json file\n",
        "current_path = '/content/kaggle.json'\n",
        "\n",
        "# Desired path where the Kaggle API expects the kaggle.json file\n",
        "desired_path = '/root/.kaggle/kaggle.json'\n",
        "\n",
        "if os.path.exists(current_path):\n",
        "    os.makedirs(os.path.dirname(desired_path), exist_ok=True)\n",
        "    os.rename(current_path, desired_path)\n",
        "\n",
        "    # Set the file's permissions to avoid a permissions error\n",
        "    os.chmod(desired_path, 0o600)\n",
        "else:\n",
        "    print(f\"Error: '{current_path}' does not exist. Please upload the file.\")"
      ],
      "metadata": {
        "id": "oKED3V7tWbFI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/isic-2019"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVEOFai5W4Xv",
        "outputId": "24747b5f-387f-4d8d-b086-d9efaf99f469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading isic-2019.zip to /content\n",
            " 32% 2.92G/9.10G [01:46<03:44, 29.5MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q isic-2019.zip\n"
      ],
      "metadata": {
        "id": "XI5mnqj4W4cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv('/content/challenge-2019-training_metadata_2024-04-17.csv')\n",
        "metadata.rename(columns={'isic_id': 'image'}, inplace=True)\n",
        "# Define the new base path for the images\n",
        "base_path = Path('/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input')\n",
        "\n",
        "# Create the full image paths\n",
        "metadata['image_path'] = metadata['image'].apply(lambda x: base_path / f\"{x}.jpg\")\n",
        "\n",
        "# Example of how to display the first few image paths\n",
        "print(metadata[['image', 'image_path']].head())"
      ],
      "metadata": {
        "id": "hva5gtJKVqSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the metadata\n",
        "\n",
        "and\n",
        "\n",
        "Explore the Metadata CSV: Load the metadata.csv files for training, test, and validation sets to understand the structure and types of data available. This step is crucial for preprocessing and feature selection."
      ],
      "metadata": {
        "id": "BW0CK9GitFSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata['image_path'].head()"
      ],
      "metadata": {
        "id": "8jv2V82HyVtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the specified columns\n",
        "metadata.drop(columns=['copyright_license', 'attribution'], inplace=True)\n",
        "# Rename the column\n",
        "metadata.rename(columns={'isic_id': 'image'}, inplace=True)\n",
        "metadata.drop_duplicates(inplace=True)\n",
        "metadata.head()"
      ],
      "metadata": {
        "id": "QYU_D6gnTfaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata.shape"
      ],
      "metadata": {
        "id": "qa8AbATiCId0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the file into a DataFrame\n",
        "ground_truth = pd.read_csv('/content/ISIC_2019_Training_GroundTruth.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "ground_truth.shape"
      ],
      "metadata": {
        "id": "F3a6vJJAXluG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hIRZpHWLIVzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = Path('/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input')\n",
        "ground_truth['image_path'] = ground_truth['image'].apply(lambda x: image_dir / f\"{x}.jpg\")\n",
        "\n",
        "# Merge the ground_truth with metadata if necessary\n",
        "full_metadata = pd.merge(ground_truth, metadata, on='image', how='left')  # Adjust 'on' parameter as needed\n",
        "full_metadata.shape"
      ],
      "metadata": {
        "id": "gdODxFjP4Ydi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct the base path in 'image_path' column\n",
        "correct_base_path = \"/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\n",
        "\n",
        "full_metadata['image_path'] = full_metadata['image'].apply(lambda x: f\"{correct_base_path}/{x}.jpg\")\n",
        "\n",
        "# Verify the correction by printing the first few entries again\n",
        "print(full_metadata['image_path'].head())\n"
      ],
      "metadata": {
        "id": "_FDMb1wGHk3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_metadata.info()"
      ],
      "metadata": {
        "id": "0BjtJ7JO9mHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values for 'age_approx' with its median\n",
        "full_metadata['age_approx'].fillna(full_metadata['age_approx'].median(), inplace=True)\n",
        "\n",
        "# For categorical data, fill missing values with 'unknown'\n",
        "full_metadata['anatom_site_general'].fillna('unknown', inplace=True)\n",
        "full_metadata['sex'].fillna('unknown', inplace=True)\n",
        "full_metadata.info()\n"
      ],
      "metadata": {
        "id": "-W0QskH29_-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {\n",
        "    'image_path_x': ['path1.jpg', 'path2.jpg', 'path3.jpg'],\n",
        "    'image_path_y': ['path1.jpg', 'path2.jpg', 'path3.jpg'],\n",
        "    'image_path': ['path1.jpg', 'path2.jpg', 'path4.jpg']  # Notice the discrepancy in the last row\n",
        "}\n",
        "df = full_metadata\n",
        "\n",
        "# Step 1: Consolidate columns into one\n",
        "df['consolidated_path'] = df.apply(lambda x: x.dropna().unique()[0] if len(x.dropna().unique()) == 1 else None, axis=1)\n",
        "\n",
        "# Step 2: Identify discrepancies\n",
        "df['is_discrepant'] = df.apply(lambda x: len(x.dropna().unique()) != 1, axis=1)\n",
        "\n",
        "# Step 3: Count discrepancies\n",
        "discrepancy_count = df['is_discrepant'].sum()\n",
        "\n",
        "# Step 4: Report discrepancies\n",
        "discrepancy_report = df[df['is_discrepant']]\n",
        "\n",
        "print(\"Number of discrepancies:\", discrepancy_count)\n",
        "print(\"Discrepancy report:\")\n",
        "print(discrepancy_report[['image_path_x', 'image_path_y', 'image_path', 'consolidated_path']])\n"
      ],
      "metadata": {
        "id": "FN_JXiQOD1KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define a function to convert PosixPath to string and extract the last part of the paths\n",
        "def detail_discrepancies(row):\n",
        "    # Convert PosixPath to string and then get the last 30 characters of each path\n",
        "    x_end = str(row['image_path_x'])[-30:]\n",
        "    y_end = str(row['image_path_y'])[-30:]\n",
        "    path_end = str(row['image_path'])[-30:]\n",
        "\n",
        "    if x_end != y_end or x_end != path_end:\n",
        "        return f\"X vs Y: {x_end} != {y_end}, X vs Path: {x_end} != {path_end}\"\n",
        "    return None\n",
        "\n",
        "# Apply this function to a new column\n",
        "full_metadata['discrepancy_details'] = full_metadata.apply(detail_discrepancies, axis=1)\n",
        "\n",
        "# Filter to show only rows with discrepancies and limit the output\n",
        "discrepancy_details = full_metadata[full_metadata['discrepancy_details'].notna()].head(10)\n",
        "\n",
        "print(\"Detailed discrepancies (first 10 cases focusing on path endings):\")\n",
        "print(discrepancy_details[['image_path_x', 'image_path_y', 'image_path', 'discrepancy_details']])\n"
      ],
      "metadata": {
        "id": "tjJ0BeOZFQIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_metadata = pd.get_dummies(full_metadata, columns=['anatom_site_general','benign_malignant', 'sex'])\n"
      ],
      "metadata": {
        "id": "sAeGNqs--IWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anatom_site_columns = [col for col in full_metadata.columns if col.startswith('anatom_site_general_')]\n",
        "benign_malignant_columns = [col for col in full_metadata.columns if col.startswith('benign_malignant_')]\n",
        "\n",
        "print(\"One-hot encoded columns for 'anatom_site_general':\")\n",
        "print(anatom_site_columns)\n",
        "\n",
        "print(\"\\nOne-hot encoded columns for 'benign_malignant':\")\n",
        "print(benign_malignant_columns)\n"
      ],
      "metadata": {
        "id": "I5FYneN9t18B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the one-hot encoded 'benign_malignant' columns\n",
        "full_metadata.rename(columns={\n",
        "    'benign_malignant_benign': 'benign',\n",
        "    'benign_malignant_indeterminate/benign': 'indeterminate_benign',\n",
        "    'benign_malignant_malignant': 'malignant'\n",
        "}, inplace=True)\n",
        "\n",
        "# Sum the 'indeterminate_benign' and 'malignant' columns to create a single 'malignant' column\n",
        "full_metadata['malignant'] = full_metadata['indeterminate_benign'] + full_metadata['malignant']\n",
        "\n",
        "# Convert any values greater than 1 back to 1 (if there's any overlapping)\n",
        "full_metadata['malignant'] = full_metadata['malignant'].clip(upper=1)\n",
        "\n",
        "# Drop the now unnecessary 'indeterminate_benign' column as it has been merged into 'malignant'\n",
        "full_metadata.drop(columns='indeterminate_benign', inplace=True)\n",
        "\n",
        "# Verify the changes\n",
        "print(full_metadata[['benign', 'malignant']].head())\n",
        "full_metadata.info()\n"
      ],
      "metadata": {
        "id": "yDPVgQn1jtTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Replace True and False with 1 and 0 respectively\n",
        "full_metadata.replace({True: 1, False: 0}, inplace=True)\n",
        "\n",
        "# Drop the specified columns\n",
        "columns_to_drop = ['diagnosis', 'nevus_type','patient_id', 'dermoscopic_type', 'diagnosis_confirm_type', 'image_path', 'image_type', 'image_path_x', 'image_path_y']\n",
        "full_metadata.drop(columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Perform Label Encoding for the specified columns\n",
        "label_encoder = LabelEncoder()\n",
        "for column in ['concomitant_biopsy', 'family_hx_mm', 'melanocytic', 'personal_hx_mm']:\n",
        "    full_metadata[column] = label_encoder.fit_transform(full_metadata[column])\n",
        "\n",
        "# Displaying the modified DataFrame\n",
        "print(full_metadata.head())\n"
      ],
      "metadata": {
        "id": "Cx-sysDioyTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_metadata.info()"
      ],
      "metadata": {
        "id": "dQacejd4qQja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to create a new method of image paths, due to data\n"
      ],
      "metadata": {
        "id": "asJkSZG1Gwyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "# Load your DataFrame\n",
        "# full_metadata = pd.read_csv('path_to_your_csv.csv')  # Assuming DataFrame is loaded\n",
        "\n",
        "# Directory containing the images\n",
        "directory_path = '/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input'\n",
        "\n",
        "# List all files in the directory and subdirectories\n",
        "all_files = glob(os.path.join(directory_path, '**', '*'), recursive=True)\n",
        "\n",
        "# Function to find a matching file\n",
        "def find_matching_file(image_name):\n",
        "    # Possible filenames to search for\n",
        "    possible_filenames = [\n",
        "        os.path.join(directory_path, f\"{image_name}.jpg\"),\n",
        "        os.path.join(directory_path, f\"{image_name}_downsampled.jpg\")\n",
        "    ]\n",
        "\n",
        "    # Check for existence of possible files\n",
        "    for file_path in possible_filenames:\n",
        "        if file_path in all_files:\n",
        "            return file_path\n",
        "    return None  # If no file is found\n",
        "\n",
        "# Apply the function to create a new column with the matched file paths\n",
        "full_metadata['image_path'] = full_metadata['image'].apply(find_matching_file)\n",
        "\n",
        "# Check for rows where no file was found\n",
        "missing_files = full_metadata[full_metadata['image_path'].isnull()]\n",
        "\n",
        "# Output results\n",
        "print(f\"Number of missing files: {len(missing_files)}\")\n",
        "if not missing_files.empty:\n",
        "    print(\"Sample rows with missing files:\")\n",
        "    print(missing_files[['image', 'image_path']].head())  # Adjust to display more if needed\n"
      ],
      "metadata": {
        "id": "dzfl760MGu9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_metadata.drop(columns=['consolidated_path'], axis=1, inplace=True)\n",
        "\n",
        "# Check the remaining columns in the DataFrame\n",
        "print(\"Remaining columns\", full_metadata.columns)"
      ],
      "metadata": {
        "id": "-DJMcfmrHZ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Augmentation Using Multiple Studies and ChatGPT4\n",
        "\n",
        "After creating the initial dual pipeline model, and subsequent retraining using augmented data the best outcome we had been able to get was a low 70's% with some heavy bias according to the confusion matrix and ROC scores.  Next, we used a study on studies around this subject to isolate the best models used in this field [Machine Learning and Deep Learning Methods for Skin Lesion Classification and Diagnosis: A Systematic Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8391467/).  \n",
        "\n",
        "After reading this study we took to CHATGPT 4.0 and went into its research professional bot, and asked it to analyze the conclusion of the study and produce the best models, as well as the best model that it could ascertain from the internet itself.  the result was the following three classification and prediction setups:\n",
        "1. [Integrated design of deep features fusion for localization and classification of skin cancer](https://www.sciencedirect.com/science/article/abs/pii/S0167865519303630)\n",
        "2. [Diagnosis of melanoma from dermoscopic images using a deep depthwise separable residual convolutional network](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-ipr.2018.6669)\n",
        "3. [Machine Learning and Deep Learning Methods for Skin Lesion Classification and Diagnosis: A Systematic Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8391467/)\n",
        "\n",
        "The next step was to load the abstracts of each of these studies into the AI CHATGPT4's model called 'Code Pilot' and ask it the following prompt:\n",
        "\n",
        "*'Im going to drop 3 different abstracts from 3 AI and ML studies on skin lesion classifications. can you analyze the 3 studies, and tell me if there is any approach to a new method built on the framework of these ground breaking methods?'*\n",
        "\n",
        "The following code is an amalgam of ChatGPT4's Code Pilot, these ground breaking studies and our responses to the hundreds of errors we encountered allong the way ✨"
      ],
      "metadata": {
        "id": "nZ78UkXSwnmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def create_augmented_data_generator():\n",
        "    data_gen_args = {\n",
        "        'rotation_range': 20,\n",
        "        'width_shift_range': 0.2,\n",
        "        'height_shift_range': 0.2,\n",
        "        'zoom_range': 0.2,\n",
        "        'horizontal_flip': True,\n",
        "        'fill_mode': 'nearest'\n",
        "    }\n",
        "    return ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "augment_data_gen = create_augmented_data_generator()\n"
      ],
      "metadata": {
        "id": "_zz_jFJMAM4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_data(df, batch_size=32, augment=False, binary_classification=False):\n",
        "    image_paths = df['image_path'].values\n",
        "\n",
        "    def load_and_preprocess_data(df, batch_size=32, augment=False, binary_classification=False):\n",
        "      image_paths = df['image_path'].values\n",
        "\n",
        "    if binary_classification:\n",
        "        labels = df[['benign', 'malignant']].values\n",
        "        labels = labels.dot([1, 0]).astype(float)\n",
        "    else:\n",
        "        labels = df[['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']].values.astype(float)\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    image_label_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
        "\n",
        "    def preprocess_and_augment(image_path, label):\n",
        "        image = preprocess_image(image_path)\n",
        "        if augment:\n",
        "            image = tf.numpy_function(func=augment_data_gen.random_transform, inp=[image], Tout=tf.float32)\n",
        "        return image, label\n",
        "\n",
        "    dataset = image_label_ds.map(preprocess_and_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Usage example for binary classification\n",
        "train_dataset_binary = load_and_preprocess_data(full_metadata, augment=True, binary_classification=True)\n",
        "\n",
        "# Usage example for multi-class classification\n",
        "train_dataset_multi_class = load_and_preprocess_data(full_metadata, augment=True, binary_classification=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Glt8X5vhANAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unprocessed_image = ('/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0000000.jpg')\n",
        "io.imshow(unprocessed_image)\n",
        "io.show()"
      ],
      "metadata": {
        "id": "5OjqSwv62LEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import exposure, io\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image not found at the path: {image_path}\")\n",
        "\n",
        "    # Convert from BGR to RGB color space\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply Non-Local Means Denoising\n",
        "    image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
        "\n",
        "    # Convert image to LAB color space to apply CLAHE\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "\n",
        "    # Apply CLAHE to L-channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)\n",
        "    limg = cv2.merge((cl, a, b))\n",
        "\n",
        "    # Convert back to RGB color space\n",
        "    final_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    return final_img\n",
        "\n",
        "# Example usage\n",
        "try:\n",
        "    processed_image = preprocess_image('/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0000000.jpg')\n",
        "    # Display the image using skimage's imshow function\n",
        "    io.imshow(processed_image)\n",
        "    io.show()\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PWwwe0fCwnwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pywt\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage import io\n",
        "\n",
        "def segment_image(image):\n",
        "    # Ensure the input image is in RGB format\n",
        "    if len(image.shape) != 3 or image.shape[2] != 3:\n",
        "        raise ValueError(\"Input image must be RGB.\")\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Apply 2D Wavelet Transform using Biorthogonal wavelets\n",
        "    coeffs2 = pywt.dwt2(gray_image, 'bior1.3')\n",
        "    LL, (LH, HL, HH) = coeffs2\n",
        "\n",
        "    # Apply Otsu's thresholding to the approximation coefficients\n",
        "    threshold_value = threshold_otsu(LL)\n",
        "    segmented_image = LL > threshold_value\n",
        "\n",
        "    return segmented_image\n",
        "\n",
        "# Assume 'processed_image' is already defined as shown in your previous example.\n",
        "try:\n",
        "    segmented_image = segment_image(processed_image)\n",
        "    # Display the segmented image using skimage's imshow function\n",
        "    io.imshow(segmented_image, cmap='gray')\n",
        "    io.show()\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the segmentation process: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uREKh4bGwoHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image as keras_image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(image_path):\n",
        "    # Load the pre-trained VGG16 model without the fully connected layers\n",
        "    model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = keras_image.load_img(image_path, target_size=(224, 224))\n",
        "    img_data = keras_image.img_to_array(img)\n",
        "    img_data = np.expand_dims(img_data, axis=0)\n",
        "    img_data = preprocess_input(img_data)\n",
        "\n",
        "    # Extract features\n",
        "    vgg16_features = model.predict(img_data)\n",
        "    return vgg16_features\n",
        "\n",
        "# Example usage\n",
        "try:\n",
        "    features = extract_features('/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input/ISIC_0000000.jpg')\n",
        "    print(\"Extracted feature shape:\", features.shape)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EoN4lfPdwoL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load your metadata; ensure 'image' column exists with image IDs\n",
        "metadata = full_metadata.copy()\n",
        "\n",
        "# Define the base path where images are stored\n",
        "base_path = Path('/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input')\n",
        "\n",
        "# Function to find image paths considering possible suffixes and extensions\n",
        "def find_image_path(base_path, image_id):\n",
        "    suffixes = ['', '_downsampled']  # Add other suffixes as needed\n",
        "    extensions = ['.jpg', '.jpeg', '.png']\n",
        "    for suffix in suffixes:\n",
        "        for ext in extensions:\n",
        "            potential_path = base_path / f\"{image_id}{suffix}{ext}\"\n",
        "            if potential_path.exists():\n",
        "                return str(potential_path)\n",
        "    return None  # If no file is found\n",
        "\n",
        "# Applying the function to each image ID in the DataFrame with progress visualization\n",
        "metadata['image_path'] = [find_image_path(base_path, img_id) for img_id in tqdm(metadata['image'], desc='Resolving Image Paths')]\n",
        "\n",
        "# Optionally, check for and report any missing image paths\n",
        "missing_images = metadata['image_path'].isnull().sum()\n",
        "if missing_images > 0:\n",
        "    print(f\"Warning: {missing_images} images not found.\")"
      ],
      "metadata": {
        "id": "JPffr4mTXMAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found at the path: {image_path}\")\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Failed to load image at: {image_path}\")\n",
        "\n",
        "    image = cv2.resize(image, (224, 224))  # Resize to fit the model input\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    return image\n",
        "\n",
        "def preprocess_all_data(metadata):\n",
        "    base_path = Path('/content/ISIC_2019_Training_Input/ISIC_2019_Training_Input')\n",
        "    image_data = []\n",
        "    errors = 0\n",
        "\n",
        "    # Loop through each file in the metadata DataFrame\n",
        "    for filename in tqdm(metadata['image'], desc=\"Loading and processing images\"):\n",
        "        full_path = base_path / f\"{filename}.jpg\"\n",
        "        try:\n",
        "            img = load_and_preprocess_image(str(full_path))\n",
        "            image_data.append(img)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            errors += 1\n",
        "            continue\n",
        "\n",
        "    if errors == len(metadata):\n",
        "        raise Exception(\"All images failed to load. Please check file paths and filenames.\")\n",
        "\n",
        "    # Labels are already one-hot encoded in the DataFrame\n",
        "    labels = metadata[['benign', 'malignant']].values  # Extract OHE labels directly\n",
        "    return np.array(image_data), metadata, np.array(labels)\n",
        "\n",
        "# Use try-except to handle any errors during data processing\n",
        "try:\n",
        "    image_data, metadata, labels = preprocess_all_data(metadata)\n",
        "    print(\"Data processed successfully.\")\n",
        "    print(f\"Total images processed: {len(image_data)}\")\n",
        "\n",
        "    # Split the data into training, validation, and test sets\n",
        "    image_train, image_temp, labels_train, labels_temp = train_test_split(\n",
        "        image_data, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "    image_val, image_test, labels_val, labels_test = train_test_split(\n",
        "        image_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "4tzHJXVrqFgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "def create_dual_output_model(metadata_shape, num_binary_classes, num_multiclass_classes):\n",
        "    # Image input\n",
        "    image_input = Input(shape=(224, 224, 3), name='image_input')\n",
        "    base_model = VGG16(weights='imagenet', include_top=False)\n",
        "    base_model.trainable = False  # Freeze the convolutional base\n",
        "    x_image = base_model(image_input)\n",
        "    x_image = GlobalAveragePooling2D()(x_image)\n",
        "\n",
        "    # Metadata input\n",
        "    metadata_input = Input(shape=(metadata_shape,), name='metadata_input')\n",
        "    x_metadata = Dense(64, activation='relu')(metadata_input)\n",
        "\n",
        "    # Concatenate both inputs\n",
        "    combined_features = Concatenate()([x_image, x_metadata])\n",
        "    x = Dense(256, activation='relu')(combined_features)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    # Binary classification output for benign/malignant\n",
        "    output_binary = Dense(num_binary_classes, activation='sigmoid', name='output_binary')(x)\n",
        "\n",
        "    # Multiclass classification output for conditions\n",
        "    output_multiclass = Dense(num_multiclass_classes, activation='softmax', name='output_multiclass')(x)\n",
        "\n",
        "    model = Model(inputs=[image_input, metadata_input], outputs=[output_binary, output_multiclass])\n",
        "    return model\n",
        "\n",
        "gpt_model = create_dual_output_model(metadata_shape=10, num_binary_classes=1, num_multiclass_classes=8)\n",
        "print(gpt_model.summary())\n"
      ],
      "metadata": {
        "id": "UxLp4m_dM8Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in gpt_model.layers:\n",
        "    print(layer.name)"
      ],
      "metadata": {
        "id": "dlDPQ-yZnBzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def weighted_binary_crossentropy(class_weights):\n",
        "    def loss(y_true, y_pred):\n",
        "        weights = y_true * class_weights[1] + (1 - y_true) * class_weights[0]\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        weighted_bce = tf.reduce_mean(weights * bce)\n",
        "        return weighted_bce\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "fzKzBoNSyUIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_categorical_crossentropy(class_weights):\n",
        "    def loss(y_true, y_pred):\n",
        "        # Extracting indices where y_true is 1 for one-hot encoded labels\n",
        "        idx = tf.argmax(y_true, axis=-1)\n",
        "        weights = tf.gather(tf.constant(list(class_weights.values())), idx)\n",
        "        cce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "        weighted_cce = tf.reduce_mean(weights * cce)\n",
        "        return weighted_cce\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "QNwl0HgTyUMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class weights for both outputs\n",
        "class_weights_binary = {0: 0.5979369275800208, 1: 3.052663292359605}\n",
        "class_weights_multiclass = {0: 0.7002156125608138, 1: 0.24593203883495146, 2: 0.9528663857959675,\n",
        "                            3: 3.652104959630911, 4: 1.2066977896341464, 5: 13.248430962343097,\n",
        "                            6: 12.515316205533598, 7: 5.041998407643312}\n",
        "\n",
        "gpt_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'output_binary': weighted_binary_crossentropy(class_weights_binary),\n",
        "        'output_multiclass': weighted_categorical_crossentropy(class_weights_multiclass)\n",
        "    },\n",
        "    metrics={'output_binary': 'accuracy', 'output_multiclass': 'accuracy'}\n",
        ")\n"
      ],
      "metadata": {
        "id": "gohNITJuyUQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Data Splitting (assumed your image_data and labels are prepared)\n",
        "image_train, image_temp, labels_train, labels_temp = train_test_split(\n",
        "    image_data, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "image_val, image_test, labels_val, labels_test = train_test_split(\n",
        "    image_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp)\n",
        "\n",
        "# Convert to TensorFlow datasets and batch them\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((image_train, labels_train)).shuffle(len(image_train)).batch(32)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((image_val, labels_val)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((image_test, labels_test)).batch(32)\n",
        "\n",
        "\n",
        "# Proper class_weight dictionary for multi-output model\n",
        "class_weights = {\n",
        "    'output_binary': class_weights_binary,\n",
        "    'output_multiclass': class_weights_multiclass\n",
        "}\n",
        "\n",
        "\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='/content/best_model.h5',  # Path where the model will be saved\n",
        "    save_weights_only=False,  # Save the full model, not just weights\n",
        "    monitor='val_accuracy',  # Monitor validation accuracy for improvements\n",
        "    mode='max',  # The monitoring metric should be maximized\n",
        "    save_best_only=True,  # Only save a model if `val_accuracy` has improved\n",
        "    verbose=1)  # Print out detailed logging information\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss for early stopping\n",
        "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,  # Print out detailed logging information\n",
        "    mode='min',  # The monitoring metric should be minimized\n",
        "    restore_best_weights=True)  # Restore model weights from the epoch with the best value of the monitored quantity\n",
        "\n",
        "# Training the model with the specified class weights\n",
        "gpt_model.fit(\n",
        "    train_dataset,  # Training data\n",
        "    epochs=50,  # Number of epochs to train\n",
        "    callbacks=[model_checkpoint_callback, early_stopping_callback],  # Callbacks to manage training\n",
        "    validation_data=val_dataset,  # Data on which to evaluate the loss and any model metrics at the end of each epoch\n",
        "    class_weight=class_weights,  # Class weights for handling imbalanced data\n",
        "    verbose=1)  # Print detailed log output"
      ],
      "metadata": {
        "id": "AdEc9170OfHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "id": "QQdcecWJy2RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Augemntation Phase 4 (NOT IMPLEMENTED YET)\n",
        "supplemental data\n",
        "\n",
        "we will supplement the weak portions of the dataset using data from other ISIC projects, later we will search for duplicates, then remerge and retrain the entire model based on this new dataset we create.  we will also create some new steps inthe pre-processing aspects of training the model.  Augmented Data is coming from:\n",
        "\n",
        "###Challenge 2018: Task 1-2: Training\n",
        "###Challenge 2020: Training\n",
        "###HAM10000"
      ],
      "metadata": {
        "id": "TfwMuADUcaCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Data into Training, Validation, and Testing Sets\n",
        "\n",
        "The dataset is initially split into training and temporary sets, with the temporary set reserved for further division into validation and test sets. This method ensures that the model can be trained extensively, validated accurately, and finally tested to evaluate its performance on unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uXEBi9lGLVro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Let's separate features and labels first\n",
        "X = full_metadata.drop(['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK'], axis=1)\n",
        "y = full_metadata[['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']]  # Excluded 'UNK'\n",
        "\n",
        "# Now, we split the data into training and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the X_temp and y_temp further into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Calculate the target number of samples\n",
        "target_samples = int((3607 + 10308) / 2)  # adjust this as needed\n",
        "\n",
        "# Create new DataFrame for the balanced dataset\n",
        "X_train_resampled = pd.DataFrame(columns=X_train.columns)\n",
        "y_train_resampled = pd.DataFrame(columns=y_train.columns)\n",
        "\n",
        "# Iterate through each class and apply resampling\n",
        "for column in y_train.columns:\n",
        "    # Filter samples for the current class\n",
        "    filter_mask = y_train[column] == 1\n",
        "    X_class = X_train[filter_mask]\n",
        "    y_class = y_train[filter_mask]\n",
        "\n",
        "    num_samples = len(X_class)\n",
        "    if num_samples > 0:\n",
        "        if num_samples < target_samples:\n",
        "            # Oversample minority classes\n",
        "            X_class_resampled, y_class_resampled = resample(X_class, y_class,\n",
        "                                                            replace=True,  # Sample with replacement\n",
        "                                                            n_samples=target_samples,  # Match the target samples\n",
        "                                                            random_state=42)\n",
        "        else:\n",
        "            # For majority or adequately represented classes, we might undersample or keep as is\n",
        "            X_class_resampled, y_class_resampled = resample(X_class, y_class,\n",
        "                                                            replace=False,\n",
        "                                                            n_samples=target_samples,\n",
        "                                                            random_state=42)\n",
        "\n",
        "        # Append resampled data back to the overall dataset\n",
        "        X_train_resampled = pd.concat([X_train_resampled, X_class_resampled], axis=0)\n",
        "        y_train_resampled = pd.concat([y_train_resampled, y_class_resampled], axis=0)\n",
        "    else:\n",
        "        print(f\"No instances to resample for class '{column}'\")\n",
        "\n",
        "# Shuffle the dataset to mix up class order (important for training)\n",
        "X_train_resampled = X_train_resampled.sample(frac=1, random_state=42)\n",
        "y_train_resampled = y_train_resampled.loc[X_train_resampled.index]\n",
        "\n",
        "print(\"New class counts after resampling:\\n\", y_train_resampled.sum())\n"
      ],
      "metadata": {
        "id": "3xBB1t4rEGjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "NcKsOCJcaFiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dual Input Data Generator\n",
        "\n",
        "The `DualInputGenerator` class is a custom data generator for handling datasets that include both image paths and associated metadata. It is designed to work with Keras/TensorFlow, enabling efficient batch processing which is crucial for training deep learning models on large datasets. Here's a breakdown of its functionality:\n",
        "\n",
        "- **Initialization (`__init__`)**: This method sets up the generator with all necessary parameters, including image paths, metadata, and labels. It also initializes the batch size, image size, and whether the dataset should be shuffled during training to introduce randomness into the training process.\n",
        "\n",
        "- **Preprocessing (`preprocess_image`)**: A helper function to read and preprocess images. It converts images to the appropriate size and scale ([224x224] in this case) and normalizes pixel values to the range [0, 1].\n",
        "\n",
        "- **Length Calculation (`__len__`)**: This method calculates how many batches are in the dataset, which is used by Keras during training to determine the number of steps per epoch.\n",
        "\n",
        "- **Batch Generation (`__getitem__`)**: This method retrieves a batch of data by processing the images, metadata, and labels. It loads and preprocesses the images specified by the batch indexes, extracts the corresponding metadata, and gathers the labels. The function returns a list containing two arrays (images and metadata) and the batch of labels.\n",
        "\n",
        "- **Epoch End Handling (`on_epoch_end`)**: If shuffling is enabled, this method shuffles the indexes after each epoch to ensure that the model does not see the same sequence of batches every epoch, helping the model to generalize better.\n",
        "\n",
        "This structured approach ensures that the model receives properly formatted and preprocessed data for each training step, facilitating effective learning and performance improvement.\n"
      ],
      "metadata": {
        "id": "lmdnx1WuLlkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dual Input Generator for creating a mo"
      ],
      "metadata": {
        "id": "86lCVqd8qSaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DualInputGenerator(Sequence):\n",
        "    def __init__(self, image_paths, metadata, labels, batch_size, img_size=(224, 224), shuffle=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.metadata = metadata\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def preprocess_image(file_path):\n",
        "      img = tf.io.read_file(file_path)\n",
        "      img = tf.image.decode_jpeg(img, channels=3)\n",
        "      img = tf.image.resize(img, [224, 224])  # Ensuring image size is consistent\n",
        "      img = img / 255.0  # Normalize to [0, 1]\n",
        "      return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # Calculate start and end indices for the current batch\n",
        "      start_idx = index * self.batch_size\n",
        "      end_idx = (index + 1) * self.batch_size\n",
        "\n",
        "      # Handling last batch which might be smaller than batch_size\n",
        "      end_idx = min(end_idx, len(self.image_paths))\n",
        "\n",
        "      # Batch size might vary for the last batch\n",
        "      current_batch_size = end_idx - start_idx\n",
        "\n",
        "      # Initialize numpy arrays to hold batch data\n",
        "      X_images = np.empty((current_batch_size, *self.img_size, 3))\n",
        "      X_metadata = np.empty((current_batch_size, self.metadata.shape[1]))\n",
        "      y = np.empty((current_batch_size, self.labels.shape[1]), dtype=int)\n",
        "\n",
        "      # Generate data for the current batch\n",
        "      for i, idx in enumerate(range(start_idx, end_idx)):\n",
        "        # Get the image path for current index\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = preprocess_image(img_path)\n",
        "        # Add processed image to the batch\n",
        "        X_images[i, ] = img\n",
        "        # Add corresponding metadata\n",
        "        X_metadata[i, ] = self.metadata[idx]\n",
        "        # Add corresponding label\n",
        "        y[i, ] = self.labels[idx]\n",
        "\n",
        "      return [X_images, X_metadata], y\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n"
      ],
      "metadata": {
        "id": "pP09P7wGBGD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Data Generators for Training, Validation, and Testing\n",
        "\n",
        "To ensure our model is trained, validated, and tested with data that is efficiently loaded and preprocessed, we initialize three instances of the `DualInputGenerator` class:\n",
        "\n",
        "- **Training Generator (`train_gen`)**: This generator is configured with training data paths, metadata, and labels. It is responsible for feeding the training data into the model in batches of 32, ensuring that each batch is shuffled to promote model generalization.\n",
        "\n",
        "- **Validation Generator (`val_gen`)**: Similar to the training generator, but using the validation dataset. This generator provides data for evaluating the model during the training process, allowing us to monitor the model's performance and make adjustments if needed without seeing the test data.\n",
        "\n",
        "- **Testing Generator (`test_gen`)**: Finally, the testing generator is set up using the test dataset to evaluate the model's performance after training has been completed. This step is crucial for assessing how well the model is likely to perform on unseen real-world data.\n",
        "\n",
        "Each generator uses the `image_paths`, `metadata`, and `labels` from their respective subsets of the data, ensuring that the model receives all necessary inputs for making predictions during each phase of the training and evaluation process.\n",
        "\n"
      ],
      "metadata": {
        "id": "PmC1R4dDLw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the training generator\n",
        "train_gen = DualInputGenerator(\n",
        "    image_paths=X_train['image_path'].values,\n",
        "    metadata=X_train.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_train.values,  # Include labels for training set\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Initialize the validation generator\n",
        "val_gen = DualInputGenerator(\n",
        "    image_paths=X_val['image_path'].values,\n",
        "    metadata=X_val.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_val.values,  # Include labels for validation set\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Initialize the test generator\n",
        "test_gen = DualInputGenerator(\n",
        "    image_paths=X_test['image_path'].values,\n",
        "    metadata=X_test.drop(columns=['image', 'image_path', 'lesion_id']).values,\n",
        "    labels=y_test.values,  # Include labels for test set\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "X_test['image_path'].values"
      ],
      "metadata": {
        "id": "8gD1J3reWT-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dual Input Neural Network Architecture\n",
        "\n",
        "The neural network architecture is designed to handle two types of input: images and metadata. Here's a breakdown of the model architecture and the role of each component:\n",
        "\n",
        "#### Image Input Branch\n",
        "- **Image Input**: The model starts with an image input layer that accepts images of shape (224, 224, 3).\n",
        "- **Convolution and Pooling Layers**: Following the input, the model includes two sets of convolutional layers (`Conv2D`) and max pooling layers (`MaxPooling2D`). Each convolutional layer is followed by batch normalization (`BatchNormalization`), which helps to accelerate the training process and stabilize the learning environment by normalizing the activations.\n",
        "- **Flattening and Dense Layer**: After extracting and pooling features through convolutions, the data is flattened (`Flatten`) and passed through a dense layer with ReLU activation, which is again batch normalized.\n",
        "\n",
        "#### Metadata Input Branch\n",
        "- **Metadata Input**: This branch begins with an input for metadata features, shaped dynamically based on the number of metadata features (`num_metadata_features`).\n",
        "- **Dense Layers and Normalization**: It includes dense layers (`Dense`) with ReLU activation, interspersed with batch normalization to ensure the model learns effectively from the structured data.\n",
        "\n",
        "#### Combining Branches\n",
        "- **Concatenation**: The outputs of the image and metadata branches are combined into a single vector (`concatenate`), allowing the model to learn from both image features and metadata simultaneously.\n",
        "- **Final Dense Layers**: The combined data is then passed through additional dense layers, including a dropout layer (`Dropout`) to prevent overfitting, culminating in a softmax output layer (`Dense`) that classifies the images into one of nine diagnostic categories.\n",
        "\n",
        "#### Model Compilation\n",
        "- The model is compiled with the Adam optimizer, using a learning rate of 1e-4. The loss function used is categorical crossentropy, suitable for multi-class classification tasks, and accuracy is used as the metric to evaluate model performance.\n",
        "\n",
        "This dual-input setup allows the model to leverage both detailed image data and accompanying metadata, aiming to improve diagnostic accuracy compared to using images or metadata alone.\n"
      ],
      "metadata": {
        "id": "Hg7trvZfbOp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_metadata_features = X_train.drop(columns=['image', 'image_path', 'lesion_id']).shape[1]\n",
        "print(\"Number of metadata features:\", num_metadata_features)\n"
      ],
      "metadata": {
        "id": "JUTOAkX0lHTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
        "\n",
        "\n",
        "num_metadata_features = 13\n",
        "\n",
        "\n",
        "# Image input branch\n",
        "image_input = Input(shape=(224, 224, 3), name='image_input')\n",
        "x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "# Add batch normalization here\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "# Add batch normalization here\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "image_branch = Dense(128, activation='relu')(x)\n",
        "image_branch = BatchNormalization()(image_branch)\n",
        "\n",
        "# Metadata input branch\n",
        "metadata_input = Input(shape=(num_metadata_features,), name='metadata_input')\n",
        "y = Dense(32, activation='relu')(metadata_input)\n",
        "# Normalize after the first dense layer\n",
        "metadata_branch = BatchNormalization()(y)\n",
        "metadata_branch = Dense(64, activation='relu')(metadata_branch)\n",
        "# Normalize before combining\n",
        "metadata_branch = BatchNormalization()(metadata_branch)\n",
        "\n",
        "# Combine the outputs of the two branches\n",
        "combined = concatenate([image_branch, metadata_branch])\n",
        "z = Dense(256, activation='relu')(combined)\n",
        " # Adding dropout for regularization\n",
        "z = Dropout(0.5)(z)\n",
        "# Adjust the number of units to match the number of classes\n",
        "output = Dense(8, activation='softmax')(z)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[image_input, metadata_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EOCCRKIH6pE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Checkpointing Configuration\n",
        "\n",
        "The `ModelCheckpoint` callback in TensorFlow Keras is used to save the model at specific intervals or after achieving certain benchmarks during training. This setup is critical for preserving model states and allows for resuming training without losing previous progress. Here's a detailed explanation of the configuration:\n",
        "\n",
        "- **Checkpoint Path**: Specifies the directory and filename structure for saving the model files. The path includes placeholders for the epoch number (`{epoch:02d}`) and validation accuracy (`{val_accuracy:.2f}`), allowing each file to uniquely represent the state of the model at the end of each epoch.\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "nZwoAjGnmiVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/your_model_directory/model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=3, save_best_only=True, mode='max')"
      ],
      "metadata": {
        "id": "WMPKC0Z4mid1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "id": "SC3rn85NqvsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Device Placement and Simple Matrix Multiplication\n",
        "\n",
        "To gain insight into how TensorFlow allocates computing resources (such as CPU or GPU) for operations, the `set_log_device_placement` flag is enabled. This setting provides detailed logs showing which devices each operation is assigned to, assisting in debugging and optimizing performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "dGvmgIMCM0LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Test with a simple computation\n",
        "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "# Run on GPU\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)\n"
      ],
      "metadata": {
        "id": "LbV-J3q1qz8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring Callbacks for Model Training\n",
        "\n",
        "To enhance the training process and ensure optimal model performance, we use a combination of callbacks in TensorFlow. These callbacks are set to monitor the training process and make decisions at the end of each epoch. Here's a breakdown of each callback used:\n",
        "\n",
        "#### EarlyStopping\n",
        "- **Purpose**: Prevents overfitting by halting the training if the model's validation loss does not improve for a specified number of consecutive epochs.\n",
        "- **Configuration**:\n",
        "  - `monitor='val_loss'`: Watches the validation loss metric for changes.\n",
        "  - `patience=2`: Allows the training to continue for 2 epochs without improvements in validation loss.\n",
        "  - `verbose=1`: Enables logging for when the training is stopped early.\n",
        "\n",
        "#### ModelCheckpoint\n",
        "- **Purpose**: Saves the model in its current state after each epoch, but only if the model's performance (based on the monitored metric) has improved.\n",
        "- **Configuration**:\n",
        "  - `filepath='best_model.h5'`: Specifies the location and filename where the best model version will be saved.\n",
        "  - `monitor='val_loss'`: Monitors the validation loss for improvements.\n",
        "  - `save_best_only=True`: Ensures that the model is saved only when its validation loss is at its lowest point seen so far.\n",
        "  - `verbose=1`: Provides detailed logs when the model is saved.\n",
        "\n",
        "These callbacks are essential tools for managing long training sessions effectively. They help conserve resources by stopping training when additional epochs would not lead to improvements (`EarlyStopping`) and by ensuring that only the best model version is saved (`ModelCheckpoint`), thus simplifying deployment and further evaluation.\n"
      ],
      "metadata": {
        "id": "jtj_1lin7bTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=2, verbose=1),\n",
        "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "aMr-5vG72Z98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "#### AND Re-Training, the first model resulted in 70% this is after going back to do futher augmentation\n",
        "\n",
        "To train the dual-input model, we use the `model.fit` method provided by TensorFlow. This method executes the training cycle across a specified number of epochs, allowing for detailed monitoring and adjustments during the training process. Here's a breakdown of each parameter used in the `model.fit` call:\n",
        "\n",
        "- **train_gen**: The training data generator, which supplies batches of image and metadata inputs along with the corresponding labels.\n",
        "- **validation_data**: The validation data generator used to evaluate the model at the end of each epoch, helping monitor its performance on unseen data.\n",
        "- **epochs**: Sets the total number of training cycles the model will undergo. In this case, the model is set to train for 10 epochs.\n",
        "- **steps_per_epoch**: Specifies the number of batch steps to complete one epoch. This is set to the total number of batches available in the training generator (`len(train_gen)`), ensuring that each sample is used once per epoch.\n",
        "- **validation_steps**: Determines the number of batch steps used for validating the model, set to the length of the validation generator.\n",
        "- **callbacks**: Includes the `EarlyStopping` and `ModelCheckpoint` callbacks configured previously. These enhance the training process by:\n",
        "  - **EarlyStopping**: Automatically stops training when the validation loss ceases to decrease, preventing overfitting.\n",
        "  - **ModelCheckpoint**: Saves the best version of the model based on validation loss, ensuring that only the most accurate model is retained.\n",
        "\n",
        "This configuration ensures that the model is not only trained but also validated effectively, with checkpoints saved automatically and training potentially halted early if no further gains are observed. This approach optimizes both the efficiency and efficacy of the model training process.\n",
        "\n"
      ],
      "metadata": {
        "id": "9azv94GqNNcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    # Include callbacks for early stopping and model checkpointing\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "yGXmw8zk2imE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_gen, steps=len(test_gen))\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "DXSsK7n7P0Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "zw7bRRuLvjHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Summary and Visualization\n",
        "\n",
        "The `model.summary()` method outputs a detailed architecture of the model, listing each layer, its type, output shape, and number of parameters. This helps in understanding the depth and complexity of the model's structure. Additionally, the `plot_model` function is used to generate a graphical representation of the model, which includes the shapes and names of each layer, offering a clear visual reference of the model's design.\n",
        "\n",
        "### Generating and Evaluating Predictions\n",
        "\n",
        "After training, predictions are generated for the test set using the `model.predict` method. This function computes the model's output predictions. The `np.argmax` function is then applied to these predictions to convert the model outputs from probabilities to explicit class predictions. This step is crucial for evaluating the model's classification performance on the test data.\n",
        "\n",
        "### Collecting True Classes and Generating Confusion Matrix\n",
        "\n",
        "To evaluate the accuracy of our model's predictions, it is necessary to compare these predicted classes against the true classes from the test set. This comparison is facilitated through the collection of true class labels directly from the test generator. Subsequently, a confusion matrix is generated using the `confusion_matrix` function. This matrix is a powerful tool for visualizing the performance of a classification model, showing the actual versus predicted classifications, which helps in identifying how well the model is performing with respect to different classes. The `seaborn.heatmap` function is then used to plot the confusion matrix, providing a color-coded visualization of the results, which makes it easier to interpret the model's accuracy and misclassifications.\n",
        "\n"
      ],
      "metadata": {
        "id": "z2AWRmDMpZ5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "# Generate a plot of the model\n",
        "plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "-j1vR9VcpZGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Generate predictions using the test generator\n",
        "predictions = model.predict(test_gen, steps=len(test_gen))\n",
        "\n",
        "# Obtain the predicted classes by taking the argmax of the predictions array\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Since your test_gen should also be outputting the true classes in the same order as the predictions:\n",
        "# We need to collect all true classes from the generator (in the same order)\n",
        "true_classes = []\n",
        "for _, labels in test_gen:\n",
        "    true_classes.extend(np.argmax(labels, axis=1))\n",
        "    if len(true_classes) >= len(predictions):\n",
        "        break  # Stop once we have all the labels we need\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_classes[:len(predictions)], predicted_classes)\n",
        "\n",
        "print(cm)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "o0zNgkl56J9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4zTsYozZ7XKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summary of Confusin Matrix\n",
        "Classes 3, 4, 5, 6, and 7 show a particular need for augmentation due to low true positives and high confusion with other classes. Class 0 also needs attention due to being frequently confused with other high-incidence classes. These focused augmentations aim to enhance the model's ability to distinguish these classes more clearly, which should improve overall accuracy and reduce misclassifications."
      ],
      "metadata": {
        "id": "NTCa0XIjR5TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "66.46% is ok, but lets do some fine tuning... and save it!\n",
        "\n",
        "### Model Evaluation and Misclassification Analysis\n",
        "\n",
        "The process starts by loading a previously trained model from disk using `load_model`. This model is then used to evaluate its performance on unseen data provided by a test generator. The evaluation consists of several steps:\n",
        "\n",
        "#### Loading the Model\n",
        "The model is loaded from a saved file, ensuring that the most effective version of the model, as determined by prior validations, is used for evaluations.\n",
        "\n",
        "#### Fetching Predictions and Labels\n",
        "A custom function `get_predictions_and_labels` is employed to iterate over the test generator. This function fetches batches of data (both images and metadata) and corresponding labels, then uses the model to predict each batch. Predictions and true labels are accumulated from all batches, allowing for comprehensive evaluation.\n",
        "\n",
        "#### Calculating Confusion Matrix and Classification Report\n",
        "Once predictions are gathered, they are converted from softmax probabilities to class predictions using `np.argmax`. With predictions and true labels in hand, a confusion matrix is generated to visually assess the model's performance across different classes. This matrix highlights which classes are being confused with others, providing insight into potential biases or weaknesses in the model.\n",
        "\n",
        "A classification report is also generated to provide key metrics for each class, such as precision, recall, and F1-score. This report helps in understanding the model's accuracy and identifying classes that might require more focus during further model training.\n",
        "\n",
        "#### Visualizing Misclassified Examples\n",
        "Finally, the script identifies and displays a set of misclassified images along with their true and predicted labels. This visualization is crucial for diagnosing what might be causing the errors, potentially guiding further data collection, augmentation strategies, or model adjustments.\n",
        "\n",
        "Each of these steps plays a vital role in evaluating the model's real-world applicability and robustness, ensuring that the model not only performs well statistically but also meets practical expectations.\n"
      ],
      "metadata": {
        "id": "e4WEe_Pe4GPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the model\n",
        "model = load_model('/content/best_model.h5')\n",
        "\n",
        "# Function to fetch data and labels from the generator\n",
        "def get_predictions_and_labels(generator):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for _ in range(len(generator)):\n",
        "        # Getting the next batch of data\n",
        "        (images, metadata), labels = generator.__getitem__(_)\n",
        "        # Predict this batch\n",
        "        batch_predictions = model.predict([images, metadata])\n",
        "        # Store predictions and labels\n",
        "        predictions.append(batch_predictions)\n",
        "        true_labels.append(labels)\n",
        "\n",
        "    # Concatenate all batches\n",
        "    predictions = np.vstack(predictions)\n",
        "    true_labels = np.vstack(true_labels)\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Using the test generator to get predictions and labels\n",
        "predictions, true_labels = get_predictions_and_labels(test_gen)\n",
        "\n",
        "# Convert softmax probabilities to class predictions\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(true_labels, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "\n",
        "# Generate a classification report\n",
        "print('\\nClassification Report:\\n', classification_report(true_classes, predicted_classes))\n",
        "\n",
        "# Identify misclassified examples\n",
        "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
        "print(f'Total misclassified samples: {len(misclassified_indices)}')\n",
        "\n",
        "# Assuming you want to visualize some misclassified images:\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < len(misclassified_indices):\n",
        "        idx = misclassified_indices[i]\n",
        "        img = test_gen.image_paths[idx]  # Assuming image paths are stored here\n",
        "        true_label, pred_label = true_classes[idx], predicted_classes[idx]\n",
        "        ax.imshow(plt.imread(img))  # Reading the image from path\n",
        "        ax.set_title(f'True: {true_label}, Pred: {pred_label}')\n",
        "        ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N7xEntx2yeYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets just run it a few more times and see if it can get any better before it gets any worse..."
      ],
      "metadata": {
        "id": "4kQ7HYQITe-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the previously saved model\n",
        "model = load_model('/content/best_model.h5')\n",
        "\n",
        "# Define your callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=2, verbose=1),\n",
        "    ModelCheckpoint(filepath='/content/best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Path where the checkpoint will be saved\n",
        "model_dir = '/content/drive/My Drive/Colab Models'\n",
        "# Generate a unique identifier based on the current date and time\n",
        "unique_identifier = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "checkpoint_filepath = os.path.join(model_dir, f'best_model_{unique_identifier}.h5')\n",
        "\n",
        "\n",
        "# Continue training with generators and include callbacks\n",
        "history2 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    # Adjust the total epochs as needed\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "BstjOq4WvTz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Classifier Performance Using ROC Curves\n",
        "\n",
        "To evaluate the performance of the classifier on a per-class basis, the Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC) are computed. These metrics are crucial for assessing the model's ability to distinguish between classes. Here's a breakdown of the steps involved in this evaluation:\n",
        "\n",
        "#### Preparing Labels and Predictions\n",
        "Firstly, the true labels are converted to a 1-dimensional array if they are not already in this format, ensuring they can be directly compared with the predicted probabilities. Predictions are checked to ensure they contain a probability for each class.\n",
        "\n",
        "#### Computing ROC Curves and AUC Values\n",
        "For each class, the ROC curve is calculated by:\n",
        "- Isolating the prediction probabilities for that class.\n",
        "- Creating a binary outcome array for the class, where the class of interest is labeled `1` and all others `0`.\n",
        "- Using the `roc_curve` function from `sklearn.metrics` to compute the true positive rate (TPR) and false positive rate (FPR) at various threshold settings.\n",
        "- Calculating the AUC value to quantify the overall ability of the model to discriminate between positive and negative classes for each specific class.\n",
        "\n",
        "#### Plotting the ROC Curves\n",
        "Each class's ROC curve is plotted with a unique color, and the AUC score is displayed in the legend to provide a visual and numerical representation of classifier performance. The diagonal line represents a random classifier's performance for comparison.\n",
        "\n",
        "- **X-axis**: False Positive Rate (FPR) — represents the proportion of negative data points that are mistakenly considered positive.\n",
        "- **Y-axis**: True Positive Rate (TPR) — represents the proportion of actual positives correctly identified.\n",
        "\n",
        "The area under each curve (AUC) provides a single measure of overall performance regardless of the classification threshold. The closer the AUC is to 1, the better the model is at predicting positive classes as positive and negative classes as negative. Values closer to 0.5 suggest no discriminative ability, equivalent to random guessing.\n",
        "\n",
        "This detailed analysis helps in identifying which classes the model performs well on and which ones might require further tuning, potentially guiding further data augmentation, additional training, or algorithm adjustments.\n"
      ],
      "metadata": {
        "id": "_WhYhVOuPPKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure that true_labels is a 1D array of class indices\n",
        "if len(true_labels.shape) > 1:\n",
        "    true_labels = np.argmax(true_labels, axis=1)\n",
        "\n",
        "# Ensure predictions are probabilities\n",
        "if predictions.shape[1] == len(np.unique(true_labels)):\n",
        "    print(\"Predictions appear to be properly formatted.\")\n",
        "\n",
        "n_classes = len(np.unique(true_labels))\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "for i in range(n_classes):\n",
        "    # Isolate the probabilities for the current class\n",
        "    class_probs = predictions[:, i]\n",
        "    # Create a binary outcome for this class\n",
        "    class_true = (true_labels == i).astype(int)\n",
        "    fpr[i], tpr[i], _ = roc_curve(class_true, class_probs)\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, n_classes))\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve by class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "MXn21DHQ1Oca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_gen, steps=len(test_gen))\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "jYDZeSsKEWjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Interface** ⚡\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fCM0RYQ6fov0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Interface (UI)\n",
        "A simple web-based UI allows users to upload lesion images, input relevant metadata, and receive a prediction. We will use Gradio as learned in class for this model."
      ],
      "metadata": {
        "id": "Y_b53V90Dlbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio --upgrade\n",
        "! pip install gradio ai"
      ],
      "metadata": {
        "id": "iZPbWw523sBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai"
      ],
      "metadata": {
        "id": "AfbMqU7FILRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from gradio import Image, Number, Radio, Dropdown\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import openai"
      ],
      "metadata": {
        "id": "vJ11PNQbghbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/API AI OPEN.txt', 'r') as file:\n",
        "    openai_api_key = file.read().strip()"
      ],
      "metadata": {
        "id": "hqA1v6buyrjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = openai_api_key"
      ],
      "metadata": {
        "id": "ffJ-dau384GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Loading and Prediction Testing\n",
        "\n",
        "#### Loading the Pre-trained Model\n",
        "The script begins by loading a pre-trained model from a specified path. This model is essential for predicting the type of skin lesions based on image data and associated metadata.\n",
        "\n",
        "#### Model Architecture Overview\n",
        "Once the model is loaded, `model.summary()` is called to print the architecture of the model. This summary provides insight into the model's layers, their shapes, and parameters, which is crucial for understanding how the model processes input data.\n",
        "\n",
        "#### Dummy Prediction\n",
        "To ensure the model is functioning correctly, a dummy input array is created using random data, shaped appropriately for the model's expected input (in this case, 224x224x3 for image data). This array is then used to make a prediction, testing the model's readiness for actual data.\n",
        "\n",
        "### Image Preprocessing and Prediction Explanation Function\n",
        "\n",
        "#### Image Preprocessing\n",
        "The script defines a function `preprocess_image` that takes an uploaded image file, resizes it to fit the model's input requirements, normalizes the pixel values, and expands its dimensions to include a batch size for model input compatibility.\n",
        "\n",
        "#### Prediction and Explanation Generation\n",
        "Another function, `predict_and_explain`, is designed to handle the end-to-end process from image and metadata input through to generating a human-readable explanation of the prediction. This function:\n",
        "- Preprocesses the input image for model prediction.\n",
        "- Uses the model to predict the lesion type based on the image.\n",
        "- Maps the prediction to a readable class description.\n",
        "- Optionally, uses GPT-3 to generate a detailed explanation of the diagnosis, integrating the lesion's metadata for a comprehensive overview.\n",
        "\n",
        "### User Interface for Skin Lesion Classification\n",
        "\n",
        "#### Interface Setup\n",
        "The script utilizes `gr.Interface` from the Gradio library to create an interactive web interface. This interface allows users to upload images of skin lesions and input relevant metadata (age, sex, and anatomical site).\n",
        "\n",
        "#### Interface Launch\n",
        "Finally, the interface is launched, making it accessible via a web browser. Users can interact with the model, upload images, input metadata, and receive predictions along with explanations right in the interface.\n",
        "\n",
        "### Usage and Disclaimer\n",
        "The interface includes a title and a detailed description, advising users on how to use the tool and noting that predictions should not replace professional medical advice. This ensures users understand the context and limitations of the model predictions.\n",
        "\n",
        "This comprehensive setup allows for an accessible and user-friendly way to leverage advanced machine learning models for educational and preliminary diagnostic support in dermatology.\n"
      ],
      "metadata": {
        "id": "zsqTPa3RhUkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gradio\n"
      ],
      "metadata": {
        "id": "ahhQ0PhdRxg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diagnosis_descriptions = {\n",
        "    'MEL': 'Melanoma: a serious form of skin cancer that begins in cells known as melanocytes.',\n",
        "    'NV': 'Melanocytic nevus: a common type of skin growth that often appears as a small, dark brown spot.',\n",
        "    'BCC': 'Basal cell carcinoma: a type of skin cancer that most often develops on areas exposed to the sun.',\n",
        "    'AK': 'Actinic keratosis: a rough, scaly patch on the skin caused by years of sun exposure.',\n",
        "    'BKL': 'Benign keratosis: a non-cancerous skin condition that appears as a waxy brown, black, or tan growth.',\n",
        "    'DF': 'Dermatofibroma: a common growth on the skin, usually found on the lower legs, that can be pink, red, or brown.',\n",
        "    'VASC': 'Vascular lesion: a type of abnormal growth or mark on the skin that is made up of blood vessels.',\n",
        "    'SCC': 'Squamous cell carcinoma: a common form of skin cancer that develops in the squamous cells.',\n",
        "    'UNK': 'None of the others: the lesion does not fit into any of the other categories.'\n",
        "}\n",
        "\n",
        "disclaimer = \"\"\"\n",
        "**Disclaimer:** This tool is intended for educational and entertainment purposes only and should not be used as a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition. Remember, this AI is not a medical doctor, and its assessments are not diagnoses. Use this tool responsibly and always consult with a healthcare professional for any medical concerns.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "M3H3C8nki6Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "# Load the model\n",
        "model_path = '/content/best_model.h5'\n",
        "model = keras.models.load_model(model_path)\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Create a dummy input array for metadata\n",
        "test_metadata = np.random.random((1, 13))  # Assuming metadata has shape (1, 13)\n",
        "\n",
        "# Combine image data and metadata\n",
        "test_input_image = np.random.random((1, 224, 224, 3))  # Assuming image data has shape (1, 224, 224, 3)\n",
        "test_input = [test_input_image, test_metadata]\n",
        "\n",
        "# Make a prediction\n",
        "test_prediction = model.predict(test_input)\n",
        "print(test_prediction)\n"
      ],
      "metadata": {
        "id": "K3tNqmqHRVlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('/content/best_model.h5')\n",
        "\n",
        "# Dictionary of diagnosis descriptions\n",
        "diagnosis_descriptions = {\n",
        "    'MEL': 'Melanoma: a serious form of skin cancer that begins in cells known as melanocytes.',\n",
        "    'NV': 'Melanocytic nevus: a common type of skin growth that often appears as a small, dark brown spot.',\n",
        "    'BCC': 'Basal cell carcinoma: a type of skin cancer that most often develops on areas exposed to the sun.',\n",
        "    'AK': 'Actinic keratosis: a rough, scaly patch on the skin caused by years of sun exposure.',\n",
        "    'BKL': 'Benign keratosis: a non-cancerous skin condition that appears as a waxy brown, black, or tan growth.',\n",
        "    'DF': 'Dermatofibroma: a common growth on the skin, usually found on the lower legs, that can be pink, red, or brown.',\n",
        "    'VASC': 'Vascular lesion: a type of abnormal growth or mark on the skin that is made up of blood vessels.',\n",
        "    'SCC': 'Squamous cell carcinoma: a common form of skin cancer that develops in the squamous cells.',\n",
        "    'UNK': 'None of the others: the lesion does not fit into any of the other categories.'\n",
        "}\n",
        "\n",
        "def preprocess_image(image):\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    image = np.array(image)  # Convert PIL image to numpy array\n",
        "    image = tf.image.resize(image, [224, 224])  # Resize to model expected dimensions\n",
        "    image = image / 255.0  # Normalize pixel values\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension for model prediction\n",
        "    return image\n",
        "\n",
        "def predict_and_explain(image, age, sex, location):\n",
        "    # Preprocess the image\n",
        "    image = preprocess_image(image)\n",
        "\n",
        "    # Prepare metadata\n",
        "    sex_encoded = [1 if sex == 'Male' else 0, 1 if sex == 'Female' else 0, 1 if sex == 'Other' else 0]\n",
        "    location_encoded = [1 if location == loc else 0 for loc in ['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other']]\n",
        "    default_values = [0] * (13 - 1 - len(sex_encoded) - len(location_encoded))\n",
        "    metadata = np.array([[age] + sex_encoded + location_encoded + default_values])\n",
        "\n",
        "    # Predict with model\n",
        "    prediction = model.predict([image, metadata])[0]\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    class_labels = list(diagnosis_descriptions.keys())\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "\n",
        "    # Build the response\n",
        "    description = diagnosis_descriptions[predicted_label]\n",
        "    response = f\"Predicted Lesion Type: {predicted_label}\\nDescription: {description}\"\n",
        "\n",
        "    return response\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_and_explain,\n",
        "    inputs=[\n",
        "        gr.Image(image_mode='RGB', sources=[\"upload\", \"webcam\"], type='pil'),  # Configured for image uploads and webcam captures\n",
        "        gr.Number(label=\"Age\"),\n",
        "        gr.Radio(choices=['Male', 'Female', 'Other'], label=\"Sex\"),\n",
        "        gr.Dropdown(choices=['Head/Neck', 'Upper Extremity', 'Lower Extremity', 'Torso', 'Palms/Soles', 'Oral/Genital', 'Other'], label=\"Anatomical Site\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Skin Lesion Classifier\",\n",
        "    description=\"Upload an image of a skin lesion and enter metadata to predict its type. The results provide a preliminary classification and are not a substitute for professional medical advice.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "JVanQA1QKvHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knU_FohFVKSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}